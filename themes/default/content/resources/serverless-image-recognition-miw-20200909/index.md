---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Serverless Image Recognition | MIW 2020-09-09"
title: "Serverless Image Recognition | MIW 2020-09-09"
meta_desc: |
    In today's episode, we use AWS Lambda to use the PyTorch machine learning library for image recognition. We setup a Lambda function to do the recog...
url_slug: serverless-image-recognition-miw-20200909
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Serverless Image Recognition | MIW 2020-09-09"
  description: |
    In today's episode, we use AWS Lambda to use the PyTorch machine learning library for image recognition. We setup a Lambda function to do the recognition with PyTorch as a Lambda layer. Then, we connect the function to an Amazon API Gateway in order to allow our function to be invoked from anywhere.  Code for this episode available here:  https://github.com/pulumi/pulumitv/tree/master/modern-infrastructure-wednesday/2020-09-09  Today's example is in Python, but Pulumi makes it easy to stand up infrastructure in your favorite languages including TypeScript, JavaScript, C#, and Go - saving time over legacy tools like CloudFormation and Hashicorp Terraform.  https://www.pulumi.com/docs/get-started/?utm_campaign=PulumiTV&utm_source=youtube.com&utm_medium=video
  sortable_date: 2020-09-09T17:39:10Z
  youtube_url: https://www.youtube.com/embed/jZQBc3G8-qY
transcript: |
    Hello and welcome to another episode of Modern Infrastructure Wednesday. I'm your host, Lee Zen. Today, we're gonna be going over serverless image recognition. So using uh serverless functions to do some image recognition and really like trying to actually use the full depth of uh machine learning here uh to do. So. So what do we cover? We're gonna be covering uh creating a Lambda function. So fairly basic stuff, but then really integrating it with a uh pit toch lambda layer. So using pytorch as a layer to load in that whole library and then get the full benefits of using a uh fully featured framework like Pytorch. And then we'll integrate the function with an API so that we can uh go ahead and uh actually start uh start calling it invoking it against uh things we want to do image recognition against. So let's get started. I already have a, a program open actually. Um And uh there's a couple of things in here kind of worth worth showing you. I I kind of already world this up because there's, there's a decent amount of code that I didn't want to necessarily live code this um So I, I'll just walk through the code and then we can kind of understand how everything fits together. And then, and then I'll show you a quick demo of how, how it all works. Uh So, yeah, the first thing we do is we define a bucket uh for putting our model. So this is the image recognition model we're going to be using uh in this particular case, I'm using a, a fairly simple model. It's a resonant 50 model for those familiar with the machine learning. And uh it's, it comes, it's comes, it's a pretrained model from uh torch vision. Uh And so, uh the one thing I did do is I converted it uh since the layer I'm using is uh in pit toch uh 1.1. And uh the, the model itself is actually coming from an older version, uh the pretrained model. So uh there's a simple convert utility here that I wrote uh to do that conversion. So we convert that, we package it up with some labels and then we ship it as a uh as a archive. Um And this, this file is shipped as a bucket object into find our infrastructure. So our infrastructure has defined both a bucket and a bucket object. You'll notice that today, I'm using Python and uh you'll also notice one of the cool things uh about uh the, the Python support if you use Pulumi Python support before is that we actually have uh uh improved typing now, uh with some of the changes we made recently, so you get better of completion uh in your ID E and, and better typing actually across the board uh with the properties. Uh So just something, something I thought I would show anyway, since we're using Python, it's a natural language to use for machine learning. So, OK, so we have the, we have the bucket, we have the model object. So we have that uploaded and that's gonna get used uh by our Lambda function. And so uh our lambda needs to be able to uh do a few things. Obviously, a we give it a service role for, for LAMBDA and then we define this role. Um And we, we set it up so that you can uh use logs and post the cloudwatch. And then we also allow it to read from that bucket. So uh we actually give it all the permissions for the bucket since there's not a whole lot to do in that bucket. Um So you can see here, I, I take that bucket. A RN then I do and apply uh so that it can um uh include this uh little asterisk here to, to indicate, you know, all things in the bucket, all keys. Uh And then we allow all things. So this role is attached to this Lambda function uh in this function um is then uh um is uh basically an archive of this app and I'll show you the app in a second. Um And then we, we define it with some uh parameters like this. Uh And then we give it a layer. So this layer includes a prep pub uh uh pie toch uh V 1.1 layer that uh someone out there kindly has, has used and actually II I reference that repo um uh in, in the app. So we'll, we'll get to that in a sec. Um And then we pass this function a couple of environment variables, we give it the bucket and the key uh where the model lives. And that's pretty much it. Uh You'll notice that I also give it a longer time out in memory size uh Because uh this actually takes quite a bit to, to, to run uh in terms of uh loading the model initially. So, in fact, like sometimes you may even have on the cold start a uh time out uh in terms of the, the initial time out to load the model, but subsequent runs once that model is loaded and the function is warm are fairly fast. Um And you actually also know since the land that we're actually running the model completely on CPU um and not relying on, on any GP US, but we still have fairly good performance. Uh for this small model, the model itself has it's roughly like 100 megabytes in terms of all the weight data So we have the function. And then, now the last thing we want to do is connect to an API here. You can see, I have a simple swagger spec that basically defines an API with a proxy path. And then um this sets up the integration for me for a post method. Uh And then uh after that, um we basically have, have this API with, with the lambda A RN. So the whole, the whole thing here basically sets it up so that when we call any path against this API, uh it will invoke uh with a post request, it will invoke that LAMBDA. And then we finally set up a deployment in an API stage and we give it uh permission. Uh We give the, the, the uh we give that specific uh API permission to execute our LAMBDA and that's it. And at the very end, we export the invoke URL so that we can actually know uh what URL to call uh off of the rest API. So that's kind of the whole structure of the infrastructure we have, like I mentioned a bucket and a bucket object that represent where we post the model. We have this function uh along with a role that has the right permissions and the function will, will get into in a sec. And then we have that function integrated with an API gateway. So uh you can see I have in my, in my structure, I have this uh app here. And that's, that's what's referenced uh by this archive uh over here. And uh in this archive, we have uh this app dot py and that's actually the uh the function itself. This is pretty much copied from an existing example. Uh One change I made is that uh uh I, I load the, the model a little bit differently um to, to make sure it loads correctly. Uh And then after that, um you know, everything else is more or less the same. But you can see we, we basically, the way this function works is we uh have a request body. The request body has a URL for us to classify the image. And then it uses a pillow to load the image and transform it into uh into the correct format. And then we, we basically do prediction using our pretrained model against it. And then finally uh return the response uh based on that. So that's all the code actually and I already ran plume up earlier. Um So all the, all the stuff has already been deployed uh to the cloud. And so now the only thing we have to do is kind of show you uh an example of running this. So the function right now is actually cold. Uh So this is a cold run of the function, but you can see the way we'll invoke it is we'll, we'll do a curl uh And then we'll give it the body that we expect, which is a URL. Uh It's a JSON document with the URL and then the URL of the um image we want to classify. In this case, if you want to take a look at this uh image. Oh Actually, sorry that finally, as I'm recording this, that site is down. Uh So let's not do this, let's do this. Instead, this is another one I had pre prepared. Um So if we look at this image, let's load this up. You can see it's this uh this dog. Um And so, yeah, let's run this and you can see it's, it's sitting here for a little bit and it's gonna take a little bit of uh while to, to. So now, right now, the lambda function is running, it's loading that model uh from the S3 bucket and uh loading that into, into memory effectively. Um So this, I expect the time out. So I'm actually gonna pause the recording here and we'll come back in like a minute after it, time's out. OK. So, yeah, you can see this timed out, as I, as I mentioned. Uh Typically the first run, it's not gonna always work because of how long it takes to load the model. You can obviously, if you want um increase the time out. And so that's actually very easy to do. We can actually just go um to the function and change this time out uh to be, you know, something, something longer, like you can imagine, like, you know, give it five minutes instead of, of the single minute. I mean, we can just, you know, run Pulumi up, uh, to change that. Um, and you can see, you know, give us a diff of, of the, of that function. Um, but I'm actually not gonna do that. Uh I'm really just gonna wait for it to, um, load the model. So let's try writing the command again. And uh this time, you can see since the model was already loaded, it completed very quickly. I mean, you can see now we've predicted that this is a Maltese dog or terrier uh with high confidence. And again, I mentioned earlier that the model is pretrained against image net. So it's, this is the labels are against 1000 various classes of, of image net. So you can throw pretty much any image um at this thing and does a pretty good job like, uh you know, this is one of those models that's done fairly well over the years. Uh But yeah, that's the whole example. Um So really, you know, in a few simple lines of code, um we were able to uh set up uh our bucket bucket object connect that to uh a role with the, with the function and then have that function call. Uh And actually really want to emphasize kind of like, you know, we didn't really have to package up any dependencies in our code. You can see that the app was super simple. It's just this one file. Um You know, there's no other things that are getting uploaded as part of this because the layer handles all that dependency management uh for us. And then once we use that layer, we, we are able to load the model, uh use pit toch to do our inference and then get back a result. Uh And so, yeah, I hope you enjoyed today's episode. Please make sure you like and subscribe the video like the chan, like the video, subscribe to the channel and we'll see you next week on modern infrastructure Wednesday.

---
