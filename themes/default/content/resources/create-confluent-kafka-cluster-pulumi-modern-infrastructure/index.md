---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Create a Confluent Kafka Cluster with Pulumi  | Modern Infrastructure"
title: "Create a Confluent Kafka Cluster with Pulumi  | Modern..."
meta_desc: |
    Create a Confluent Kafka cluster using Pulumi. You'll use the Confluent Cloud Pulumi provider and Pulumi to create a Kafka cluster, topic, and cust...
url_slug: create-confluent-kafka-cluster-pulumi-modern-infrastructure
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Create a Confluent Kafka Cluster with Pulumi  | Modern Infrastructure"
  description: |
    Create a Confluent Kafka cluster using Pulumi. You'll use the Confluent Cloud Pulumi provider and Pulumi to create a Kafka cluster, topic, and customer account. â–º Step-by-step guide at https://www.pulumi.com/blog/create-manage-confluent-kafka-cluster-with-pulumi/  
  sortable_date: 2023-03-01T21:09:15Z
  youtube_url: https://www.youtube.com/embed/NWm9kAzQGXY
transcript: |
    Welcome to another episode of Modern Infrastructure Wednesday. I'm your host, Aaron Cow. Today we're gonna use Pulumi to create a Kafka cluster and confluent cloud. Apache. Kafka is an event store and stream processing platform. It's actually used by more than 30% of fortune five hundreds today. Uh C FA was created to allow scalable high throughput applications to store analyze and reprocess streaming data. So, however, managing coffa clusters can require some operational expertise. Um And that's sort of where con cloud comes in. They provide manage co clusters um along with some major value ads like elasticity, security, stream governance and improved monitoring, uh clusters and conflict cloud can be provision in Aws Azure or Google cloud. Um and Confluent cloud also offers cluster linking capabilities to on prem producers and consumers for hybrid cloud scenarios. So let's jump right in into creating this Kafka cluster. OK. Let's um create a new folder here. OK. We'll do a Pulumi new type script uh to create a empty project. Call it conso demo. OK. Dove. It's gonna go off and install everything. OK? That's done. OK. We're good there. OK. We're good there. So let's uh look at the uh real quick. OK. So I'm gonna copy and paste in some code and then walk everyone through it. OK. So our example architecture here will have the following components. Uh a Kafka cluster for our messages, an admin service account which will use to create objects within the cluster. A cof a topic for a cluster which will hold our sample messages, a producer service account which will use the right messages to the topic and a consumer service account which will use the read messages from the topic. So if we look at this code, um it's pretty simple. Uh So first you need a confluent environment where it's a container for everything. Then we deploy a standard Kafka cluster with multi zone availability. We're deploying it into us west two here. Uh Then we create a admin level service account to create cof the cof a topic and producer and consumer accounts. Um So this admin level app manager account is similar to the DB A account and relational databases or root accounts and Linux. Uh Then here we create a cof a cluster using the cluster admin service account credentials that were created above. OK. Then we'll create a consumer service account and uh give account permissions the right to that topic. All right. Sorry. I mean a producer service account and then we'll create a con consumer account uh which will read messages from the Kafka topic. And then finally, we're gonna export out um a, a bunch of these um variables like topic names, environment ID, which will actually reference uh with some of the confluence cli commands that we're gonna use. All right, with all that we're gonna hit, save and then we're gonna run a plume up and see what happens here. OK. That looks good. Hit. Yes. And it's gonna go off and create that and we'll be right back. OK? P pull me up is done running. Now, we're gonna simulate a producer and consumer. Uh So what we're gonna do is we're gonna use the confluence cli to send messages to and read messages from uh the topic that was created. Um And then what we'll do is we'll use the values of the Pulumi stack outputs to formulate the command. But first things first, we need to log into confluent. Um I had a bunch of issues um trying to debug this but um but you gotta lo in first. Uh um Actually, uh we need to actually have confluent, installed the sea lion stuff first. So let's do that. Um Let's do that. And now let's run conflict login. OK? It's gonna open up um a web page to do the authentication. Um So once that's done, we're good there. OK? So with that, it can be done to simulate a message producer. Uh We're going to issue this following command. Uh So you can see here that um some of the um parameters are set using uh the plume stack outputs. All right. So the Kafka producer starter started and then we're gonna enter in a few sample records. So we're gonna copy that and then we're gonna control, see to get out of it. Now, now we're gonna simulate a consumer to read the records that we just wrote into the Kafka um stream. Uh So we're gonna enter this command. So while, while that's running, um a quick thing, I didn't point out at the beginning uh which is um before you can use the plume um confluent cloud provider. Um You're gonna have to um obviously sign up for a free trial on confluent cloud, get a account and then create an API key and set its values as environment variables. So just want to quickly call that out. Uh Since I didn't show that OK, this is done. So um we, that works. So the consumer was able to retrieve the same messages that the producer was able to send in. Uh This is how you deploy a Kafka cluster in confluent cloud using Pulumi. Uh This video is actually based on a blog post by Jo Josh Cod. Um And the link is below in the post. And if you enjoyed this video, don't forget to hit like and subscribe below that. Is it for today's modern infrastructure Wednesday? See everyone next time

---
