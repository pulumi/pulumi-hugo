---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Cloud Engineering Summit 2020: Being On-Call Doesn't Have to Suck!"
title: "Cloud Engineering Summit 2020: Being On-Call Doesn't..."
meta_desc: |
    Companies rely on key services to be available nearly 100 percent of the time in order to make revenue. A consequence of this situation is that it ...
url_slug: cloud-engineering-summit-2020-being-oncall-doesnt-have-suck
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Cloud Engineering Summit 2020: Being On-Call Doesn't Have to Suck!"
  description: |
    Companies rely on key services to be available nearly 100 percent of the time in order to make revenue. A consequence of this situation is that it has become natural for Engineers to get woken up late at night or early in the morning to resolve incidents. But whether you rise to the occasion or not, it eventually becomes a very taxing experience. Unfortunately our industry has accepted this as the norm. There is a better way. Chaos Engineering. In this session we will explore how we got to this point and how we can adopt Chaos Engineering to help us wake up less and sleep better.
  sortable_date: 2020-11-11T00:29:00Z
  youtube_url: https://www.youtube.com/embed/fFDo_o3XM0k
transcript: |
    I loved being on call. The fact that a company much less a multibillion dollar company uh relied on me to save the day felt incredible. Is this how it feels to be spider-man? Maybe. Um Of course, in the context of getting woken up at 4 a.m. to save the day, uh hundreds of incidents resolved by little old me. Well, I remember the day that things changed. Um Cyber Monday uh was the best year over year, best day in company history uh leading into uh a particularly memorable one on November 27th, 2017. We scaled up our instances in advance, Prew warmed our low balancers. Um even went through this large launch readiness checklist. We felt ready and personally, I was super excited. Then later in the afternoon, we started seeing some index domain errors which we then started looking at our DNS cluster and couldn't find anything, everything looked healthy. Uh Then the panic started to kind of set in a little bit, felt like the walls began to close in. Um I remember specifically that um the person uh talking to me on the call was our VP of infrastructure as well as our CTO asking me what was going on through all the, the kind of the white noise I had no idea what was going on. Um And every minute thousands of dollars were going down the drain. Of course, this is also the same week as a W us reinvent. So people are popping in from Las Vegas, the Venetian in the background um to try to jump in escalate and figure out what was going on. We eventually looked at our, at our console cluster for service discovery and saw that our EVs volumes couldn't handle the the IO load. So it essentially fell over. Um We didn't realize later on that um Our ebs volumes that we low testing, uh low tested in our standing environments were actually a different size than they were in production, which completely invalidates the test. Um Of course, this came from a post warm several days and for unfortunately, a few million dollars later on call can't suck, but it doesn't have to. So good evening. Uh My name is Jacob Blick um or if it's morning or afternoon, good uh morning and afternoon as well. Um I'm a senior solutions architect at Gremlin. Uh I help our customers across a variety of different industries including finance, e-commerce, Airlines, uh retail and insurance, um help build out their chaos engineering practice. Um Because you didn't know Grimm is a hosted platform that lets you run cast engineering um experiments simply safely and securely. I've actually been at Gremlin for just over uh two years and previously worked at a large sports e-commerce company that I alluded to called Fanatics, uh where I served as both a cloud operations and senior site reliability engineer for about a little over four years. Um So there I was responsible for providing a reliable e-commerce experience to process upwards of over 1100 orders a minute all while training junior SREs. So uh reliability has been sort of the bread and butter um as for a long time, especially on days such as Cyber Monday and Black Friday. So this on call situation, how did we get here? So especially if we zoom in to right now in our current uh living situation, this recent move to working from home shopping purely online um has put a lot of strain on different companies across every industry. Um Many of them not being prepared for this level of demand um during this pandemic. But if we actually zoom all the way out a bit, we can actually see that this has been a problem for years in our crazy rapid innovation, digital world. Uh We see things on Twitter of companies running into issues hashtag hug ups and uh whether that's regular e-commerce failures on Black Friday breakdowns at banks and financial institutions. Um And in some cases, life threatening incidents on airlines, the cost of these major technological breakdowns goes far beyond just the billions lost in company revenue. But of course, more transactions uh and business are performed online than ever. We have our network speeds that are constantly increasing and users that are getting more and more demanding. So we're looking for new ways constantly to satisfy that demand quickly and cost effectively. Uh This may mean breaking up monolithic systems for performance reasons, uh distributed systems for ease of management and the promise of uh reduced infrastructure costs. Um as well as the need for checking all of these services wherever they are. So, but frankly, with more complexity comes more risk that things will break. So in order to keep up with the speed of innovation, we've adopted new technologies and approaches. So your team or your company's journey is likely somewhere um on this slide. But of course, this added complexity comes at a cost. So of course, we're increasing this rate of change which then increases the complexity of our systems, which increases the numbers of the numbers of the number of failures unless we're investing in both velocity and reliability. This is what allows us to shift the curve to achieve both reliability at the speed that we want to. However, as this journey continues, we start to think about the definition of operational maturity. So the majority of the industry considers being operationally mature from this perspective as being ready to fight incidents. So this is a major component um in the maturity of a company. Uh but I would argue it's only half the picture. I actually like to think about operational maturity as being proactive. So we can interpret this as architect for failure in development and then testing our assumptions about our systems early and often. But of course, we have dependencies on networks that we don't own infrastructure, that we don't control uh orchestras that are black black boxes, open source or legacy dependencies. Uh and the people operating the systems, testing the code of or just the code I should say isn't enough. So we need a new way to test the other parts of the application stack. So we make sure that we configured everything right and that all of our processes are in place. So, so far, we're just looking at a small piece of this proverbial iceberg and then on top of that to make that is even more fun. Uh We're responsible for the reliability of systems that we just don't understand. So how do we efficiently test and operate these new complex and distributed systems? Well, that's where chast engineering comes in. So what is it? And how does it fit? Um So if you remember nothing else from this particular presentation, remember these, these four words, buffle and planned. The term chaos is actually more of a misnomer. Um I've heard it more referred to as like a marketing jargon, but the truth is is we're trying to validate or disprove a hypothesis. And then as we reveal weakness. The ultimate goal of cast engineering is to shine a light on laten issues that already exist. Um One of my favorite analogies I've heard is around the fact that um if you take a flashlight or, um, or you know, your phone in the case, you know, 2020 shine a, into um, a basement or, you know, if you're in Florida like me and don't have a basement, maybe you're more of an attic, you have all these like spiders and stuff and uh in, in this particular area. Um If you turn off the the flashlight, it doesn't mean that the spiders are suddenly gone, right. So there's a concept called the blast radius. We always recommend starting small and carefully and purposely increasing the blast radius. So this typically means experimenting with a single or a few hosts, not your entire fleet, but this also can mean starting in your development environments and expanding outward. Speaking of expanding outward, um So this typically starts over as you escalate up your environments. So you can adopt the practice in your development phase so that your engineers are thinking about and validating that they're architecture for failure early. And then once you're, once you're confident with a particular failure mode, you could begin testing that and staging on a subset of your, of your environment and then expand from there and then you simply rinse and repeat on your way to production. But what's great is, cas engineering brings two crucial benefits. So first, we can pra proactively identify and fix bugs that could produce an outage rather than waiting for system failure to show us where the weakness is. Secondly, by running proactive game days, our engineers grow more familiar with systems behavior and makes them more effective during an incident. Not to mention, this also helps us tune our monitoring and detection systems so that we can detect issues earlier. Now, a quick note on fire drills. So I, I think in my younger grade school years, I have to admit I took fire drills for granted, but they were a colossal waste of time. But now that, you know, I'm an adult and we can actually use uh fire drills to train for incidents. It makes a lot more sense to me now. So we actually are able to use these fire drills to train ourselves to stay calm and know exactly what to do. So many of us, if you think back, you've probably been um placed on call for the first time, given an on call, phone or pager and been pointed out the run books that are covered in cobwebs, maybe they haven't been updated in a long time and essentially been told, good luck. Um That's pretty common, unfortunately, um including myself, um I've been responsible for uh systems that I didn't have a run book or even an architecture diagram on. But um liability and internet management fell to me. So we can actually make on call less painful by running these fire drill scenarios as part of either onboarding a new engineer, as well as ongoing training exercises. Then even when run books and solutions do exist, they're a great way to make sure that they make sense. Um For example, I once ran through an exercise where everything was going perfectly, alerts, triggered to the right people. They responded, Uh probably no issues logging in and getting into things and getting things working. But when we got to the actual run book itself to fix the issue, the run book simply just said, wait for 15 minutes, not great. So there's actually plenty of places where folks uh tend to get started. Um whether you're moving to the cloud migrating to micro services um adopting KTIS um or uh actually figuring out your monitoring gaps. Um A pretty common example here is if you're evaluating a new monitoring tool and you're trying to figure out how to decide between um one or another, you can actually do cast engineering experiments and see um what responds first. So to dive in a little bit deeper. Um So there's a few experiments in which um are really common for folks to get started um around verifying monitoring just to avoid those missed alerts and um prolonged outages because of the fact that there was an issue that wasn't responded to. Um a pretty common one is a slow response from um your application to your database. So that's a really great way to get started. Um, as well as, and we'll get to autos skiing here in a second. Um Which I'll admit the very first time, uh that I knew that I fell in love with, with the, with the cloud was when I found out that autos skiing was even a thing. So it was really cool then as we start thinking about um incident response, um this becomes a um kind of the level two. So you wanna start thinking about perhaps you had an incident and a, a pain, a particular pain point you wanted to re implement and see um either as part of a postmortem or much later just to verify that you've um that you and your team have, have gotten better. Uh This is a great way to get started. Um I actually remember uh a story about uh a cloud architect uh that I worked with and I was, we were talking about this large confluent stock which had all the different dependencies that we had as well as all of these different failure modes. And uh at the time, I was kind of the low man on the tom pole uh from a cloud operations team perspective. And I asked, when are we going to test these? Um they, this is a long list. We should probably, you know, get started on it. Um But we didn't have time. And uh so it just kind of stood there, but also we didn't even know that cast engineering existed uh back then. So we actually have a really easy way to get started now. And then from a cloud architecture perspective, specifically, we can actually make sure that our auto dealing is tuned. Our teams are prepared and able to handle those degraded or, or lost networks as well as we can Um actually invest in doing region evacuation and make sure that that doesn't knock us off line. What's really great is that Aws Azure and Google Cloud all have design principles used internally for their own systems that they published for other companies to leverage. So these actual design principles include CASS engineering practices such as running game days to test out your workloads and train your teams. So that way you can simulate failures in an automated fashion just like that. So what can you do if you're on call this sprint, run some experiments in your lower environments? Of course, making sure that you're communicating this but have a bit of a chaos hour during that sprint. Worst case scenario is you validate an assumption that you had about your system that you're responsible for and you learn something. The best case is that you might find some surprises that they happened upstream in production, you'd be running that incident. So that's where you get started. But where are we heading? What's next So there's no need for us to wait until something bad happens and then postmortem. So let's start small in our dev environment with a hypothesis, slow down or black hole that dependency and learn more about our systems. So also we can stop reading uh those postmortems and start sharing these pain points with each other. Imagine if there's a world where we are all talking to each other about um what we've learned about our systems, then we can understand the need for our on call by sharing these stories and doing chaos engineering to help it, make, help us make it not suck. We gotta get better at telling these stories to each other. Frankly, it's why we're all here to learn and to get better. So, Caci Engineering actually creates this really interesting force function that causes us to ask these questions about our systems and just as if not more important to our people. So a few links before you go. Uh So right now while you're watching this, uh there is a, the largest, you know, no big deal, the largest cast engineering uh event uh happening right now uh at Casco dot IO. And uh by the time you're watching this, it's almost over, but you can sign up and make sure that you get um all the recordings um from those um from those talks. So those are, that's an awesome way for you to really kind of jumpstart your cast engineering. Journey. Secondly, um, everyone asks about swag stickers. Uh We have some really, really awesome ones. So if you go to grim dot com slash talk slash Duval, just as a short, thank you for, uh, joining me on this, this crazy journey. Uh You can actually absolutely. Um, grab some of those stickers there. They're pretty, they're pretty cool. Um, main point Grima dot com slash community. Um, those conversations that I mentioned are already happening in our community Slack. There's somewhere over 5000 different engineers um with a bunch of different um places in which to get started. This also will link you to several tutorials um As well. Touched on being on call hero a little bit. Um My uh compatriot Vince wrote an awesome blog about not being an all call hearer or not being uh just an on call hero. So check that out and lastly Vince and I actually um just wrapped up yesterday, the first half of two webinars about planning and architect for reliability. So right now you can actually watch the recording of part one and part two will go live on October the 22nd. So just in a few weeks. So amazing, amazing time uh I had putting this together. Thank you for listening. I hope it was helpful uh tweet at me, add me on linkedin, email me Slack me. I'll actually be available right after this for a bit of Q and A always happy to chat. Um and as one of the best teachers, Mr Rizzo, the magic school bus once said, take chances, make mistakes and get messy. Just not a problem just yet. Thanks again.
---
