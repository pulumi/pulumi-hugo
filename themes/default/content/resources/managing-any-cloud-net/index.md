---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Managing any Cloud with .NET"
title: "Managing any Cloud with .NET"
meta_desc: |
    Get started defining cloud resources using .NET!  Pulumi engineer Mikhail Shilkov will show you how you can use C#, F#, and VB.NET to define and ma...
url_slug: managing-any-cloud-net
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Managing any Cloud with .NET"
  description: |
    Get started defining cloud resources using .NET!  Pulumi engineer Mikhail Shilkov will show you how you can use C#, F#, and VB.NET to define and manage infrastructure for Azure, AWS, and on-prem Kubernetes clusters.  Try it out!  https://pulumi.com/start/  The examples are in .NET but Pulumi makes it easy to stand up infrastructure on any cloud using your favorite languages including Python, JavaScript, Go, and Typescript - saving time over legacy tools like CloudFormation and Terraform.
  sortable_date: 2020-04-21T15:40:03Z
  youtube_url: https://www.youtube.com/embed/hXhZiHtT8f0
transcript: |
    Hello, welcome everyone. Today's session is going to be focused on Pulumi for dot net developers. So if you know dot net, but maybe you don't know what Pulumi is or maybe heard of us, but don't have deep understanding yet. This session is for you. The session is not specific to any single cloud. I'm going to show several examples for Aws Azure and even very briefly about myself, my name is Michel Skov. I'm a software engineer at Pulumi and my main focus for the last month was all things dot net. So if you happen to have any questions, any time, feel free to reach out to me on Twitter at Mihael Kov or via email Mikhail at Pulumi dot com. Now the agenda is simple. I'm going to start with a demo of how Pulumi works, how we can use dot net languages to define cloud resources. There will be a section with theory about desired state configuration tools and the brief comparison to other tools that you may already know and then I'll go ahead and hammer down on why I think dot net are super awesome ideas. Yeah, I really do think so once again, the concept applies to any cloud of your choice. OK. So what is Pulumi pluming is a tool that allows you to manage cloud resources and hopefully makes this process as straightforward and reliable as possible. You can use the net languages to define those resources. Actually, we support multiple runtime, not Js with typescript or javascript Python free go, but today we'll focus entirely on dot net. So in terms of dot net poly me runs on the latest dot net core 3.1 and then all the languages C# sharp and visual basic dot net. Pulumi is based on the idea of providers. So Aws has a provider, Asia has a provider, Google cloud has one in total. We have 40 providers currently including smaller clouds like digital ocean and cloud flare or even things that are not exactly clouds. You can create and upload docker images, resources or even things like positive equal tables or dashboards. Everything that has a notion of resource creation and has an API can become a plume provider. By the way, everything that I'm showing you today is open source and github and can be used for free. You can also create issues. We accept pull requests. For example, if you miss a provider that you really need, you could create your own and then maybe share it with the community from here. I want to jump straight ahead to the first demo in this demo will create and deploy a static website that would run on AWS S free. I'll start from the scratch and we'll walk you through the workflow of Pulumi. So I started my command line. I created a new empty folder called AWS website and now I have Pulumi Cella installed on my system so I can use that command line interface to bootstrap a new project with Pulumi. I'm gonna go for Pulumi and then AWS C# is the name of the template that I want to use. I wanna give my project the name here and then accept all the defaults for Stata and origin that I want to deploy to. And then CLI creates several files of my disk and bootstraps my AWS C# project. I'm gonna go ahead and switch to the ID to see what exactly was created. So now I switch to my ID. I'm using jet brains writer in this case, but you could be using VS code or visual studio if you prefer, prefer those I have my project open that we just created a W website. My and it has three important files here that I want to show you. The first one is CS P file. So the C# project file you can see that's a typical Microsoft dot net is the K file that uh dot net core console application which runs on dot net core 3.1 the latest one and then automatically by Pulumi I it got a reference to a new get package called Pulumi dot aws. That's what we're gonna use to define our resources for the rest. It's pretty much a blank project. Now, then there are two C# files. The first one is program dot CS. So that's the entry point for our consult application. It has a class program and a very simple static main method. All the method does is it just calls deployment, run a sync here and it passes the name of the stack, my class that I'm going to show in a second. And finally, all the smart things should happen in this my stack dot CS files. That's a class which derives from stack based class and it has a constructor and a hint that I should start adding my resources here. I have some short cards to walk me through creating a static website on aws. The first snippet creates an S free bucket in aws. It's called my bucket. That's the first parameter for the close constructor. And then it accepts a bunch of arguments. In this case, it says that I want a provision, a static website with index document pointing to index dot html. Then on my parent folder, I have a root folder which contains several files that I want to upload it as my static website. Someone see sharp code so I can do whatever I want here. In this case, I want to use the standard directory class to get the list of files in the root folder and then iterate for that list of files and do something for each file. Then for each file, I want to actually extract its name and then create a new bucket object for that file, which means that the file will get uploaded to S3. So once again, creating a new bucket object is a constructor call. I pass a name for that file and a bunch of arguments saying that this file should be public. It belongs to that pocket name that we just defined. And then that's an html document. And finally I pointed source to file asset on the disk, that file that I listed. Now, the last step is to define the output of my program because uh I don't know in advance the URL of the static website that Amazon is going to create for me. I need to export that URL from my program as a result of its execution, I define a property called endpoint here. And I assign a value to this endpoint format in the stream with http prefix and using the output property of a bucket called website endpoint. That's gonna be returned by Amazon when I run my program and that's about it. Now, I'm ready to run it and see how it works. So I'm back to my command line interface and now I want to run my plumbing program. The way I do it is with poly I command, I type ply up and run it here from the same folder and what it does, it builds my project with dot build and then it evaluates the graph of the resources that I defined there and shows me a preview. So the program already ran, but none of the resources are created yet. It shows me the preview that it's going to create four resources. The top one stack is like a virtual resource. It doesn't correspond to anything in aws. But then there are three real resources that Pulumi identified. One is as free bucket and then two bucket objects, one for each file. So if I'm happy with this and I am happy with this, this is what I expected, I can say yes. And now Pulumi will run my program again and this time it will translate the program to the actual calls to aws API. So that's probably gonna take a couple of seconds more than last time. S free bucket is actually being created in the cloud. And then once the bucket is created, the two files are uploaded as well. And the result is the URL that I get here. So now to check this URL right from my command line, I can do a curl and then pass it the result of Pulumi. You remember I defined the endpoint in my program so I can get that value from the endpoint and curl to it. And the result is the html document that I uploaded. Hello from Poum and C#. So my static website worked. Now let's switch to the id and do some changes to my program. For example, let's go back to this code that defines an S free bucket and add some text to it. So let's say I wanna tag it as my environment is production and then maybe as well. I want to change the contents of my website. So for instance, open this index dot html file and add a bunch of exclamation marks in the end so that we could see the difference to apply these changes. I run pulling me up again. And this time it should go ahead and compare my new program with the previously deployed program and figure out the difference that it needs to apply to AWS resources. In this case, you can see the preview, it's about to update the tax property of the bucket and also update the source or the content property of index dot html file. That's exactly what I would expect. So I'm gonna hit yes and wait until Pulumi runs my program again and applies these two differences to my static website. It's changing the bucket with the tax and then it should replace the file contents as well. Now it's done. I can scroll up to my curl program again. Hit it and you can see that I get a bunch of exclamation marks which means my changes were successfully applied and the website is updated. So Now, after the demo, I want to step back a little bit and look at some fundamental ideas that might be helpful to understand this stuff a bit deeper. I wanna explain what desired state configuration is and how it squares with the imperative languages. But first, what is so cool about the cloud? It's that everything is available via an API every cloud provider has its own layer of management. API in case of a, it's called as a resource manager, but everybody has something for any cloud resource, you can call a proper rest endpoint, wait several seconds or maybe minutes and get a resource back. And it doesn't matter whether you use a web portal, some sort of C I tool and decay or any third party tool like PLU. In the end, you deal with the exact same API but in very different ways. For example, every cloud provider ships management is the case for all major languages including dot net. You can open your ID type, the right command, run the program and the method calls in this case, easy to run instances will get translated to the API calls. This is one on one translation. The second you call the method from C# there is an http request going out. Alternatively, maybe you are familiar with tools from cloud vendors, you call them from command line bash powers or whatever. And again, each command is directly translated to the API call to say create an instance. Now this approach is a bit limited in multiple ways. It's very imperative if you have multiple resources to create, which is always true, you end up writing scripts or programs and describe how to create resources step by step in the exact sequence. Instead of describing what you want to get. Also, you probably have multiple environments like production stage and lab and they exist for months or years, they are sort of similar, but at the same time, different, you end up with manuscripts with lots of built in conditions and also update scripts that move the infrastructure from the previous state M to the state N plus one, maintaining all of that is hard. Finally, the scripts are super hard to make resilient. If they fail in the middle, you don't really know whether you can run it again to complete or if it's just going to create some duplicate resources or fail with a new error. These things are really hard to test. So the industry has come up with a different idea, it's called desired state configuration. Instead of describing your steps, you always describe the target state, your goal. And then for any possible current state of your environment, a tool will take care of transitioning to the goal. So it's a tool that takes care of all the migrations. You focus on the desired state itself. A useful way to look at both of those states is think of them as graphs. The notes here are resources and the edges are dependencies between resources. Dependencies are crucial to figure out how to reconcile those two graphs, whether or not can be replaced or what's the right order of execution. And so on. Now again, you can see this polymer snippet that I showed you before. It defines two resources, a resource group and a storage account. The most important idea of this whole presentation is to understand that this snippet still defines the desired state. When I run this code, it's not directly translated to API calls. Instead, it creates a definition of a graph. And that's what you are able to see in the preview during the demo, I can cancel and no resources are created. This picture should help explain how Pulumi works. It consists of several components at the bottom. There is a language host, that's a thing that is specific to each run time. So there is a dot net host which knows how to co dot net build and get the result back then it executes your program. And the result is that desired graph, the graph is then passed to the cli the engine. That's why we need the Pulumi cli to run the deployment. The engine compares the desired graph with the last deployed state and figures out the difference. And until then it knows the commands that need to get to AWS or, or DC P the cloud provider. So if you're on that simple program of two resources. The first time the engine compares it to the empty state and issues two commands to the cloud. If we say change the type of the storage account, the program is almost exactly the same but the engine will know to update or replace that resource and do not touch the resource group because it already exists. The idea of desired state is definitely not new. Actually, each cloud provider has its own tool to support it. A cloud formation, Azure resource manager templates, Google cloud deployment manager, and even third party tools like terraform, they're all based on the desired state configuration. Also, they are similar in the sense that they use market languages, Jason HCL to define the target state. However, the details of languages for each of those tools are completely different. If you know one, it's pretty hard to start with another one and broadly creating cloud applications with markup is quite challenging. Each language basically starts with no tooling. You just edit the text file, they're trying to add to on top of it like current templates plugged in for VS code, but they're still not really great and very different. Again, authoring a new template from scratch is quite hard. Usually people copy paste snippets from the web or stack overflow or the web console and try to adjust them for their needs and those markup files are still quite low level. So the sheer size of them grows really, really fast, you end up editing thousands of lines of Jason or which is a poor experience. Pulumi is very different here because as you saw, you can use the net languages instead of Marko files. So for the rest of my presentation, I want to focus on why using general purpose programming languages and do languages in particular is a great idea. We start with tooling if you are a dot net developer or at least you know a dot net language like C# to some extent, you are probably already familiar with lots of the tools that they show on the slide Pulumi runs on dot net called 3.1 which is the latest cross platform dot net run time, you can run on Windows, mac Os or Linux. Wherever there is a command line really, you can use your favorite editor or ID from Microsoft Visual Studio to vs code to writer from jet brains with all the power they built over many, many years of existence, water completion with tele sensor factory and with uh tools like sharper Linkin doc generation. Everything is now available for infrastructure management code. If you want to turn some useful bits of your code into a reusable library, you can wrap it up into a new get package and share privately within your company or with the broad community publicly on the nuge dot org. And later today, I'm gonna show you even how to do unit testing with dot net. So I already showed you a couple examples of polym code and C# this example here just reiterates that yes, we do support C# but C# is not the only dot net language is the case is dot net is the K not A C# is the case. So we also get support for other dot net languages on par with C# if say you are more into functional program and then you love F sharp, you can write pin programs in F sharp, the structure will look very much the same you knew up resources and flow properties from one resource to another one. If sharp is quite a bit more strict with type conversions and C#, you can get away with implicit conversions most of the time. While in F sharp, you would need to use some extra helper functions like IO or input in this example. Yet, even if you don't know F sharp at all, you probably can still read this snippet. So it's quite the same type of code. We also support visual basic dot net if that's your language of choice, same idea, slightly different syntax. Again, I think that one of the most valuable features of using dot net codes to provision cloud resources is the ability to define reusable abstractions. Quite often, you want to use several resources as a combination and you end up using more or less the same combination again and again with code instead of say copy paste in text files all over the place. You can make a class or a function and just use it in multiple places. Let's take a look at an example. If you wanna create a simple Asia function app, just a hello world HDP function, you have to create five resources, a storage account, a storage container, put your binaries to a storage blob and create a function app hosting plan and finally add the function app itself point to all of those things. By the way, a resource group would be a six required resource. And this combination of resources is the recipe that you are going to use many, many times. So with that net, I could create a usable component as a sharp class. I actually did that. In this example, I defined a class called archive function app. It's super easy to use. Just pass a resource group and point it to a folder on a disk which contains in my function and that's it. Now, if I run pulling me up with this program, I will get a nice preview of which resources are about to be created. And I still see that yes, all those resources are still required. But as a user, I had to write very little code and the chance to make a mistake is also small. Now, I actually want to switch over to my ID and show you the code behind that component resource. All right. Here is my function APP component. It's a C# class which derives from the component or source class. Remember last time our stack was derived from the stack base class. The component also calls the base class constructor and passes the name to it. This is the name we see in the Pulumi like preview as a parent for all subs resources. The rest is pretty similar. There is a constructor and it creates all those resources that we need for the component. A couple of things are interesting here. First, the service plan, you can see a condition there. That's because the component checks whether it already got a preexisting service plan and its parameters or it needs to create a new one. I'm going to show why in just a second, then there is a code to retriever storage, access token, it's C# so we can basically do whatever we want in the code, full flexibility. Finally configure some upset ends and puts an end point to an output. Now let's switch over to the usage examples. So at the top, we have a dot net Asia functions which is deployed from the folder somewhere on the dot net dot net. Publish. And uh because we create this component from C# the dot net run time is assumed by default by the component. The next example is the javascript function which has to be deployed to node run time. That's why we changed the two settings. First, we pointed the archive to a different folder. But also we set the run time option to note in this case. Finally, the most sophisticated example here is the Python function. And because Python function has to be deployed to Linux, we first have to create a separate source group and then manually provisional Linux plan, we set up its kind to Linux that reserves to true so that we get the consumption running on Linux. And then we can pass this plan as a extra parameter to the archive function up component. We pass it to the plan. You remember we had a check inside the component. So now that check will use Linux plan and the runtime Python. So we can make the component logic as sophisticated as you want. But the goal here is to keep the usage relatively simple and cover all the target scenarios. At the same time, these custom abstractions can be as simple or as complex as you want. Next, I want to show how this might play out with a slightly more sophisticated example. Let's say you work at an e-commerce company building a web shop for customers around the world. Your application consists of multiple services they all deploy to, let's say Asia cloud, maybe your team works on a product page. Now you have a challenge because your users are geographically distributed around the world. You want to deploy your application close to them to make user experience. Great. You need to keep latency as low as possible so that people in America Europe, Asia are equally happy. Maybe you use Cosmos DB to distribute your data. It's a great service. You can configure it to replicate data to multiple regions and it does its magic. However, if say you use Azure functions for the application layer, Azure functions are always deployed to just one region. So you'll have to deploy N copies of your function each two separate region close to the target users. Then you have to provision allowed balancing front and service like traffic manager or front door that would route users to the nearest location with the lowest latency. You can see how you easily can get to dozens of cloud resources that you have to deploy to deploy your service. And maybe there is another team working on, let's say a shopping cart service. They have the same challenge of serving the users around the world. So they have to go for a similar deployment. They also use cosmos the B but maybe they don't use functions but deploy container instances or whatnot as they compute layer. And maybe there is a third team working on the pricing engine and they use some legacy application bits that can only run on a custom virtual machine. So they use a RVM skill sets, but also they have to deploy load balancers, public IP SV nets, subnets and so on and they have to do it at times once forever region and still have Cosmos and configure traffic manager in front. There's a lot of repetition, but also there are unique bits. Now, we can sit down three teams of ours and decide that we are going to create a shared component. I call it Cosmos app because it's built around Cosmos Deb and that component would have a plug compute layer so that every team can customize it for their needs while the rest of the resources have the same shape. So I implemented such a component. And you can see it's usage example on this slide, it's pretty simple to use. You pass a resource group and the list of regions to where you want to deploy three regions. In this example, West US East and West Europe and then goes the magic of general purpose language. I can reuse my developer skills and use a factory pattern. Here. The component gets a factory, essentially a callback that creates a plug compute element for for a given region, it gets the region as input parameter. And then this example creates a function app in that region. In the end, it returns the function app identifier so that the traffic manager can link to its endpoint and that's it. The component takes care of the rest. It creates the cosmos DB. The traffic manager calls your factory end times and wires, everything together. We can package this shared Cosmos App component as a new good package. Put it into our private registry or share it with the community, we can apply semantic version and evolve it independently from the team's code, test it and apply all the best practices from vacation development world to the cloud infrastructure management. Next topic. And this one is pretty cool. So how do you test your infrastructure deployments? Well, traditionally, you would deploy the infrastructure and then run some black box integration tests with it. That's still super available, no doubt. And you can totally still do it with plume. However, I want to share another option unit testing. Here is a sharp example of a unit that written to inspect a pluta. You can see that instead of running deployment, run a sync, it calls deployment test a sync. This call does nothing in the cloud. All external calls are actually mocked. So it runs completely in memory and just returns the list of resources that stuck with otherwise provision. Now we can do whatever we want with those resources. They are just objects. In this example, I want to test that I don't expose a necessary part to the internet for that. I search for a security group resource, then get in grass rules and then I iterate for blocks to find if there is a block that exposes the port 22 to the public. If yes, the test fails. Now my test runs in memory so it's super fast. I can run hundreds of them. I can run during my DD flow. And of course, I will run them in my C I CD pipeline. We can make another step forward here with what we call policies code. Unfortunately, this feature is not supported in the do not decay yet. So this example is typescript, but the idea is that you sort of write a similar test that checks a rule searches for an open port maybe. But then you deploy this rule company wide so that any time anybody deploys any resource, the rule will get triggered. And if it fails, the deployment will get blocked and somebody will get identification a great way to ensure some security or cost management best practices up. Next, I want to talk about more sophisticated scenarios. I'm going to show you example. And also it's a good illustration of what a bigger project part by Polu could look like to take a step back for a second. As we see it, cloud applications are going through some evolution process, early applications were mostly lift and shift, which means people would move their V MS from on premises to the cloud from there. More teams start to modernize application, focus less on V MS, maybe use containers and also manage cloud services applications on the right sort of the future cloud consist of much more components that interact to each other. The components are highly specialized and they utilize the full power of the cloud which is great, but obviously, just because the applications have much more moving parts. The management process gets quite a bit more complicated. And there are multiple dimensions to this complexity. On this slide. On the horizontal dimension, we see that spectrum of compute options from V MS to containers to several. But there is also the vertical dimension which means that there are several layers of management. At the bottom. We configure the foundational infrastructure, networking storage monitoring computer is one level up, it relies on the foundation and then there are application bits to manage and deploy at the top. The great news is that you can use Pulumi to manage all of those layers and all of the options. NIS is a great illustration to this challenge. Again, there are multiple layers to manage to be able to run knits applications. First, you need to have storage network and identity and the foundational stuff. Then you need to create the cluster itself, maybe with Eksaks or G key. Then you probably also want to use a manage services could be managed no SQL database or Q or whatnot. And finally, there will be multiple applications deployed to the cluster. You could create all those resources in one Pulumi program, just reference several nugget packages and define the right resources. However, probably those layers are managed by different people at different time. Say your it team will take care of networking and then it's going to be pretty stable over time while application teams will keep updating applications at high frequency. So usually a project would be split into multiple parts. We call them stacks, one stack for foundational infrastructure, one stack for a cluster and a bunch of stacks. Each managing an application. Each stack can utilize stack references to load information from other stacks. For example, networking details or keep config to connect to a cluster. So now I want to run a quick demo of a multi project that deploys an A KS cluster and a sample application into it. So I have a sample solution open which illustrates a multi project with C# I have three projects. In this case, the cluster project defines all the foundational layers and a cluster. While the other two projects are applications deployed to that cluster. The cluster project has the cluster sta class. Its output is going to be a cube config string which is like a connection strain for the cluster in the constructor. I create a whole bunch of resources, a resource group, some SSH private QR ad application service principles virtual networks, subnets. And finally, the cluster itself. And finally, it assigns the coupon output. Now if I go to the stack project, it's a pretty simple stack. The essential part here is that it uses a class called stack reference. Then it can say, hey sta reference give me your output called coup config. Then it creates a provider with that coup config and now it can start defining the application resources. In this case, my application creates a service and the deployment to deploy a sample container. Finally, the third step is very similar but it uses YAML files to define the application. The point is that maybe you already have some existing community deployments with or maybe he charts. In this case, you can mix those right into your Polu program and reuse the existing assets. The example shows how to deploy a single Yamma file as well as an instance of an engine helm chart. All right. This were all the demos I have today time to wrap it up. So Pulumi is a modern approach to called infrastructure and application management. As more and more companies start to embrace the cloud native tech, there is an increasing demand for great tech to support that move developers get involved into cloud management too. So it is really natural to embrace the developer technologies here. You use all the great tools and ecosystem that was created in the last 10, 20 years. And many people are very familiar with. You get all the language features like conditionals, loops, functions classes basically for free in the package. For me personally, the biggest advantage is they used to create abstractions, say C# components and there is them over and over again. And of course, your infrastructure code becomes much more testable too. If you want to get slides to get started or read more, you just need one link, there's a link on the screen B dot slash Pulumi links. The shortcut will lead you to a guest with all the links I recommend you can see the preview done below the slide. Please follow us on Twitter at mcorp and myself at Miko. All P code is open source and github. You can create issues. We definitely accept pull requests. You can find more videos on Pulumi TV, on youtube. Other upcoming webinars are at Pulumi dot com slash webinars. And finally, if you have questions, you're always welcome to our communities like channel. There are more than 2000 people there and the whole team is there and everyone is very helpful.

---
