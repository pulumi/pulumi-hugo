---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Mercedes-Benz R&D NA - Building Self-service Clouds with Pulumi using Azure and Python"
title: "Mercedes-Benz R&D NA - Building Self-service Clouds with..."
meta_desc: |
    Mercedes-Benz R&D North America uses Pulumi to de-centralize cloud infrastructure and empowers their development teams to pick the resources that b...
url_slug: mercedesbenz-rd-na-building-selfservice-clouds-pulumi-using-azure-python
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Mercedes-Benz R&D NA - Building Self-service Clouds with Pulumi using Azure and Python"
  description: |
    Mercedes-Benz R&D North America uses Pulumi to de-centralize cloud infrastructure and empowers their development teams to pick the resources that best fit their application.   In this tech talk, MBRDNA demos Infrastructure as Code to provide reusable Python to manage Azure as well as PagerDuty, Datadog, and other resources.  The demos show Azure and Python but Pulumi works with any cloud and your favorite programming languages.  Get started: https://pulumi.com/get-started
  sortable_date: 2020-04-22T18:49:34Z
  youtube_url: https://www.youtube.com/embed/MGA_Bba2GqQ
transcript: |
    All right. Thank you everyone for tuning in today. Thanks to all the folks that plume me for putting this on and having us here. I'm really excited to talk about building self-service clouds and what we've been working on at the NBR DNA here in Seattle. My name is Carter Williamson. I'm a senior infrastructure engineer at the MBNR DNA Seattle hub. I've been with the hub for uh about two years and the majority of that time has been working on this product, uh the self service cloud that we've been building. So a brief bit of background before I get too far ahead of myself, the primary focus of the Seattle hub is to build the future back end for the connected car in the cloud. So this involved a two pronged approach of both migrating existing services out of a data center into the cloud without having to rearchitect the services themselves while simultaneously enabling teams to use cloud native solutions and uh building new services and applications. We went through quite a few different iterations on how to how to quickly build infrastructure and automate infrastructure for different engineering teams. Uh The solution that we ended up landing on is what I'm gonna be demoing today and that is our uh our infrastructure portal. So we wanted this uh this portal to really be something that uh kind of acted as a, as a kickstart to get teams, you know, focused on their application and their service development and give them a place just to run their code as quickly as possible without them having to take on the overhead of understanding all the different details and bits and pieces required uh to set up their run time in a in a environment on the cloud. Additionally, this is a a nice centralized platform for us to apply and kind of manage some global policies as well as provide some best practices uh for development teams. So without further ado I'm gonna go ahead and show this demo. So I'm gonna flip over here. Uh So this is our, our current portal. This is uh as you'll notice, it's a little different from what we saw on the slides, the screenshot we have there is our new beta portal that we're in the process of developing for. Now, I'm just gonna show what we have existing. Uh You can see there's a couple different types of deployments that we provide for our teams. Uh The kind of core piece here is this environment which is just a generic. It spins up a uh a number of resource groups with some network connectivity and uh a TTI cluster for teams to run their containers in. Go ahead and hop into here. Oh Timed out. So I'll just log back in. All right, I come in here and I'm gonna go ahead and add an environment. So I'm just gonna fill out a couple of things in this form here. I'm gonna call this demo one. Go ahead and find that group in here that I can use and we'll just set a development. Yeah, and we'll just say we have it expire tomorrow. So you can see here, we got some details around what was should be deployed and some, some kind of general cost guidance around those as well. Um We have some additional pieces here for more advanced users and then some tagging as well. Um So I'm gonna go ahead and just fill this form out real quick. Not say hm, let's call this development test project service portal and we got those ones out. So cool. All right. So that's it. I filled out my form. I can click save. They give me a quick pop up to confirm that I actually do want to create this, which I do right. So in the background right now, what it's doing is it's reaching out to the Azure devops uh pipelines um and kind of kicking off a uh a build there where our pipeline will run, do some configuration, some kind of set up stuff and then it will eventually run some plumy code uh which will spin up our resources in Azure. Um After Pulumi is done, there's some kind of post uh post steps that will install some uh some helm charts as well as a few other things. And then finally, it'll validate that the network connectivity between uh our internal network is is working properly. So this will take a few minutes. So I'm gonna briefly pause here and uh switch back to, to our slides while that's while that's running. So while that's uh while that's building in progress, I wanna talk briefly about kind of our organization's approach to operations and managing the infrastructure. Our team is uh pretty small. So we don't really have the capacity to manage and operate these uh all these clusters floating around in our cloud. Uh So what we're doing is we're decentralizing our operations and our team is acting as a centralized uh automation team where we can provide automation and environments to other teams that they can then manage um as part of their own pipelines and their own processes. Part of that is providing gloomy code. We also use the Azure SDK and some other Python tools as well as uh Azure cli and uh various other tools depending on what the platform we're we're targeting with our automation is. So what I'm gonna show next uh after our after our build is complete is how do we hand off the Pulumi automation to teams this is a big request. After the first initial round of environments were provisioned, we started getting teams coming back to us saying great. But now I want to manage this as the infrastructure code. Uh We want to own that ploy code essentially, which is is great. Uh And we were really encouraged to, to hear that teams really want to take the ownership of their environments to that next level and being managed with Pulumi. OK. So I'm gonna switch back over the portal now. Um The bill should be done now. Yep. So we can actually go ahead and go to our Azure portal here and get an idea of what this looks like when it's all complete and when the automation is finished running. All right. So here we can see all of the resource groups created by this automation. Um We have our KTIS resource L A workspace which is uh for logging and monitoring. So we set that up and we attach it to a KS automatically so that when teams come in, they immediately have access to their cinti logs and then they can pipe out their custom logs through that same interface. And then we have our kind of core pieces. This is just networking. So if we dive in here, we can see that there's a AV net and a route table and there's some subnets attached to this V net. And then the uh resource group created by the Azure KTIS service managed service provider, which actually contains uh information and resources about the nodes themselves for the KTIS as well as low bouncers. IP addresses security groups, et cetera. So coming back to the I DC A portal here. So what we were talking about before coming back here is the, the idea of wanting to give teams a way to manage their Pulumi code. So we've added this uh manage infrastructure as code button here. So what this will do is we click that and it's kicking off a pipeline that is running in the background and doing some uh initial setup. It's pulling the state from the provisioning that we just did as when we were building this and it's setting up a new project and zipping it up into a nice folder that then we can download from here. Um And what this is doing is really giving the the end user a way to just grab on to their Pulumi code. The Pulumi state, the resources that are already provisioned and essentially copy that into their own repository and then take over management. The downside to that is this creates potential for user error. When we export the plume code, teams could potentially modify the address space configuration value and not only mis configure their own environment but possibly start overlapping with other environments and creating conflicts to counteract this, we've set a ploy in such a way that depending on some runtime parameters and configuration values. We can tell if our team is running this through our pipelines or if the code has been handed off to the development team. If the code's been handed to the development team, their virtual network and address spacing and all those resources that we want to hold some control over, they will see those as a read only resource so they can interact with it the same way you would interact with any other resource in Pulumi. But it will also just not actually pull in that resource as a, a managed resource in their state file. Um Great. So it looks like uh that process is finished. So I'm gonna click download. It's gonna pull down a car ball that will have uh some scripts and plume uh YAML files as well as the requirements. TXT for pulling in different Python uh components and, and packages. All right. So I'm going to unzip this so we can see we have our Man dot Pie uh that actually contains our Pulumi code and I'll show this in a moment. Um We have our uh Halloumi config oops, our Pulumi config value uh config file here as well as uh read me with some instructions for initialization and our initialization script. So I'm gonna go ahead and move over to my terminal here and we can see it looks like openness and yes code, I can take a look here. So we gotta read me here. So I'm gonna go ahead and walk through these instructions quickly. Um But essentially what the script is gonna do, uh The initialization script is we're gonna set up some rules and permissions for user access to ensure that we have everything set up correctly. So when the users go to run, uh plue me up after running this initialization script, they'll be 100% ready to go out of the box. They don't have to go to Azure and set up any roles or configure their service principle. Uh We've taken all that and just put it into a, a small little bash script. And uh this will include setting up the uh Pulumi state and ploy stack in the uh the back end or locally if the team doesn't have a back end configured already. OK? So let's run this real quick. So just gonna run this script, I've already set up some environment variables um to identify my service principle and logged in with the AC cli and here we go. So this is actually setting me up um and giving me the appropriate access to our uh our local encryption key. Um That's a cool feature that we started using recently with plume is the kind of bring your own key methodology. So that allows us to have a key vault where we can manage access and who can uh who can access that key and who can read it and who can update it. And that is then our encryption key for our ploy state and our plumy secrets. So while this is running, I'm gonna head back to slides here briefly. So why is this important this uh this process of being able to hand off the infrastructures code? Uh It's really driving towards an end goal where we have infrastructure and application delivery in a blended way. So using Pulumi to do both your infrastructure provisioning as well as deploying applications and services through he charts setting up databases to integrate with those creating secrets and passing those into a key vault. There's a number of different ways that you can integrate. Pulumi with your current C I CD process that really allows this kind of blended infrastructure application deployment process. And so that's really the goal is first step is provision, infrastructure, get the code over to the teams, then they can decide to what extent they want to add on new resources or integrate their C I CD process with the Pulumi stack that we obtain to. Some teams prefer to just keep Pulumi as their infrastructure component, uh infrastructure management component rather while still using tools like helm to deploy um other teams really like to have everything kind of under 11 umbrella and that's a great, great chance for them to use Pulumi come back over here. And so now after that's been ran, we should be around plu me up. Whoops. Yep. Looks like I forgot to activate my virtual environment. So just do that real quick. OK? All right. Let's try that again. Let me, so we've already, again, we've already set up their stack. We've already selected the stack, imported the resources the con. So at this point, the developer has just had to click a few buttons and run one script and they've been set up. Cool. So we can see that there's really no changes here. Uh It looks like there might be a, a provider update available, but uh we'll just go ahead and ignore that for now. Um Yeah, so I'm gonna go ahead and uh I'm gonna skip this update for now since there's no real changes. And uh let's uh let's do a quick demo on what that would look like if we want to say, add something else here at the end of this, right? So for example, let's say that this particular um application maybe they need a post database. So let's add one of those things we can do postcrash db. Actually, we'll need a uh server here and I actually have these uh this code saved up on a different, different buffer here. So I will go ahead and just copy this in to save us some time. And so I'll walk through this briefly one second to set this up it. OK? So what we're doing here is we're setting up a resource group for the post server to go in. Uh We're gonna generate a random password using Pulumi built in uh random passwords library. And then we'll set up a server as well. Um Additionally, we could set up more databases attached to the server right here. But for simplicity's sake, I'm gonna go ahead and just show you this as this. All right, and bring me up. You should see those changes come up here in just a second, then we can apply those and we'll start building our servers. So this is a great example of say an engineering team who has a requirement, they need a post server um alongside their application for it to run properly. Uh This is a great example of how that can just be integrated into and expanded upon our environment template. Uh This is the same concept works for adding com charts and uh other deployment pieces. So I'll just go ahead and climate change, ok? And I'll go ahead and uh navigate over here. We should see the resource group is populated here. Yeah, just a second. Let's grab the name of that. Ah There it is, we can see that this is still in the process of creating the uh the post server here, that'll take a few minutes. So without uh without making us wait for that, let's jump over to our slides. So then looking beyond just the core infrastructure deployments, um we see some real benefit around uh using some of the providers, the offers and uh especially around monitoring and dashboard this code, this is really critical for us to manage our operations components um and being able to provide a kind of out of the box base level of, you know, some sort of health checks, monitoring dashboards is is is is critical uh for us. And on the right here, you can see an example of um not only how to how we can utilize the kind of native class structure of Python to abstract away the the little detailed components. But also um this is a, a rough outline of how we can integrate with data dogs. So that when an environment goes out, you can simply call this function and it will provision uh integration with in this case pager duty. Um There's a uh another great example here that we actually use for this service. Um I'll show you this here. So this is actually part of our provisioning. Uh This sets up our alerts. So you can see here is that we're using uh some of the built in uh components from the Pulumi Azure provider package. Uh So this is setting up an action group. This is really nice because we can build in our uh in this case, our gen web hook uh as well as some email receivers. So we can follow up with some emails. Um This is just kind of to show that, you know, you can use multiple different approaches um to alerting, but you can still wrap them all up nicely in just one little class. Uh So here we've got a list of different alerts and some of their thresholds and severity. Uh again, instead of, you know, creating a, a custom block of code for each individual metric, we can just kind of outline them as a dictionary and uh loop over them and instantiate them as we go. Um This particular use case is an interesting study because uh one of the limitations of application insights is that you aren't able to create an alert um on a custom metric until that custom metric has been initialized or has the first instance of that metric has been pushed into application insights. Um So this would have been a a pretty big challenge for us in the past. Luckily, we were able to just add a quick little function here that will hook into the actual um Azure SDK telemetry client for application insights. And all we're doing here is we're just calculating the threshold minus 10 so that we're not initializing this over the threshold of our alert. And then we are just sending that metric just below the threshold uh with some just kind of filler values to, to get those up there. And then after these have been initialized, we can easily create the the resources using the native Pulumi uh provider. This kind of practice really improves the integration between development and operations and infrastructure tying all those things together in uh one platform and one code base really has made our lives a lot easier. Um And kind of dealing with this, this sort of uh challenge and problem. And uh we have a few minutes here for some Q and A. Um So feel free to ask me questions about Pulumi, about Mercedes and how we're using Pulumi uh really anything around infrastructure automation, we'd be happy to answer at this point. Uh And with that, I'll just say thank you. And uh we appreciate again, Pulumi, thanks for having us. It's been great working with everyone on this project and sharing this journey with the Pulumi team and on behalf of MBR DNA, I'd like to extend a congratulations to Pulumi for their 2.0 launch.
---
