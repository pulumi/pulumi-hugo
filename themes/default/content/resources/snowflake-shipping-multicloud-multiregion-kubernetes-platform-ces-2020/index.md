---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Snowflake - Shipping a Multi-Cloud, Multi-Region Kubernetes Platform | CES 2020"
title: "Snowflake - Shipping a Multi-Cloud, Multi-Region..."
meta_desc: |
    Learn how the Snowflake team shipped a truly multi-region, multi-cloud, global-scale service in a few months using Kubernetes.
url_slug: snowflake-shipping-multicloud-multiregion-kubernetes-platform-ces-2020
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Snowflake - Shipping a Multi-Cloud, Multi-Region Kubernetes Platform | CES 2020"
  description: |
    Learn how the Snowflake team shipped a truly multi-region, multi-cloud, global-scale service in a few months using Kubernetes.
  sortable_date: 2020-11-11T00:29:26Z
  youtube_url: https://www.youtube.com/embed/oD9m6e3Bo2o
transcript: |
    Hi. Today's topic is multi cloud, multi region communities at Snowflake. My name is Raman Hariharan. I'm the director of cloud platform engineering here at Snowflake. Uh We at Snowflake are building a, a cloud data platform to break down data silos and enabling uh data collaboration capabilities while leveraging the near endless performance and scale of the club. The experience I'm gonna talk about is one of the flagship products that people were working on at the beginning of the year. The solution itself was developed on top of cities for a lot of, you know, reasons. It's it's becoming the de facto uh platform for developing containerized applications uh and which in turn allows for faster innovation and roll out across different cloud providers. The challenge uh me and my team faced at the beginning of the year as we were looking for a new tool was we needed a solution that can build deploy, manage communities, you know, clusters at scale right across different cloud providers and 20 plus regions worldwide. This was a problem that's not easily solved and probably limited to only the cloud providers. The traditional approach for solving this problem uh did not work for us. We are a software engineering organization. So the tool that we were looking for wanted was they had to meet some certain key criteria. One of them being, you know, it had to uh support our standard programming language, which was gola we wanted the ability to kind of treat our infrastructure as code, have testing capabilities and make it seamlessly integrate into our C pipelines. So we also were faced with a very aggressive deadline. So from we were innovating rapidly. So we had just three months on one end, we were supporting the product and efforts in terms of rapidly reading and developing uh you know, the the prototypes and iterating through it. On the other end, we had to actually think about rolling it out at scale. Um And that's where we're looking for a tool that could actually help solve our challenges at this time. I'm gonna hand it off to my colleague Joni, who's gonna talk more about our journey and how we leverage, you know, Pulumi to solve our objectives. Yeah. Thanks Ramen. Um I'm Jonas. I'm a software engineer here at Snowflake. Um Before that I worked at IBM and Mercedes Benz R and B. Um My primary area of focus is infrastructure and automation. So as Raman mentioned, let's dive into the solution overview a little bit. This is the high level architecture of how we were setting up our new infrastructure co located um to regional sofa deployments as we can see, we have AWS Azure and TCP S and max providers with multi regional deployments and each of them has a deployment. What are the components that constitutes our solution and how we were able to embark on this journey in the first place as no fake values, customer security. We had to really look into private networking with strict and English controls. Uh for that, we use security groups, calico and to we use postscript for our two major applications which is no site and data exchange. This is what you see when you go to app dot dot com. For mission critical infrastructure, we want to make sure we have good monitoring components in place for that. We use telegraph and wave from all of our application marks, make their way into blob storage accounts on the respective top providers and are then imported into our internal deployment which helps our developers to trace issues and look at certain events. On top of that, we want to make sure that all of our developers have a unified experience accessing all these deployments for which we use teleport. In order to have a seamless deployment in place, we use a GTOs workflow using RU CD that takes care of deploying our manifest off of a GIT branch and ensures that the state is reconciled at all times. Now, let's dive a little bit into how an actual single regional deployment looks like. I know it's a lot to unpack here. But bear with me, we have two public law balances. One is for ARGO CD that is used by our developers to actually access Argo and get a glimpse into how the application state looks like for our customers. We have a public law balancer for which traffic is directed to the respective applications we use for MT MS. And we have a layer network security model in place where we have security groups on the CP CS, we have calico rules for strict controls and still for service to service, communication and authorization policies. This is all done in a multi a fashion where if a certain note goes down, you can make sure that the application stays responsive. On top of that, we do have the post they never applied to it. Now, let's look into the networking architecture a bit more and how we ensure that there's connectivity to the existing snow deployments as mentioned before. Snow fake focus on security is high. So we need to make sure that the traffic does not traverse the internet. We started out with having a big co located VPC that uses VPC ping to the regional deployments using an internal opera on the side. And a core VPC that hosts shared services for monitoring. We use Telegraph as a data set running on each note forwarding stats the metrics to wavefront proxy which end up in wavefront. Ron really is our view into what's happening life in over 30 classes. Today, we do use ping them for our time checks. So we get alerted on that and we pay for incident response for logging. We use fluent D as a Damon said on each note that tails the logs and forwards them to the respective storage location based on the cluster. So for Aws, our logs go to S3 and for Azure, they go to Azure block down here, you see a small glimpse of our snowflake U I and how our developers are able to retrieve the logs. So now let's talk about how we were able to manage all these deployments and how we were able to create reliable and repeat of automation. Our infrastructure deployments, we use Pulumi, all of our platform components reside in git, we use a city that is met that is managed by it, clusters, logging, monitoring components and a bunch of custom controllers that helps us automating even further in these repos, a mix of customized resources and help shots reside a city reconciles the state that is based on you. Our internal customers have their own reports where they host their application manifests. Let's look how the deployment pipeline helps our developers to get the applications deployed. We have a developer pushing a commit to get and the specific branch of that application is picked up by A CD I CD. Then goes ahead and deploys all the application components into the respective customers name space the classes itself are deployed by Pulumi. Now, let's have a look at a small demo of how that could look like using Pulumi. What I'm about to show you is a setup that uses micros stacks to separate infrastructure layers from each other. So they can evolve independently. Similar to the notion of micro services. We also be using the next generation as a provider that has access to a broad and extensive list of A API S using the GO SDK. By using automation API, we are able to have an easy orchestration layer for stack updates across multiple stacks. We also make sure that we use a custom secret provider to keep our state secure using our own provider key. I switched over to my term here. So let me give you a quick overview of what the code does. We have two projects, network and cities network provides a shed layer of networking resources that are going to be used by Cobern. Let's have a look as it's customary for Azure. First, we create a resource group. Next, we create a virtual network as seen here was a Cyto annotation and the private IP space. We then go ahead and create a subnet as seen here that does not span the four V net. Last, we're gonna export certain properties that are needed by communities. Now let's switch over to and see how that looks. First, we create a stack reference to read off of the remote stack and retrieve the properties that we need for our cluster. We then go ahead and create an sh key pair that is needed to access the worker notes. And then we create a cluster. As you can see, we also limit the access to the API server to a certain IP which already shows the power of using a real programming image. We're calling another go function here. All that it is doing is retrieve the public IP of the host running it. We create an agent pole with three notes, create a user profile using 1 16 13. And lastly, we're going to export the coupon fake. So now let's look at the automation api what the automation API does is allow us to it go over stacks. First, we're going to retrieve the pass phrase that is going to be exported on the environment and then we dynamically create stacks and run, pulling me up. Lastly, we're going to export the cuc fake to write it out to the file system so we can interact with it. Let's see how that works. First, I got to export the past race and now I'm going to run my program just like any other program. As you can see, there are huge benefits to using this approach. We have static typing, we have ID support. If auto complete this is gonna take a while. All right, great. Our stack has been deployed now that we have validated access to the tests. Let's have a quick look at how it looks in the Azure portal. We created a research group, we have the cluster and the with our custom tags on it. So working with Pulumi helped us get to our goal fast. First, we were able to use a standardized language and framework. We have full ID E support that includes debugging. We have out of band operations such as making API calls. As we just see, we can use custom stack encryption to store secrets in the state. We have static typing and ultimately the type feedback loop that we had with the team over at Pulumi really helped us to get to our goal. Our team was also involved in design reviews for new features to Pulumi that will benefit the whole community. What's next? We hope to broadly adopt the automation API Once it gets out of alpha, we will set up C I systems to drive the automation even further. And ultimately, we're going to be putting cross guard policies in place right now. We're up and running in over 20 regions in a very short time span covering all major cloud providers. And we're super happy with the result. Thanks for watching my demo. Now, let's throw it back to Ramen. Thank you Jonas. So to summarize like we were uh very successful in the launch of the product and we are grading great adoption, uh you know, for the product and you know, I wanna just give a huge shout out to the Pulumi engineering and the support team who are a trusted partner along the way. Uh We couldn't have done it, you know, without them. And we feel like, you know, we just, just the start of the partnership and as we continue to innovate, you know, we, we're gonna continue to leverage, you know, the platform even greater possibilities. Thank you.

---
