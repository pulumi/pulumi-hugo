---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Office Hours 003"
title: "Office Hours 003"
meta_desc: |
    Pulumi CTO Luke Hoban talks through new features, new issues and any community topics. Ask him anything.
url_slug: office-hours-003
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Office Hours 003"
  description: |
    Pulumi CTO Luke Hoban talks through new features, new issues and any community topics. Ask him anything.
  sortable_date: 2019-01-30T20:12:23Z
  youtube_url: https://www.youtube.com/embed/lVvVHf_-bEA
transcript: |
    All right. Hi, everyone. And welcome back to this week's episode of uh Pulumi office hours. Uh As always, I'm Lou Cobin CTO at Pulumi. Uh I'm just gonna spend the next few minutes talking a little bit about um some of the things that are new with Pulumi. Uh and some of the questions that have come up over the last uh week or so um in the community discussions and discussions with, with uh users and customers and uh and talk about a few of those in a little bit more uh detail, maybe do some live coding and that sort of thing. Uh So again, for folks who, who haven't checked out Pulumi before, um you can get started with Pulumi at Pulumi dot IO uh install, get started, learn uh whatever uh you want to do here. Uh One of the things, of course, uh we have our blog. Um I'll cover a couple of the things we've announced, launched uh recently on the blog uh first and then I'll dive in a little bit more detail on a few things. So, one, just today, actually, this morning, we uh announced support through a uh identity. And so previously, folks could log in with github and a few about a month ago, we supported GIT lab as well now supported last year identity as well. So no matter what, where your source code is, no matter what kind of team environment you have very easy for you to connect up to the Pulumi dot com back in service and easily manage your Pulumi deployments. And so folks who are using at uh we, we've talked to many uh users and customers uh using at um who have asked for this. And so now that is a possible uh sign in option and of course for signing in, you know, is one thing. Um we actually then have a lot of capabilities that hook into other parts of uh your source control system, give you links back into your source control for your deployments, that sort of thing. Um And so definitely encourage folks to, to check out their lasting identity if they're, if they're lasting users. And also check out some of the features that are available for folks connecting into their source control within app dot Pulumi dot com. So you can read the blog post, see a few examples of some of the things that are available uh already like the C I integration there. All right. So that's a, that's a nice one. just opens up access to, to some new users uh as well as folks who have been using and get that uh the, the other blog post we put out um uh earlier this week or maybe it was last week was uh the bug post on using our Eks package with Pulumi. Um So, one of the nice things uh that we showed off in this blog post uh is just how easy it can be to install a fairly complex thing like EKS. Um So EKS is very nice managed service for doing cos deployments from uh from A W. Um But the getting started for this is actually fairly complex and involves a multi step process of, you know, spinning up the masters uh using, using Eks, then uh then spinning up some EC2 instances, then deploying some stuff into your cluster, a variety of different steps you've got to take, they're fairly complex. And so with, with our Pulumi Eks package, we actually make that just kind of one step, you know, you can, you can either run it just to install an ES cluster with a single command or you can write a very small kind of program to customize that to say I want to create a VPC and then I want to create a cluster with, with a small set of configuration available here. Um And so overall just a really easy way to um to use eks for folks using this. This has been something we've actually seen a lot of users being very interested in right now. Um is the ability to do this and is actually really well suited to kind of some of the things that Pulumi can do uh really nicely here in terms of managing your infrastructure. Uh So that's definitely if you are interested in using Eks um or if you are doing Plus Manage cloud um in Aws, uh definitely encourage folks to check out that blog post and go into a little bit more detail. Um So those are 22 great things we've done recently. Uh A bunch more uh features kind of that are, that are coming up soon that, that we'll talk about in the coming weeks. Uh So when I, when I reached out to folks in the community, uh uh last yesterday, I think about kind of topics to, to go into detail uh on here. Uh One of the things folks pointed out was, hey, we've, we've talked a lot about kind of a s over the last, you know, few months here. Uh And we've talked a lot about kind of some of the simpler serverless scenarios and really quickly getting up with the um with this service application or an HT PM point. Um We haven't gone into as much detail on Azure and GCP or on. Uh And so I thought today I'd spend some time just kind of uh trying out a few of those things, doing some live coding, using those libraries and show off kind of what some of the Pulumi experience looks like uh for both GCP and uh and Azure and kind of at that intersection with uh with containers and Kubernetes. Um which is where a lot of folks that we've talked to recently are kind of trying to, trying to do interesting things with, with Pulumi. Um And so I'll show you just a bit of some of how that works and some of what is available there in Pulumi. Uh So first off for folks kind of getting started with this, um you know, no matter what it is, what platform it is you're targeting uh with Pulumi, the, the best thing to do is really to come in. Uh either, you know, jump in on to one of these icons here for the platform you're interested in or uh go to getting started and find the platform that uh that you're interested in. And so I'll start with uh with Google cloud platform. Uh And so we can come into the uh instructions here and the landing page here gives us uh some of the key things we need to know some of the examples we can go look at for sort of a full running app, sort of very minimal like, you know, what is the, the simplest possible program look like some links over to the API documentation uh for those libraries. And then, so finally kind of some of how we can configure the providers so that we can uh connect up our deployments to the particular projects and accounts and credentials we want to use to deploy the best. Um And so this is definitely a good place to kind of get started to orient yourself. Um As you're starting up for the first time using any one of these platforms, whether it's AWS, Azure, GCP T open stack or any of the other platforms that we uh we support. And so what I'll do is actually come over here and just kind of go to a um kind of a blank slate and uh and just kind of from scratch uh work through building up uh an example that uses uh GKE. Uh And actually, what I'll do is first show, you know, we actually have a, a fully functioning sort of example here of using Google engine, just click on this and this is in our examples, repo you know, you can come in here, this is sort of um you know, show us how to spin up the Google container cluster, how to uh you know, set up some of the configuration that you might want and then finally how to deploy some resources into that. So we're largely going to kind of recreate some of these pieces. But if you want to see something that's sort of the finished working example of this, I'd encourage you to kind of go just look at that, look at that example, but just to give you a feel for what this looks like going from zero. I'll, I'll start just from scratch here and so I can do something like here, Pulumi New. Uh, I'm in this empty folder, Gke demo. Uh Actually I just bump up the fonts a little bit here. What I can do is come and pick out the oops, pick out the GCP typescript example to start with and this will just give me, um, a very simple uh project to deploy on GK. Now you notice that for the GCP project to deploy into uh I happen to an account that I have credentials for locally on this machine, I happen to have a Pulumi development uh project. This would be dependent on kind of what projects you already have set up. Uh And of course, with Pulumi, you can manage your projects as well if you want to create a custom project and then deploy resources into that. Um But for me, I'm just gonna make sure I use an ambient uh project and one that already exists when we do that, we'll see that. Now I get prompted to, you know, with a simple uh this template, just deploys a GCP storage bucket. I'll just go ahead and say yes to deploy that. Um Just so I get something deployed and can uh can get a feel for what things look like. Uh Right away. You now I have this file as well which describes this templated project that I created. So I'm just importing the GCP library. I am creating a storage bucket and I'm exporting its RL. And so now we've got uh we've got a Google cloud storage bucket deploy. As always, I can also come over here and just see, you know what resources are part of this project. So that uh that storage bucket is right here and all the details about it uh are inside the console. OK? Um But what we wanted to do instead was actually create a GK a cluster. So let's kind of like play around with this and see what we might want to do. So the first thing we can do is to just say cluster equals new. Um And of course, the the sort of first thing you can do here is just use the teens and things to kind of figure out what is available here. And so I type in containers and we see that there's kind of two things so I type in container and now I've got a cluster here so reasonably easy to kind of go and find that, you know, we've recently added examples into our documentation. And so those also show up nicely kind of inside, inside the tool tips here. So as well as me getting kind of an overview of how this works, I can also see an example of kind of what a real use case of it looks like. And this gives me a good sense of how to use maybe this master off, you know, what kind of values am I supposed to provide there? Um How do I do node config that sort of thing? Uh So let me call this cluster. Uh And now let's go ahead and create this. And so, uh you know, there's a bunch of different options available here. Uh One that I'll need to do is set up the zone. Well, actually, you know, what I'll do is I'll just show, you know, it looks like I can call this with nothing. Uh And so I'll just see what I can do with that. I'll say cluster dot You know, um And so I'll just create that and, and deploy the, the, the cluster ID. Um I'll say, pull me up to go and deploy this and it should say that we're going to create this cluster and we're going to delete that bucket that we're no longer using. Um And so we get kind of exactly what we expect there. You should notice. Of course, this is a desired state model. And so that bucket that's no longer here. We no longer have that in our desired state. And so it's going to get removed as we go and move to that desired state. And of course, as usual, I can see the details and see, you know, what um uh what, what setting I'm, I'm passing here for those different uh things. So uh we noticed that it immediately tells us that this cluster cannot determine zone set in the resource and set it in provider level zone. And so the only reason I didn't get a static error here is because I could actually have this provided through some ambient configuration. So I could do you know Pulumi config set GCP zone us central one A and so I could do this to just set it ambient. So anything that needs its own can, can default into that. Um But I would, instead of doing that, I'll actually provide it directly in line here. Um And so I'll say us central 18 which is uh one of the zones available in uh Google, Google platform. Now, what I'll do is I'll just say skip preview here for the next one so that I don't, instead of seeing the preview and then doing the update since I'm kind of in this just uh development mode. Uh I'm just gonna quickly go through and try the updates every time without looking at the preview. I wouldn't want to do this if I had sort of real infrastructure I cared about deployed. Um But as I'm just building up my infrastructure and I don't care if, if something goes wrong, um It's, it's faster to kind of do that in the loop. So, you know, I get another error, that initial node count must be greater than zero. So I can come in here and say initial node count is three. just try that again. Now we're going to go ahead and, and create that cluster. Um So uh one of those things obviously about uh um Google engine is uh these things spin up very quickly. So this should only take, I think it takes, you know, two minutes or so. Um But while we're doing that, let's kind of look at like what the next thing we'll, we'll want to do is so one of the nice things in Pulumi is that we can um as well as creating this GCP resource uh to create this Kubernetes cluster, we can actually create uh Kubernetes resources. And so, for example, I can come over here and say, uh you know, I could say from from Pulumi cnet. And of course, I need to actually make sure I'm going to open up another window here. Uh And I'll just say M PM install at Pulumi Co uh since I'm using javascript here and node, uh I can just install any of the Pulumi libraries to get access to those capabilities. And so, in this case, uh Pulumi ktis gives me access to all of the different API S available in KTIS to use those to deploy things as well. Uh And so now I have that available and we'll see I've got this KTIS here. And so what I want to do is now, I want to actually connect up to this cluster that I'm in the process of creating uh on GKE and I want to use that to deploy things into the knas cluster. And so the first step I need to do is to create a provider. And so I can say new Cotis dot provider. And what this does is it gives me an instance of the provider that is connected up to that cluster. And so I call this GK uh you see, I can do that using a few different things I can say, you know, cluster contact names, space coop config is, is sort of the easiest one to use. That lets me specify the entire sort of configuration I want for that cluster. Now, one slight challenge is that uh on GKE uh unlike on A KS and, and some of the other platforms, they don't actually expose. Um you know, I I might, might want to have a cob config available here, but there's nothing there, nothing with that name. And so when I stand up this cluster, it doesn't expose as a property, the cob config I need to use to access it. So if you go back to that example that I showed you uh it actually had a little solution to that built in. Uh There's some code here that you know, describes I want to manufacture a GKE style coup config uh which is sort of a unique uh style of coup config that that's hooked up to some of the specific things that GKE does. Um And so this is, you know, uh for folks who are familiar with GK with uh with Cobert, generally, this is mostly kind of a normal coup config file or we've just templated in some of the things that we do get from the Cobert cluster. So the uh the clusters uh C A certificate, the end point of the cluster and some information about the context. So let me just go and grab that. Um I'll put it over here. So I'll just say uh the coup config uh is this. And I think I didn't call it, I called it cluster. So let's just um grab the clusters, grab the cluster's name, grab the cluster's endpoint and grab the cluster's uh master off. OK. So now that I've got that I can do KNS config OK. So now I've created this provider and I can use that to then uh deploy resources. Um And so, for example, in this case, uh I can say constant deployment, I'll just try to do the simplest uh deployment I can tones. So now I can say cos dot V one dot dot apps, do you one and again, like we see the completion list here, give me access to sort of seeing everything that's available inside KTIS. Um In this case, I'm doing an apps V one deployment uh and I'll call this engine. Uh And now I can go and see exactly what I need to specify to define a legal deployment. Um In this case, I wanted to provide the um the spec for the deployment. Uh I want to provide the, uh let's do the template for the deployment, uh which will be what I want to actually specify. Uh And in that I want to um specify the tags for the deployment. So let me actually uh do some engine labels. So I'll create some labels I can use here in uh in the metadata. I want to set the labels to be the engine labels. And then I think I need to specify a selector somewhere. Uh There we go selector and I'll select the match labels and your next labels. Yes, I can't spell the word labels. OK. So, uh so there we go, I've set up some basic uh you know, cooper deployments that uh you know, has a selector, which will pick up all of the pods with this label and then a template which will template out each of the pods inside my um uh inside my thing. And so I can say, you know, replicas is two or something to, to stand up two of those. Uh And now I want to actually specify the spec for the containers themselves. And so this is where I'll do things like um is this right? Yeah, the pods back? OK. Uh And so inside here I will do things like containers. Um And then I'll say image engine engine. Ok. So we can see what our name. I also need a name. Ok. So pretty quickly you see there, I got a few of those benefits of, you know, using the ID, getting those error checking on the fly, getting tool tips and help. I can see that this container thing is a is a pod spec I can come in here and I can, you know, click through these things and see exactly all the documentation of what I want to use. So very handy to be able to do that. But now I've created this deployment which will deploy two copies of the engine next container into, into my that I just described. Now, uh we see that that deployment I was doing previously actually succeeded. And so, you know, two minutes, 28 seconds, deploy the uh the cluster. And now if I do Pulumi up to skip preview, what we should see is we had now also deploy. Um oh, actually, there's one thing I wanna, I'm gonna cancel that really quick. One thing I want to do is make sure that I deploy this into my. Um It's I want to make sure I provide this into my uh what do they call that thing uh provider? Ok. I'll call it KS provider. Um So this provider that I have, that is my connection up to cos I want to make sure I use that to deploy into. Uh and I want to do that. So that uh instead of picking up any potential uh cos cluster, I might have available my ambience configuration, I want to specify the specific one that I just created within this program. Uh So let me go ahead and do that. We'll see who created that provider and now we're going and creating the deployment itself. And so we'll see when the deployment stands up, you know, waiting for the replica set to be marked available. A few more things happen. OK. There we go. And now we've actually deployed that. So we should be able to see is uh if I come in here, let's open up the Google cloud console. We're in our uh we're in that same Pulumi development project that I was just talking about. Let's go over to engine, look at our clusters. I think this is the one, this is the one in us central one a that we just stood up. So I'll go in here. Uh And now if I, I guess I've got to go look at the workloads on here. Oh, there we go. Uh So this is that, you know, that cluster we just saw um I'm running this engine X deployment. There's two of two pods that we just described. Um So we can see all that information about uh my uh my cluster that I just stood up and the deployment I did into it, you see, it's pretty quick, you know, just in, you know, a handful lines of code here being able to stand up a and actually deploy some stuff into it all using a single kind of API and that's a, that's a nice thing, you know, I can, of course, now combine that with doing things like, you know, I could uh you know, uh go back and recreate that bucket for example and say new GCP dot storage dot bucket or I could create a new GCP dot um SQL dot database. Um If I want to create a sequel that managed SQL database inside uh GCP, uh I could do that and now I could reference, you know, I could grab the connection strings for that, that database and pass them as environment variables into this deployment of, of my application image. Um It's really easy to combine the best of, you know, your managed services inside GK and inside Google cloud for in this example, with kind of that is, is the, is the API for doing my, my compute. Um So I could just complete this example really quick and, and try and get something that I can show off end to end. I'll just say service equals new uh dot co dot P one dot Service. Um This will give me a, a, you know, a service end point that I can use to target this thing. And so I also call this engine and here I want to do spec uh type make it a load balancer. Uh And I want to uh make the selector. Mm what type of selector expect to be? Uh That's just uh my engine level. Yeah. Um So this will create a load balanced uh service targeting uh those labels. And so let me just see what happens if I now go and stand up. Yeah, 90 ports. Yeah. And I can go again and I can see, you know what is ports expecting to be. It might expect to be a service port which is a port required port and then optional name, node port protocol and target port. Um So in this case, we know that our our image exposes port 80. So I'll just say, OK. Uh So I'll just go ahead and do that. I'll just format this. So we get a nice uh invitation and I'll go ahead and try that one again. This is going to take all the containers that map this engine label, which is these two of above and just expose port 80 of them on the load balancer that we create. Uh And so the service is uh you know, finding the pods to direct traffic to. Uh so it's going to find, try to find those two pods matching engine labels. Now it's attempting to allocate an IP address to the service. So that's because we have the load balancer. We actually have to uh you know, allocate a load balancer in a Google cloud and point that at the service. And you notice that this is a nice thing that we provide this sort of status here. So unlike, you know, using coop control or something to deploy these, where you just kind of uh put these, uh you know, you say throw these at the API server and don't get any feedback about kind of what the status is here. We're actually giving you status on kind of what are the key steps we need to take to get this thing to be healthy. Um And giving that feedback immediately here. And so after a few seconds, we see it uh it succeeds and gets created. Um And so now we've actually got that service running inside our cluster with a load balancer in Google cloud connected to it. It's like an export sort of the um end point for that uh engine X IP. Uh And so I can do service dot status. So the service, the status will actually be populated because we wait for it to finish deploying. The service is actually populated with the running status of the um of the thing. And so I can say status goes to uh status dot And we got load balancer information on there, we got REX uh And then we've got an IP so I can just do that and run this, run this again. And we now should now we need to export data or else it won't work. So that said nothing was changed. This time, we are going to export the load balancer ingress IP address off of that service. We we just created and now we see that it's running at this end point. And if I curl that, we should see we get welcome to engine X. And so yeah, we've actually stood up a service running. You know, this could have been anything I want. I could have stood up wordpress, I could have connected wordpress up to that managed SQL database. You can imagine taking this further. You can imagine taking your own apps. And if you want to get those apps running really quickly inside Google cloud, for example, you know, doing this is is a simple way to take your app, get it running inside a kind of robust knas based environment on Google cloud, taking advantage of, you know, load balancers and a variety of other managed services as part of that. And again, the key thing is here, you have the full flexibility so you now can go and modify anything you want here or you can modify things, you know, in that cluster itself. Uh So there's lots of different capabilities around uh you know, configuring the Cobert cluster and you can go and configure any of the the resources. And so all of that is available from within that one plane there. So OK, so that was a really simple uh example, using Kubernetes, folks also asked about Azure and I thought for Azure, I show something kind of a little different but similar. One of the things I actually love someone actually mentioned this. Exactly. And I'll just install this while I'm talking uh all the Azure package that we have access to Azure resources as well. Uh One thing that folks were actually raised was, you know, talking about uh containers inside Azures. Uh It is actually a cool uh capability, Azure called Azure container instances. And that's the ability to just run a container uh directly in in Azure and get an end point to it without having to do a lot of this work about standing up a cluster and all of this sort of infrastructure you need uh to, to run that. Um This is a really simple way to go from zero to nothing with a container. And so I thought that'd be a nice place to show off uh to show up Azure. Let me do this sort of the, the, the simplest way I might imagine trying to do it. Um So I'll just say next instance here. Uh And I could say new Azure dot containers. And again, we have Azure container service is our name space we find for that. And uh you know, we have a Konna cluster resource here. So Azure has the Azure cos um and we could use that to do effectively what we just did below for uh uh for Google. Uh We can also use a thing called um uh group. So this is a, a container group instance. Uh The terminology here is a bit funny because the product itself is called Azure container instances, but it's published by Azure underneath this uh name uh here uh which I honestly don't exactly understand but naming aside very cool, very cool feature here. Um And so you see the things I can do, I can just specify I want to run some containers and so those containers, I want to have the engine X image. Uh We'll see, I actually got an error and it tells me kind of some of the other additional things I need. So uh it says I need CPU memory and name as well. So let's make sure I provide those. Uh And again, actually, I can do something like this and I, you know, don't know off the top of my head whether these are strings or numbers or what. But I look at this and it says, you know, type string is not assignable to type number. So I it looks like these are numbers. So I'll give it one CPU and one presumably one gigabyte of memory. Um And so there we go. So now that kind of worked except it's now telling me I need OS type location and resource group name as well. Um On my resource, this is something about Azure effectively all resources inside Azure need to be allocated inside a resource group. And so let me just go ahead and create a resource group, uh hazard dot co dot Resource group. Um And I can actually specify the location I want here. So I'm gonna say West us two there. And so now what I can do and almost all resources will end up needing effectively. This, I can say resource group name is resource group dot name and location is resource group dot location. Now, I could have to have this resource be in a different location uh than uh the resource group, but I may as well uh for now keep these all the same. And then OS type I can specify Linux. And again, if I click on this, we can see, you know, allowed values are Linux and windows. So really easy to see that right in line there. OK? So now I'm not getting any errors. So uh that was a quick way to kind of get that inner loop uh Figuring out what I need to do to create that in the next instance. Um So let me just see what happens if I, if I deploy that. Um One interesting thing you notice is that I'm doing this in the same program. So I actually still have that uh that GK cluster and that those deployments and services available inside GK. I'm also deploying resources in Azure from the same program. That's a really nice thing as well, but I can if I do need to manage resources across multiple clouds, I can do that. I can have clusters in two clouds and then deploy things to both of them from the same program. A lot of flexibility in how I do that sort of thing. Now, the way I'm currently authenticating here is just using my ambient credentials on the machine. I could specify explicit credentials as part of my configuration and that's all kind of specified. Um If we come back over into uh these are the pages. So the, all the ways I can configure it. Um So the client id and client secret or you know, the some of the other options I have for, for, for specifying my credentials. But instead, I just want to use the local credentials on the machine so that I can take this and run it in C I, if I want to pick up the credentials that are available inside that environment. Uh But to do that, I'm gonna need to kind of run a login. Uh Azure logs me out very, I think it only keeps these things live for, you know, 30 minutes or something. Um But now I'm logged in, I get those credentials and I should be able to continue to run that deployment using my uh my using my Azure account. OK. So I'm creating that resource group that's, that's built and now I'm going to create the container. So um at the ports in Uh OK. Yeah, so it says ports uh cannot be empty. So uh let me just set the ports uh gonna say port 80. So I'm gonna expose port 80 I'm gonna set um uh I think it's IP address up. Um So I'm gonna expose this actually out to the internet. So I think this allows me to specify public. So the Australian public is legal there. So let's try that again. So what this will do is actually give this a public, give this running instance a public IP address and expose port 80 on the uh on the container to that. So very quickly actually, that that completed, I've noticed that they, they let that complete before the instance is actually uh running. Um But behind the scene, they are going and spinning up uh spinning up some compute to host this instance um and exposing an IP address that's going to load, bounce that. So I hit that IP address that I got back. Actually, it's the wrong IP address. That was my other engine X. Uh Let me just say export constant uh Azure engine X IP equals and let me grab engine X instance IP address. OK? I'll just run that update again to get the output out. And now I've got this new output as well. So let me curl, let me stack outputs. Yeah. Sounds like it might still be launching here. Oh, there we go. No, we hit it. So we hit the end point finally and got back the uh the same uh same content from the engine X image. Here we go, we have now running in Azure um just with, you know, a very few lines of code using Azure container instances. That's a really nice uh just capability of Azure, a really simple thing to kind of show off what the workflow looks like for that. Um But you can also do all the exact same thing we just showed with Google, Google cloud uh using Gog cloud instances. And of course, now, um uh I can go and take this uh if I put the stack can sort of see the details of this stack. And of course, what folks end up doing doing from here is, you know, taking the stack and the stack represents kind of my base layer of infrastructure, maybe my my container cluster cluster, some of my core infrastructure like my databases, that kind of thing. And then maybe they rebuild additional Pulumi stacks, which represents in projects which represents some of the kind of applications they want to deliver. So maybe each application will be its own project which maybe has a few different instances and they can actually deploy on top of that. And so you can split up these applications into uh into a few different stacks as well uh to, to work across those things. So uh yes, that was a quick tour of kind of uh some of the intersection of Azure and uh and Google Cloud and, and, and containers. Um So hopefully there are some sort of interesting uh new things there for folks. Um Happy to dive deeper into any of those topics in uh in future live streams as well. Um Just to some uh some notes, uh you know, we're gonna keep kind of doing the live streams uh weekly from, from the next few weeks. Um So definitely let us know if there's topics you'd like to, to see kind of a little bit more depth on. Uh And also we're um we're gonna be uh doing a webinar uh with, with Aws and with uh one of our uh early uh Pulumi users and customers learning machine uh in, I think it's next week. Uh We'll add it to the, the notes here. Um So definitely encourage folks to check that out um to go into a lot more detail on some of these cases, especially around Aws and Fargate and that sort of thing with Pulumi. Uh and folks in Seattle, uh we're doing our first uh inaugural Pulumi user group uh in February. Um So definitely uh anyone who's uh who's in the area, uh definitely uh drop us a note and, and come and join us for that. Um And we're gonna be hopefully expanding out into some other regions. So if you, you want to host your own user group uh in, in your area or want us to, to swinging by and, and host a user group. Uh Just let us know and we'll, we'll get that set up. So thanks everyone for joining again. Uh, and we'll see you next week. Thanks. Bye.

---
