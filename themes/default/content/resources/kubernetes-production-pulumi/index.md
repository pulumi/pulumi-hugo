---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Kubernetes in Production with Pulumi"
title: "Kubernetes in Production with Pulumi"
meta_desc: |
    Slides: https://docs.google.com/presentation/d/1dxCCNV9d-EcV9oFIfDkBxNukfcEbaRwhn8fGu5KKhSM/edit?usp=sharing 
url_slug: kubernetes-production-pulumi
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Kubernetes in Production with Pulumi"
  description: |
    Slides: https://docs.google.com/presentation/d/1dxCCNV9d-EcV9oFIfDkBxNukfcEbaRwhn8fGu5KKhSM/edit?usp=sharing  Quickstart: https://pulumi.io/quickstart/kubernetes/index.html Pulumi's inaugural Seattle Meetup: In this video, Mike Metral talks about Kubernetes on Pulumi and how to provision clusters quickly in the cloud to get to production.
  sortable_date: 2019-02-22T23:22:20Z
  youtube_url: https://www.youtube.com/embed/uJ9gyCy8Ho8
transcript: |
    Welcome everyone. Thank you for joining. So as uh Eric said, welcome to our new shiny new office. This is our, our first home. Uh welcome meet up here at the office. And uh I thank you most importantly to you all for joining us this evening. As Eric said, my name is Mike Metro. I'm an engineer here at Pulumi, focus on the of our tool chain and to give you a little bit more context uh about me, uh I've had the good fortune of working with many organizations, small, medium large to go into production uh with containers and committees throughout the last couple of years. And I actually got involved in committees like a week after the initial commit. So just on the last like 4.5 year mark for like five years of experience in. Um So as I say, I've been part of my life to this project for last couple years and up here, Pulumi. So that's um with that right. I've also seen the full gambit of how complex and just mind boggling the space can actually be not just to use it but to really get it ready for production. And in this talk, I essentially want to talk a little bit more about that problem space. Give you a little bit more context and background. Pulumi in case you aren't aware of what we do and then showcase how Pulumi can help you with your clusters and really aim and stop reinventing the same clusters because we're all played by the same problems. So let's talk a little bit more about the problems faced, right? In a form of life, we worked at OS and we had our own company that installer, right? It's called Tectonic. And Tectonic was uh it started off as a bunch of confirmation templates on AWS as we had more demand for folks for other like cloud writers. We grew that into something that was uh pretty much a Frankenstein of an installer. Uh cloud formation was very brutal for us. And at the same time because we wanted to pivot to other clouds, we wanted to have a way of pragmatically writing code once and applying to all different providers. So Terraform naturally went itself pretty well for that. Uh That being said as a, as the leader of the field team at co working literally hand in hand with all these companies. I quickly saw that there's not one through them all, there's many different distributions of humans. So cos had their own redhead has their own with open shifts. There's a bunch of other small players in the mix and now, you can see the club writers in the public space have manage offerings. So we've definitely gotten a lot further than we were four years ago, 4.5 years ago. Uh But we're still not even close to being no, if you actually want to use this stuff for production. So that's kind of one of the the biggest pain points, right? Like no customer, no configuration, no cluster was the same because none of the requirements are the same. None of the designs of the applications for staff are the same. So there's no way that we can create a single installer with a common foundation that's going to be able to like leverage public private. And on top of that, the amalgamation of configuration, you can imagine both for the and the on top of it. So we hit our limits pretty quickly uh with terraform and core because as you all may be familiar, it's not a programming language. Like we had a manual internally from our team, we like bullets 12 and three where terraform is not a programming language. And like we repeated that for steps two and three because it really was worth repeating the amount of of lack of logic conditionals, usual kind of bells and whistles you get in the full programing language. Ultimately, what we always wanted out of terraform, but we could never get there. That being said, Terraform opened up a world of differences for us as hard to be able to support many clouds uh both public and on prem and ultimately getting acquired by red. But that kind of shows kind of the pain points of, of really what the state of the art as it is today, right? And so because we wanted a programming language, we didn't really have those, those constructs and uh it so happened that it just happened to on blue and kind of fit that bill. So what is Pulumi, right? B blue is a collection of infrastructures code libraries for your favorite languages that allows you to define deploy and manage your cloud native infrastructure and your and your applications on top all through real code. Um And that is not to be taken away you right? Like you now have the full expression of a programing language and your favorite language of choice, but you also have all the extra add-ons as a developer or an operator you're used to operating with, with regards to like ID E integration type checking, linking, auto completion documentation and even auto completion of just the actual destruction of, of these API types. And so we have an SDK for various clouds that are on uh a couple of different languages today. And it really is multi-language and multi cloud. Not to mention, we have a platform that allows you to essentially run uh this out the gates with your distributed teams uh in a in a method that allows you to really have a delivery platform that protects against concurrent runs and also locks like the recording state files that goes with the resource graph that we build out of your, of your actual pieces. On top of that, we have the SDK um which is kind of a general kind of word for many different loosely defined packages. But at the very bottom, we have the ability to hook into many different providers, both public and private cloud. On top of that, you can control either containers directly on the kind of ecs uh platforms, right? You have service for LAMBDA and you can have core infrastructure like puberty. And then on top of that, we try to have an instruction where it makes sense, that doesn't break the abstraction to the point where you can't be useful with the core resource if you actually need to. But it's helpful enough that it removes a lot of the pain points and it really is about empowering you, your team and your ultimately your platform for how you want to actually operate across various source code tools, languages, uh tool for your C I CD uh and even again, environments for other providers. So let's get to the real stuff. If you took a higher level kind of overarching view of what it is to actually run the ready in production, this is more or less what it looks like, right? And if you're familiar with the O SI model, it's no different. So let's work from bottom all the way up if we just focus on public accounts for this, for the sake of this conversation, right? You have your providers who actually are the ones in giving you your infrastructure on layer zero, layer one, you have the core building blocks that you're all used to working with VM blocks, words, keys, registries, and then you actually have the clusters of KTIS. So the beauty of that right is that with services like Eksaks and even GKE layers 01 and two are given to you pretty much out of the box with the hard part, right? The control plate being administered for you without having to necessarily worry about it. That being said you're left with a vanilla CTI cluster and that is practically unusable for anything remotely for production grade. So on one hand, great, this thing's not gonna fall over and now I have a like a neck to what it does, but I still have to employ all the work refactor, all my services. And then on top of that configure that clusters to my settings and tunings based on whatever those apps that will be running on top of it. So you can quickly see how this gets out of hand, right? For any amalgamation of applications running in a single cluster, let alone across multiple clusters on layer three, right. If you look at each, each of these segmentation as not only a segmentation of responsibility but also as a segmentation of work and time and effort that you have to actually perform. This is just the tip of the iceberg that actually goes into each layer. So on layer three, right, the cluster services, these are the services that are poured to the cluster logging off policies for run time and resource management, making sure that runtime are locked down, making sure quotas and multi tenancy uh bodies of service are actually being met. And then you also have the things like the registry and service measures that some of these providers are providing to you as a manage service or you can write in cluster. Once you get the cluster configured, now it's time to set up all the the services that your applications depend on logging, monitoring, an English controller for North South traffic, right? Service measures kind of bleeds in between both for East and West traffic. You have DNF managers, you have managers, right? And the list goes on and on. And these are just the shared services right now of the cluster and the applications. And then at the very top layer five is your real, real work where it's trying to port your existing stuff to work in this new world, uh where you actually need to properly handle sick term and sick kill and have to have role in deployments and be able to have zero down time. A lot of these things are taken for granted by folks, um especially who are brand new cloud native, and more importantly, brand new new. So how do we leverage Pulumi to help here? Well, in a couple of different ways. So as I said earlier, we're a collection of different packages for various providers, various components of the infrastructure. And most importantly, the various languages for LA 01 through two, we have essentially a package for each of the managed providers um on A W GCP and Azure. Those are the ones that are going to give you the core building blocks, right? The VM, the storage, the VPC, the network, et cetera. But we also have other packages like for EKS and then an abstraction that lives on top of A AWS to make it a little bit easier. So for the most part, we give you essentially libraries that are again extensions for your language of choice to configure each of these pieces. And so now we have complete control of our infrastructure and how we're expressed and format that we want to actually use. And we move up the stack. We also have packages for CNET natively itself. So this also exposed just like in the future packages, we expose the V MS and the storage devices and the networking components in communities. We also expose the same API resource sites that you know and love from the API subsystem. In also, we have a small preview experimental package that I'll be demoing a little bit called the KX package, which is essentially again, we have these, these core resource types in communities and, and the native providers themselves. But sometimes that just requires so much boilerplate code that it just be nice if I can just have abstractions to simplify some of the stuff, either for ease of use, either for ease of development experience or more for reducing the amount of working code I actually have. So layer five, we don't really have anything yet, but we are thinking and always about how can we make the ref factoring of your applications that much more fruitful for you to do it out of the gates. We have some theories but we're not quite there yet, but we'd love to hear any feedback you guys have in that domain. So yeah, and at the end of the day right now, your apps are left to you. Ultimately, you know what goes in these apps, you know, if they're pets or cows, you know, if they need love or not, you need, you know, if, if we can eradicate certain portions of it, if it's ready for cloud uh zero downtime rollouts only you can answer those questions. We can certainly help. But at the end of the day, you own that stack. So although we've drawn this off on you, like I said earlier, we have some ideas as to how we can make this a little bit better that we hopefully drop in the future. So that's all the material I have, we'll ship it to another. So, in these various stacks, right, if I just happen to pull this, uh this slide here, I'm going to essentially strip this down a bit and refactor it just for the sake of this demo. So I moved around aws from the bottom to the top, but that's neither here nor there. Each of these colored sections. Layer 012 and three are different segments that I will be doing and how you can leverage gluing to actually construct these porches for you, right? And we have packages for each of these. Now, mind you, this is just a tiny subset of what it would take to go from zero to nothing in somewhat of a semi production capacity. But nevertheless, at the very top layer three, I'm just running the deployment of an A to B server with a slow balance service and an inverse object. But I still need the ability to configure DN to configure load balance for the in English control that I have in the, I have to have login for both the cluster and the pods themselves feed into cloud watch. And then I also need IM creds because some of these services like the DNS manager and fluent D for cloudwatch need proper creds with aws and Q to IM is a good source for that. So we also leverage on layer one, the core AWS package, we have to create AWS policy policy documents roles and the attachment of that. And then we move further up to staff into pro the cluster and then distribute actual workloads into those clusters so far. So good. All right. So is is a good thing. So let's start with the very. So each of these layers, I have four Pulumi programs one for each layer, I'm going to start from the bottom all the way up. So step zero, we're going to actually provision a Pulumi cluster with EPS. So this is what it takes to actually can we see in the back? No, I don't, right. You think better? OK. Cool. So uh there's more comments in this than code, which is always a good thing, but nevertheless follow along with me. So as you can clearly see um in here, we have a couple of different packages. So let me look at what's in the actual itself. We have a couple of files, these yama files are religious kind of ways for us in ball to help you describe the configurations of the set should have. And the other parts like TS config and the index and packaging is because we ultimately employ the run time in this case, typescript through M PM locally in the machine to actually use the packages and then deploy it to your provider of choice whether that be a public cloud, whether that be or both. So the first thing you're gonna notice is I'm getting yelled at by my vim editor saying, hey, I don't know what these packages are, right? I cannot find these modules. So this is already a great start to what we have with Pulumi just by using code, right? We don't have this in information. We don't have this in terraform, we don't have this in other tools. So immediately I'm out the case of winning in this situation because I get to actually have usual tools at my disposal. But now for the infrastructure and the studies itself. So how do we solve this? As I said earlier, we have uh typescript here and we use node. So uh my package dot JSON is nothing crazy, right? We import a couple of the packages that I alluded to earlier that I'm going to leverage here. I'll do an M PM install, does this thing and hopefully my editor, oops, I believe something. There we go and it stops and complaining so immediately, right? Like these are amazing uh pluses that we get just by using our packages. I could show you a default kind of out of the box and create everything from a VPC to an IM roll to this and that, but that's not realistic for what people actually use, right? Like people already have existing VPC S, they have existing IM. Um uh All right, cool thanks. Um And so here that's on this one. So I have a pre existing BBC, I just specified the BBC ID, some of the I DS that I actually want to uh to provision into. And then in my definition of uh my cluster, this EKS package was which we have in Pulumi essentially uh abstracts the definition of how you're going to create a cluster. You set the minimum size of the worker nodes, the maximum size of storage class. In this case, it doesn't apply to the dashboard. And then I'm going to output some of these variables that I'm going to leverage in proceeding stacks. So if you're following along along the bottom of my team logs, uh I'm going to step through each of these different layers. And so the beauty of these outputs is I can leverage these outputs from Pulumi programs to Pulumi programs. So I'm going to literally pass them on along the line as I need them. In this case, I'm going to leverage the cluster role A RN and against this role A RN that EKS requires for me to actually envision uh this service to Eks. So in Pulumi, if I do preview, I'm going to get basically a planned proposal of what's going to be created, what what resources are going to be instantiated and then how it's actually going to play out if I do Pulumi up, which is how you do a Pulumi update, it'll actually go and execute that plan, but it tells you if you want to perform the update, yes or no or details in details, you get more of a rich depth of what what actually is being created here. So if I if I scroll up a bit and this text is huge, um you see things like security groups, right? You see the actual cluster definition itself, you see some I am some policies, some roles on and on, right? You guys get the idea. So is that output in the Yes. So in the dependency map, right? Uh because everything essentially is but is essentially depends on another object to some degree, we do that in a sense, we create the map. So we create the the tail object first and then work our way up the map, right? And so this presents to you what's going to be created, not necessarily the graph itself, but the graph is actually exposed on our um my exit here if I'm in my dashboard. So this is mylute uh panel and on my cluster itself, which is nebula here, I can actually see the resources both listed and I can see that a resource graph of what all this looks like makes sense. Cool. So if I wanted to order something I do, yes. But since Eks takes 15, 20 minutes provision, I don't want to bore you guys. And I've done the cooking show demo and switched over to one that is working so cool beps to show you that these are uh basically uh they don't stomp over each other. I can issue another plume update on my existing cluster and it'll just yell at me saying, hey, there's something wrong here. Why are you doing running this? Right. Because nothing's changed in my spec for how I my cluster. I have 25 pieces in here. If nothing is gonna change, I have no work to do and it exits. Right. So awesome. I have an EP plus stood up in two minutes. I beat their own time. So now that we've got EKS cluster up, right, we now can move up the stack. So we've taken care of layer zero. Now let's actually take care of layer one. So again, the demo here is an A TP server that's exposed through an English controller. And I have the ability to do external DNS with the DS manager to configure the actual hosted record sets for that I have logging for both the pods in communities and the cluster itself. And then on top of that, I have the ability to do credential through Q to Im if you're familiar Q to IM. And that essentially allows me to enter each of my pods. And then the qim basically interjects on the worker and allows the worker to assume the necessary role for that pause uh necessity. So we're taking care of layer zero. Let's take up this care care of the cluster services, which here is creating a couple of IM policies, some roles and then setting up fluent D cloud watch lobby. So I'll move over to my next too much time. OK. So this may be a little bit hard to read. Um Let me close this. So as you can, as I said earlier, right, you could have output go from stack to stack to stack a stack. And Pulumi is essentially just nothing different than an environment, right? DEV staging test a et cetera. So I have dev stacks across all four of these programs. So each is theirs. This is a stack, I just stood up in the previous one, right? So I can actually reference the previous stack based on a on a path that pull that over tolu and I can get that output from that stack and leverage it in this subsequent program, right? And so if I look at the output from the previous one, I can say a full stack output if I can type and it'll spit out things like here's your cluster name, here's your cluster role iarn the same for your and I keep confit. Don't worry, I'm not flashing any credits, there's nothing sensitive in there because it's still missed. So, but these are the outputs that I chose to, to output, right? You can make this however you want. So I'm going to leverage the A RN because I'm dealing with I am right now. I'm going to attach a lot of these things to the existing profile groups that EK plus stood up. So I'll only close this tab because that's all it really is. And in here, if I scroll down a bit, I have a couple of things. Right. So, remember I got the cup config as an output from my previous cluster in and Pulumi, we have this notion of a community provider. We don't really know if it's a AK community plus, if it's a GK A one, it's a, if it's an Amazon one, the reality is we kind of abstract over all communities. And so all we require to set up a provider in polluting is the cup config, right? So if you use that is all you need. So we specify the coupon fig from the output got in the previous stack. And then we will use that provider in each of the creation or definitions rather of all these different instantiation types. For example, we're going to create a new data space with the name my configuration using that provider and you can hook that writer into all of your resources that you need to openly stand up these pieces. So what am I doing in this cluster services? Right. I'm setting up a new name space for my cluster services called cluster services. And then on top of that, I'm setting up all of my IM. So if you've worked with AWS, all these uh all these I am decorations, look very familiar to you. So I have a policy for Cube to I am I have a role for my cluster that attaches to that policy. So I can assume other roles are my workers. I have another policy for external DNS so we can configure route 53 accordingly. Again, pretty standard stuff here as far as Aws goes. And as you can see, we don't really hide in this behind the scenes, right? We just give you the facility to describe these arguments as you need. We create a role for the certain DNS. And then on top of that, we do the same thing with fluent D, fluent D has its own set of requirements to put logs into cloud was we have a role for it. And lastly, we actually can deploy this uh using what actually behind the scenes here. I'm using the fluid D cloud to watch Helm chart so I can use existing he charts. If you want, I can use existing ya. We have these constructs built into ball to allow you to leverage these things. So you don't have to refax everything into ball from the get go. You can marginally go as you please like it does something as well that you glossed over. Was that the IM roles that those are actually composed of resources, right. Correct. Yes. So this is essentially right, if I actually go into this. Thanks for noticing, right, I ended up creating a class of my own to abstract a lot of this, right? So if you know, you have a particular way how you want to deploy fluent D cloud A as an example, I have a class in text with the, with the, that I need accordingly. I do some, some general checking just to make sure everything's employable. And then I even have another function that says, hey, I'm going to use the helm chart here. In this case, you don't have to use the hem chart. It was just the easiest thing for me at the moment. And if I chase that one down, that one essentially is actually going in and standing up the Kubernetes Helm chart from the API itself where all the helm charter store, I can define extra variables. I can annotate it accordingly. Like I do it here. Remember, fluent needs to be able to configure the logs by Anno with the pod uh annotation. Thanks to Qim. And on top of that, I can uh tweak it however I please and always pass in this provider saying, hey, use this plus the boy into. So that is essentially the gist of that. So let's actually run this one. I will run because that's quick. So again, we're setting up ion policies rolls and on top of that, installing and running fluent D cloudwatch to aggregate both my cluster logs and the pod logs in the cloud watch. So let's look complete pretty quickly as you can see you have natural updates uh given to you live which oh then we're done and it'll spit out all of the completion pieces as you need. So uh you don't have to believe me. Let's just show you uh if I can actually click on. So if I look at all of my, I am here, it'll go and actually provision all these pieces for me. Here are the policies I created. Here's some more rolls and right. And then I have the attachment of this. So this is my instant role where my cluster actually gets uh is permissions that's gonna by EKS we go ahead and attach that additionally with one more, right with the cube to IM So all this is done for me through code. So also we now completed layer one. So back to this one, right? So we've installed the plus through layer zero, we've installed logging and policy for I MS and layer one. Now it's actually go. And so the application services that my apps will ultimately depend on and the English controller for North South traffic das manager for record sets and then logging is already done for us and the good stuff. Let's look what that code looks like. So if I as as I said earlier, you can pass reference to reference from each stack and pull out more output here I pull in even more output from not only the stack itself but also from the uh the stack cluster surfaces that I just finished running and I can then stack this up as much as I want and leverage this output uh to in 10. Let's close this one up in here because I'm essentially creating engine X, here's X, right. I have just like I had a class for flu and D I have a class that I created for X that allows me to provide all the arguments as I need, right? If you've ever used cornets, the reality is these installer don't work out of the box because there's no way we can, we can literally contribute to all the studies you're going to want and to even extrapolate that even further something as, as fixed as engine is not used identified by any two or two companies. So you should ultimately be able to construct the pieces you want and swap those out uh with whatever configuration you need. Here is how I prescribed it. But by no means this is gonna be the way that necessarily use it. But the reality is you can do it however you want. So I create an X English controller with a service type load balancer. I did to do the same thing I stand up Q to I am as another uh data set across all find workers. So that way they can all assume roles. And again, all of these are just simple, uh pretty straightforward pieces of these are all the arguments, for example that I want to configure the actual uh exam run time with here are the reports that I want to open it up for. And then lastly, I do the same with external DNS to manage my record sets. So let's actually run this. So if I look at my cluster, so because I have this output right of the C config I'm gonna output my cup config um this is my previous cluster. So I'm gonna show you this real quick and then I can actually use Q control as I normally would. So right now, um uh well, that's a, well, for some reason, my keep and pig is not working at the moment. I want to show you the, what's the value of new? That's it. Um Yeah, to the side to just, I have to actually declare, I want the cube config out of my output. There we go. OK, we're in business now. Cool. So the Q system is what comes out of the gates and since we already have the cluster services and then you see fluent D, so I'm going to run the application services, right, external DNS and the English controller and ultimately Q to I am. So let's do Pulumi up on that program for the plus their services or sorry, that one for the app services and we can watch this in live as all these pots come up and down. So I have my application services a space which is populated right now with the controller portions external DNS Q to IM and they'll all continue to create in the background. We should be done close. So again, the English controller exposes itself through a low bouncer in Amazon. If you wanted to use the A LD English controller, you can certainly do that. In this case, I went the self serve route where I have controller with the low balancer provision for me that I will use as my single ingress for all of the applications that they will run. And I can actually see all this uh being not only created, but I get the output of that as well. So lastly, with all of these pieces now in place, now I can actually run my applications and right, like you can imagine how lengthy this can be across a suite of packages for all these labs. So now let's focus on layer three to the last and final one. So again, I use the output from the previous cluster. I'll close that one for now. And in this application, this is just my demo. That's all it is. It is a name space for it. It's a service that exposes it accordingly. If you've seen the the spec, this is no different than, than what the usual Houston working with. And here's the actual deployment with the spec itself. I'm going to expose this. And lastly the ingress object that will be picked up by the controller to provision it as far as post pa routing and external DNS which will go and set off appropriate record sets for it on route V three. And the beauty of it is that this is my entire location, start to finish, but this is all I actually care about from the app. All of the other stuff are the requirements that I cluster my applications need. So the breadth of it is pretty wide, that's actually run this now. So this will fire off. Uh I have a little window over here. So I'm uh currently running ahead a look, look up on a host for the English objects. So I have to meet up the apps. The Lulu um is the actual host name that will be ultimately uh served by the engine exchange controller and then passed along down to Mac for deployment. So as this comes up, we're seeing, we're, we're kind of waiting now for the low down sir to be provisioned as this completes it just shortly. Sorry, the text are so huge, you can't really see it well, ultimately, so the English soccer is being created and then ultimately being fulfilled by the controller and external. So obviously, this is kind of where the cop writers really shine, right? You ultimately have to wait for these things to stand up. And even when, for example, on Amazon, when a little bouncer comes up and you got to return we all know it's not really alive for a day or two. So, no different here. It's just facilitated for you. Hold the steps. So let's run this. Let's run this probably a little bit. Let's see what's happening in the background. Oh, here we go. OK. Actually, it did exit. Cool. So I actually output some uh some useful curl commands for myself. But since I'm currently doing an look up on the top right corner, I'm just gonna wait for that to actually return some, some ballad record sets. Uh And then we'll actually curl it. Let's, let's see, make sure I don't know. Um So if I look on route 53 external DNS should have done its job. This thing is huge. So after the application comes up, it takes a minute or two. So no, there it goes. So external DNS actually set the record set for me and an S logo should return successfully soon. Once it does, we can curve come on DNS Cash. Well, while that goes, um I'll, I'll and it succeeds, I'll do a man, but that is essentially kind of the gist of it, right? Like you've seen how I have tackled each of these layers and there are all separate programs segmented by responsibility and really kind of the focus of what it is they ended up doing and I've done it all through code. So I have not locked into any sort of like templating tour matters. From D or, and Jason that I don't want to deal with. Uh and the logic and expressiveness, expressiveness that I have is really up to the author, right? So with that here are all the links that you will need to get started. Um If you aren't already using Rene on moving, um the demo itself is the KX package and in that we not only have the demo um oh cool. This returns successfully. So this should work now and I get food bar. Awesome. So I have a deployment running on top of Cres figured the way I need it and on EPS and in these packages, right? So I'll, I'll flash that again. This KX package is still experimental. Um If you want the more solid grain stuff, our packages with regards to the writers are way more stable, but this library is essentially showcasing the various pieces I uh demo here, right? Which is the English to Q to I am fluent D external DNS. All these are kind of the vision we see as far as the building blocks going beyond tunes, right? Some of these pieces can be rinsed, repeated and included, right? If you can just import this in a lot of code and configure it in one declaration. Well, how are you messing with doing this all yourself, right. Your organization should be consolidating all of your code into, into actual usable components, uh not kind of bastardization of those. So with that, I will leave up the link slide just so if you guys wanna take pictures or wanna actually make a note of it and have the questions.

---
