---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Creating a ‘Cloud-Aware’ Code Solution with Demo | PulumiUP 2023"
title: "Creating a ‘Cloud-Aware’ Code Solution with Demo |..."
meta_desc: |
    Ala Shiban, the Co-Founder/CEO AT Klotho, will share how Pulumi enables a new category of “cloud-aware” developer tools that leverage familiar prog...
url_slug: creating-cloudaware-code-solution-demo-pulumiup-2023
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Creating a ‘Cloud-Aware’ Code Solution with Demo | PulumiUP 2023"
  description: |
    Ala Shiban, the Co-Founder/CEO AT Klotho, will share how Pulumi enables a new category of “cloud-aware” developer tools that leverage familiar programming languages and can lower barriers to entry to deploying cloud infrastructure for small teams that lack dedicated infrastructure experts.   He’ll show how developers can easily create infrastructure from code using familiar languages and without having expertise with a given cloud provider. This enables rapid iteration on cloud applications for early-stage teams who are prototyping and iterating on new products.  
  sortable_date: 2023-07-28T13:03:01Z
  youtube_url: https://www.youtube.com/embed/zqhn0cejVZU
transcript: |
    Hello friends. My name is Ala Shaban and I'm the co-founder of a startup called Clotho which enables writing universal backend applications that run anywhere. I led teams in the visual studio org at Microsoft in the developer division and more recently led the cloud services org at Dr Games. My co-founder Aaron led teams in Linden lab, Los Alamos National Laboratory and at Dr Games, teams need to move fast focusing on building products but products also need backends at at a certain degree of scale or product complexity. Teams need to use the public cloud, whether it's aws or GCP Azure or even something more specialized like cloud flare. The mechanics of building a backend system is not simple and the complexity grows as the application requirements grow. A real application uses databases, caches, multiple application services with non-trivial Toppo while still needing to satisfy latency throughput, reliability, scalability requirements and be audible and secure research and surveys show that the biggest unexpected factor in the slowdown of teams is underestimating the complexity that microservices introduces to developers and cloud engineers. I'd like to introduce you to Kloo Kloo is a new open source tool that can transform plain applications into cloud native ones using a few declarative and contextual annotations that give your code cloud powers. It makes it possible to automatically generate Pulumi code for the necessary components and resources that are needed to power your application in a cloud setting. It then surgically modifies your application code to wire in the usage of those cloud resources. For example, this is a Python application that uses fast API and A RED SDK. And just by adding the persist capability and the exposed capability clo can transform this application into a cloud architecture which in this case has an API gateway, a LAMBDA and an elastic cache wired with VPC SI M policies and then use configurations to have the LAMBDA talk to it. This approach is part of a larger paradigm called infrastructure from code. So what is infrastructure from code? It is a broad term that refers to tools which are able to analyze and infer the cloud resources needed for an application to run from the application code and then creates and maintains those resources using a description language. But how is it different than infrastructure as code infrastructure as code refers to tools that allow developers and cloud engineers to describe infrastructure you write templates or code that describes lambdas or cabernet's clusters im policies, SQSS gateways and how it all wires up together while the application code is external to that and is loosely aligned with that infrastructure by injecting connection strings or environment variables into the application. So how does this relate to Pulumi? And how does clo help freeze developers, especially in earlier stages from having the deep expertise in infrastructure engineering? It allows them to zoom out and think about the features and capabilities they're building while letting Kloo leverage its understanding of architectures to power those applications through using a few annotations. But the key here is that once the complexity gets to the point where you do need to dig deeper into the infrastructure as code or the Pulumi code, you can and you can learn about how these things are wired up just by observing the code that Kloo generates. And so it serves both as an understanding layer for you to be able to observe what Kloo is doing, but also for you to be able to patch it, change it and also attach it to your existing Pulumi code to expand it beyond what Kloo knows how to use today. And it maintains the power of everything that Pulumi brings to the table from stack management to the deployment process, configuration storage with clo absorbing the complexity of building those and translating those application constructs into cloud constructs to show you how this works. I'm going to ask Aaron my co-founder to walk you through a demo. Next, I'll be giving a demo to showcase some of those capabilities. So for this demo, we'll be starting with a plain application. This is a typescript app. And, and the main thing to show here is that there is no extra depend cs dks or anything that we're dependent on. This is uh a native app that will run locally. So what this app will do is it will take an image in this case, an image with a gradient and we extract the text using OCR and return a JSON Pay Lo which has the original base UR and the extracted text as a string. So to showcase that here's our app uh just again, local app, it's exactly the same as what I showed on the slide. And I am going to go ahead and run TS node against our source index file which will start a little web server listening on port 3000. And what I will do is I will curl passing in uh the same exact image that we have hosted on our service. And I'm, I'm passing the result to JQ just to kind of show it nice and formatted. And what you see is you get back the expected result right now. It's all done in one shot, but we have a separate endpoint that will also return the result in this case, pulling it from a, a local ES six map which is storing the, the temporary result as well. So if we, if we get this multiple times, you should continue to get the the same results back. So now that we have this local app what we want to do is use Clotho to create a cloud native version of that app. So let's start with the, the most basic version of that, which is a app hosted in some sort of compute in the cloud being pointed to by a public accessible uh gateway API gateway in this case. So in in our architecture, that'll be an API gateway connected to a Lambda function which will host our entire application. So to accomplish that with Clotho, we will be using our annotation system which allows us to annotate parts of the code. Uh and that will then turn it into a, a cloud data version when it's compiled and deployed. So in this case, what we're going to annotate is our app dot listen, which is the part of the app that's listening on port 3000 for requests we'll be using to close to expose annotation which takes two parameters, the target, which can be public or private. We're specifying public for apps. We want this to be a publicly accessible input and a name or an ID. And this has to be a unique identifier for your app. Uh In our case, we will call this, uh we'll go with the same name OCR gateway. So what I'm gonna do is I will then take this app. I'm going to, we'll be running two commands next against this app. The first is TSC which will take our typescript app and compile it into a javascript app. And the second is the cloth, the compiler which will take the compiled output and two pieces of metadata, the app name, which we're calling Pulumi demo and the provider which is AWS in this case. And when we run this, uh the first one should create a dis directory. And the second one will create a compiled directory based on what Clotho found in the application. And you can see that it's detecting certain things here including the routes uh as part of the, the Express app that we're Grano and that will generate a compiled directory which has everything you need to deploy, run and operate your service. So this has the Pulumi I ac uh along with a very useful diagram which I will pull to the side here, which is the architecture that we've generated so far. So as you can see right now, this matches our expected architecture. I also want to take a minute to pull open the Pulumi code just to, to give you an idea of what's happening in here. So in this case, it's typescript code and this typescript code is everything that Kloo uses or that Pulumi will then use to deploy this app. Uh So this is dynamically created based on your application. We only pull in the pieces that you need and it's all the components including uh I AM policies, security groups, everything you would need for this app to, to run and operate inside of A US. Uh And just to show that that is the case, I'm gonna do a quick Pulumi up just to give you a preview of what this will look like, right? So to start, uh I'll go into this compiled directory. OK. So just to show how this would look, I'm gonna run a Pulumi up against our generated stack and you see that this generated all of the, the components we care about. So it includes uh ECR repository for holding our container images. I AM rules the gateway itself uh along with cloud watch monitoring, et cetera. Uh When you were to, if you were to completely deploy this, you would end up with a public end point at the very end which you can curl with the exact same uh the exact same arguments that we've used inside of our local app and it'll work in the cloud version as well. So this is a, a very basic app but we can expand this functionality very easily with clo. So the, the next thing we would like to do is make our data be persistent to cross sessions. So right now I mentioned before that we're keeping statuses inside of the E six map. Uh What that means is that this is all in memory, which if the process were to die or you were to, to have a cold start of your Lambda, that means the map would clear and you would no longer have any of the statuses that you had stored previously. So the first thing we would like to do is modify this to allow us to persist that data uh between sessions and across line does. So what I will be using here is the persist annotation which will take our claim. Yes, six map and turn it into a dynamo D table. We'll call this uh same, same as before text result. And now I'm not changing the code in either way. We're still just reading and writing from this map. But now it will behind the scenes, turn that into Dynamo DB calls. So if we run the same TSC and close the compile step as before and we have the same image pulled up on the side with our new annotation in place. What you'll see is that it will pick up that text result. And now we have Dynamo DB added to our architecture. Similarly, if I were to do the same Pulumi deploy, you would now see that there's all the bits needed for that Dynamo DB to exist, including I AM rules and everything else it means. So next, let's talk about a situation where your app functionality needs to change and you, you need to make your app handle things as synchronously. In this case. The reason for doing that is because OCR might be compute heavy and you might want to run that in a separate uh unit of compute rather than the API S uh or split it into a separate LAMBDA. So, what we'd like to do is turn our architecture into this. We'd like to have a right path where you write on to this OCR API. Uh It then does the processing and stores the result in text result. And similarly, the read path goes through API here and you can then read those results. So the way we accomplish that in Kloo is by using the uh executive adaptation. So I will go to each of our routers. And um like I mentioned, we wanna make the routers be their own area of compute. So I'll make one here called API. I'll make one over here on the OCR rep file called uh OCR API. And lastly for the, for the piece of typescript that's actually doing the recognized text, which is the OCR operation, we'll make that its own as well. And we'll just call this, uh we'll call it OCR process. And once again, rec compiling the end result after all of that is what we would expect. Now, we have three lambda connected together. One thing to note here is before these were calling each other with function calls and we obviously didn't change that piece of code. So what's happening now for this to, to be able to invoke other LAMBDA? The answer is we take those calls with original function calls and we translate them into R PC calls. So now this is a, a procedure call when you call it, it's actually sending a request across the wire and getting the result things to note here. Uh Nothing in this is, is concrete or set in stone, meaning that you can do things like through configuration change the network protocol being used. Uh Right now, it's rest it could be GR PC as well. Uh All that will be handled automatically and, and will change just on next compilation with config changes. We have this right path and this red path right now, it it seems kind of weird that we have these two lambda on the right path. Like you could have just been one. The reason for that is because um we really want to make this app be event driven, meaning that uh we want to decouple the request to recognize the image to reading what the result is, especially if people were to in into a bunch. So the idea here now is when you send your request to write, uh we enqueue that into an event and the OCR EXEC has a handler for that event which takes the, the event and does the processing. So the way we accomplish this with clot though, uh as we'll begin with a emitter, which is AES six emitter, nothing special here. This is all handled in memory. Uh And emitters allow us to specify handlers and also to Emmit uh events. So in our case, our router is going to be emitting an event. So rather than just doing recognized text and training the result, instead, what we're going to do is pull this emitter in and have it emit a recognize event and pass in the URL. And then we change our results to instead of being the results, just say we're recognizing text. So now when you do a post, uh you should get back this recognizing text And then when you do the subsequent read, assuming it's finished, you'll get the result. Similarly, um let's go ahead and save this. Uh We'll go to our OCR module, which is the, the place where we want to put our handler. And we're going to add a handler for recognized events using the same emitter. Uh I'm adding an import share. And so what happens is when we receive a recognized event with URL, we call our recognized function and that result is then stored inside of down mode DV. And lastly, this won't work in, in a world where you were deploy this through a cloud because we have three separate lambdas. I mentioned an emitters all in memory. So they couldn't pass events between each other automatically today. Uh But we're going to accomplish that by using the cloth of hub set capability which takes this emitter and turns it into an SN uh which lets persist these events in the cloud and we'll call this OCR PEP. Once again, let's recompile and as expected, we now have this SNS hub sub in between our OCR APR and OCR process. So to showcase the, the final cap blows this up, I'm gonna deploy this version to the cloud and we're going to curl the public API gateway end point. Just to show that at this stage, everything is connected and we're getting the expected result. So I'll go into my compiled directory. OK? So next to show how the current state of the app works. If we were deployed to the cloud, we're going to actually complete a Pulumi app which will then persist this app in Us East in native Us. OK. Now that this is completed, we have a URL that was given to us here. This is the URL that represents the API gateway and is a public URL. So I'm going to copy it. So, what I'm gonna do is I am going to come back to this tab where we run our URL S and I'm gonna recall the same request we sent earlier. And all I'm gonna do is I'm gonna come to the local host and I'm gonna replace it with the URL that was returned to us in the Pulumi output earlier. So I'll go ahead and just paste that verbatim. Um Sorry, a couple of typos here and here. So now I'm running the recognize end point on our deployed app. And as we mentioned with the early modifications, you'll see that the result now says recognizing text rather than the actual result. So now we're going to do the same thing on the result. And sure enough, you see that our result actually has the process text inside of it. Once again, this is a, a separate Lambda that's pulling from Dynamo, you need to get those results. Now, for our final update, we're gonna do something that would normally be extremely challenging at this point in a, in a apps life cycle. Uh But it's something that's actually quite easy to do with Clotho thanks to the investments in adaptive architectures. So what we wanna do is we wanna take these two API lambda and turn them into long running processes. In this case, we want them to be pod running and cos or EK more specifically. Uh The reason for that is to avoid cold starts on these et cetera, we still are gonna leave our actual processing as the Lambda. So the way we accomplish this in Kloo is through config when we ran our last compilation, we actually have an additional filing here which we didn't take too close to, to look at yet called the dot yaml, uh which I'm gonna actually go ahead and pull out and what this has is everything that clotho currently configured per app. So a lot of this is defaults based on the fact that it hasn't been specified yet. So what we're gonna do is deplete everything here that uh that we don't want to modify, we're only gonna keep the modified aspects. So in this case, what I'm gonna leave is just this front matter up top at our execution units. I'm gonna take out all of these extra parameters including network placement, memory size, et cetera. And all we're gonna do is modify both of these API S to be instead of Lambda Eks. And if we run our exact same uh compilation as before we're going to pass in our CBO dot YAML as an extra argument passed in with dash sheet. Config the best way to think about this is sort of like style sheet. We're gonna override what's on top of the default. So those things we wrote into this Pogo Dia will override the default that that clover generates. And the end result is that we are given a similar diagram. But as expected, the two Lambda are now UK S pods. If we were to rerun the deploy here, this would do everything to spin up the EPS cluster wire it such that the API K we can communicate into this pod as well as this pod. Uh and everything should continue to work as expected on your next deploy. Thanks, Aaron to summarize CLO allows you to write plane application code, add annotations that are both contextual and declarative and it will transform that code into a cloud native version of it, producing Pulumi infrastructure as code and a slightly modified version a free application that runs and leverages those constructs. It allows teams to have a generational leap in productivity when it comes to back end development. While still keeping all the benefits from Pulumi and the underlying layers and making them plug into your existing infrastructure as code. Give a try at DEV and thank you for your time.

---
