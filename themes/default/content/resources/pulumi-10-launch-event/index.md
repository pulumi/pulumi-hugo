---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Pulumi 1.0 Launch Event"
title: "Pulumi 1.0 Launch Event"
meta_desc: |
    In celebration of our 1.0 launch, here are four great talks, amid much celebration (and consequent room noise -- sorry!) from several members of th...
url_slug: pulumi-10-launch-event
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Pulumi 1.0 Launch Event"
  description: |
    In celebration of our 1.0 launch, here are four great talks, amid much celebration (and consequent room noise -- sorry!) from several members of the community on working with Pulumi in production:  00:01:49 -- Introduction to Pulumi (Joe Duffy, co-founder and CEO Pulumi)  00:10:52 -- Secure, Production-Ready Kubernetes Apps, Productively (Duffie Cooley, Staff Architect, VMWare; and Mike Metral, Engineer, Pulumi)  00:39:23 -- Best Practices for Infrastructure as Code (Christian Theilemann, Senior Software Engineer, Solvvy)  01:01:33 -- Azure Cosmos DB, Microsoft's Globally Distributed Database Service (Rimma Nehme, Architect and Product Manager, Microsoft)  01:17:57 -- Azure Cosmos DB and Pulumi (Mikhail Shilkov)
  sortable_date: 2019-09-30T19:40:08Z
  youtube_url: https://www.youtube.com/embed/lehOTk-n1Nw
transcript: |
    So, first off, thanks for coming. Thanks for enjoying a little bit of our hospitality with beer, pizza. Um We are very proud to celebrate the launch of 1.0. I want to thank our speakers in advance. I'll thank them again, but I know some people travel from far and wide. It's a pretty big, pretty big deal for us. So Duffy and Mike are going to talk about some KTIS apps from this morning and then I think Macau would see work for traveling furthest away. Sorry. Oh, ok, great. How long was your flight? 10 hours. So I think that's quite, quite gloomy loyalty. So how many people are from Seattle? Not bad. How many people are not from Seattle but still from the US? And how many people are not from the US? Oh, it's pretty good. So thanks for thanks for joining us. How many people have actually written and deployed resources using Polloi? How bad, how many people have tried pollute but not actually all the way to production? Ok. And how many people have never tried Pollo? Ok. Good, good education overall. I think we were trying to decide whether to kick off with a little bit of a quick demo before we get to Duffy and Mike session. So with that, I'll hand it over to Joe. Joe is literally going to show me in five minutes, a quick tour around the block. No one gets to the regular talk. So thanks for joining us. I hope you have a great time. We have a bunch of time at the end questions and socializing and all that good stuff. So without further ado, thanks and thanks for coming by. Definitely appreciate everybody coming out and you know, has been a huge rally cry for the whole team. It's really helped us focus on end user kind of basically making it easier to use. I think the key feedback we've heard from a lot of people that have tried to get up and running with me is like they love the idea and it turns out cloud infrastructure is kind of hard. So we've been really working hard over the last couple of months to make it as easy as possible. So, because not everybody knows what Colombia is. I'm just going to do a quick five minute overview of why Colombia is a little bit different than the typical take on infrastructures code. And then I'm going to turn it over to, to show you secure production ready here. So the key thing with Pulumi and you know, Eric and I our backgrounds came from developer tools, you know, we're at Microsoft, we work on platforms like dot net languages like cr for longer than I care to admit. And, but I, you know, when we came to the space, we were super excited about just infrastructure in general, the rapid pace of innovation in cloud platforms, containers, cnet, serverless. But you know, when we started getting our hands dirty, it was like learn another handle dialect that has, you know, go templates embedded in it or you know, a TL that like maybe four loops were coming, but they weren't quite real four loops and there wasn't really any sharing and reuse. And so our approach was, hey, what if we just took general purpose languages and married that with everything we know about infrastructures code, right? Which is, you know, infrastructure code is great because you sort of contain the complexity of your infrastructure by getting previews of things before you deploy them, you can get gifts, you know, it's not like open coding against cloud seks or writing, you know, photo scripts. It really is a much more robust way to do deployments to manage infrastructure. So what we said is, hey, what if you could take everything we love about languages sharing, you can use, I use test frameworks, but we want to take that and actually retain everything about infrastructure code. So that's kind of what Pulumi is. So the idea is right now. So it's a multi language run time. It's open source, it supports typescript, javascript python and go right now. So you basically just off your infrastructure and it is declared, this is one the, I think one of the common misconceptions because it's a general purpose language. People assume that it's going out and mutating infrastructure as you run it. No, it's actually building up a declarative model of your, of your desired goal state, much like a cloud formation, an art template or Terraform HC file. It's just that you're doing it a language, you get functions, you get classes, things like that you can deploy with our C. So we've really made it as pleasurable to use for a CL as possible. Some people said, oh, I didn't know I could do those sorts of things. It's got nice little animations used to have emojis, but the team deleted them because they didn't like them as much as I did. But, but we've also worked a lot on integrating into C I CD environments, especially as we've gone into production with a lot of customers. You know, they're using lab, they're using Spinnaker octopus like all these CD systems. So it's important to us that we weren't trying to replace them and we're trying to actually integrate with them. And then we've got a console and so we sort of built it a little bit differently than existing tools. We built it as a free state back end by default. And then if you want to manage state by hand you can do that. But we've got a nice web app that makes it really easy to keep track of state. So that if you're in a team environment, you don't have to worry about. Hey, is the state property locked and backed up. So this is just an example that you know, once you have languages, you get abstractions, right? And so this example is 27 lines of code, sorry, the resolution isn't quite what I imagined, but you create an EC cluster, associate a load balancer with it, allocate a private container registry, er invest build and deploy a doctor container to it and spin up a load balance service and then export the result of all in 20 seven lines of text script, you run Colombia and it just provisions all this stuff, we understand the language. So we know all the dependencies, we can paralyze deployments, we can do the tear down, you know in the right order. And so this just shows you the power of abstraction. And of course, we support a Azure Google cloud, you can even make it match. That's one thing that makes Colombia a little unique, you know the cluster I think might refer to. But spinning up an E plus for Amazon is actually not very easy. It turns out this cut, which is a special purpose tool. It turns out the challenge is you need to orchestrate deployments between a resources and Cotti resources and cloud formation can't do that. And terraform doesn't really have full support for the COTIS object model. So the magic we've been able to do is mixing these in the same sort of tool with the same workflow, which is pretty I mentioned integration. I think one of the key things with 10 is what does 10 mean to us, it means it's complete. So we've worked with enough customers where we brought it into production lots of start up customers, but also larger global 2000 enterprises. And along the way, we've had to add a lot of integration points around. I did see providers like Sam and director and also see I systems, I mentioned the plug state storage. So a lot because of the default experience, which frankly, we're trying to make a little clearer for our end users. The default experience is to use ourselves for state management. But it turns out you can actually just like with terraform, for example, you can manage it in your own S3 bucket. If you don't want to share the state with our service, you can do that a similar thing with our management system. So you can easily do encrypted deployment time secrets for token and passwords that we actually encrypt it in the resulting state. In fact, we transit encrypt anywhere that that secret goes. So you don't have to worry about it leaking. But if you want to use has or vault or KMS or anything we actually have integration points for all these things as well. And then, you know, I encourage you if you're kind of coming to the system, you're like, hey, I don't find much value in this ad. I'd encourage you to take a look at some of the features that are actually quite handy around things like web hooks. You see a lot of people building slack bots and custom integrations and things like we also have. There is a recent addition, also the ability to import state from existing clouds which most people coming to blooming are not coming from scratch. They have mountains of terraform or mountains of cloud formation. And so we're trying to make that really easy to convert over. So we have a number of kind of options there. And then finally, the major area of focus frankly, for 10, we have a really good reliability, robustness but documentation I mentioned at the start, you know, getting started was kind of tricky. So we now we got over 100 tutorials. We got a lot of user guides, we revamped all of the docs. So if you look at the docs, you know, three months ago and you're like, wow, this is too hard to use. I'd encourage you to take a second look. It's and then finally, I I really want to thank the committee because obviously the reception has been kind of overwhelming. I thought there was a good chance we launch the open source project that people would hate it, that I would use a like a general purpose language for infrastructures code. But it turns out the reception was overwhelmingly positive. I think it really came in at a time where people are getting sick of gamble and recognizing the limitations of a lot of the tools that we've been using. I would say the other thing is a lot of people who work with nowadays are trying to help their developers to work better with their team. And we really see that by using a general purpose language, at least we're all on the same playing field. Sure, there's still going to be specialization. The devo team is really going to be experts in networking and clustering and security. And I am the development team is probably going to focus more on application level concepts like load balancing and services, but giving people the tools where they can make that a policy decision so that it's not this hard wall between the two groups, we can actually sort of collaborate and make share components with each other. That's been amazing to see the customers that have picked up me. And I think that has been frankly more than the technology, that sort of cultural transformation. If you will, has been sort of the most powerful thing that we've seen. We are hiring pretty much across the board. So we're scaling up in all areas. So if you're interested, you can talk, talk to me and talk with Eric. Shoot us an email. We love to talk with you, spread the word that I'll turn it over to Mike. And for those of you who may not know us. So I am an engineer at Pulumi, I work on ready focus portions of our product. And likewise, Duffy does the same at vmware and we shared a former life with some coworkers of ours here. Shout out to and in that time we've seen so many good and bad things about taking protection more bad than good. And we're here to tell you why we think things are starting to take a different turn for the better. So I'll turn it over to. What are you doing? So, um I'm Duffy, I'm Ma on Twitter. Feel free to call me. I'm trying to increase my account. Um I'm actually, I'm talking about Uber news pretty much all the time in this talk. We're actually going to talk about some of the security, some applications on how to think about them. We're going to show you some kind of cool mainline tools or some tricks about things. I'm also gonna give you a quick high level overview of who we need is we're going to talk a little bit about what is in your team we're gonna talk about. Um I'm gonna give you a quick five minute overview of what is CODIS and we're gonna talk a little bit about what is a container? And then we're gonna talk about how you can use uh container orchestration, basically the form of container orchestration that Cudi takes to do some things, maybe that you don't as a cluster operator want people to do. And so we kind of go through those examples and then we'll also show you how you can actually mitigate that. And so this ties into the the application security story by actually showing you some of the controls that you have within Goober and discuss to actually mitigate some of the some of the risks that are just. So this first slide just talks about like all of the I'm right in front of the speaker. So this first slide talks about basically all of the moving pieces of NITAS and we're just gonna talk about it as an application here. So in this slide, so I'm actually gonna step on the other side, go for it. So in this side, we have all the moving parts of a need cluster. So when you set up a cluster in in EK or in some of these other environments, you really don't have any idea of what's happening inside of the dot in box, it's gonna happen outside of your control. But if you're actually standing up a coar need in other environments, leveraging some of the source projects like cluster API or, and you actually get to see all of these pieces working together. Let's talk about what they are just really quickly give you a high level overview. Um The API server, as you can tell by all of the arrows are basically, is basically kind of like the center of the universe with this API server does exactly what you might think it might do, right. It's going to do things like validate resources that are being submitted via the API. It's going to handle things like authorization, authentication, those sorts of things. It's actually also where admission control happens and we'll talk a little bit about that. Like as we move through this, the we have the controller manager, this guy's job is actually to have a number of control loops that basically iterate over and over again and try to break down a higher level of obstructions. Things is called de or stale sets down into pod or for a level of instructions. And we have the scheduler which does what the schedule associates work with actual notes. And as we're talking about nodes, we can look over here on the right, see two clits and a container run time. This is actually where all of the heavy lifting happens. Um When we were talking about pods, that's actually the schedule of the minimal viable spendable resource within the 2 m. Those pods are actually realized on the clit and the clit takes care of the life cycle of that real process running on a server somewhere. That's what the job is lastly, we have CD, which is basically a key value store that stores the state for all of the Cooper clusters or all of the state within. That's a quick overview of all of the components. Let's talk about a flow here. So in the previous diagram, we said, keep at my pod, we're going to actually be doing that live in a demo. I want to talk about what actually happens under the covers when these things happen. The reason I think it's important is because co is itself is a distributed system. And at least a couple of months back, once a pod is actually created, that pod object or spec has been submitted to the API server. A P server goes through the process that we talked about before. Once it's actually allowed admission into the cluster, that pod spec is persistent into the key value store and then the API server returns to 100. OK. You've given me a resource I've accepted it. We're done. The next thing that happens is that the scheduler upon determining that a pod has been created but is not associated with a node will actually take that pot object, determine where to schedule that object and then persist back to the node name field or the selection that it makes. And again, the scheduler is now done doesn't do any more work than that. It basically just populates that node name field based on the predicates that are provided by that spot. Next thing that happens is actually the heavy lifting, we're going to start this process on a clit somewhere. So, Cubit will actually also have a watch on the API server. It will see that there's a new pod that's been defined. It's been associated with this note. I'm going to download that spec and start doing real work. I'm going to start those processes that are, that are defined within those containers and run them on a server and then there's the life cycle of them. And I'm going to report back up to the API server, things like status. One thing that's interesting about this slide is if we go back to the previous one, we can see that there's a controller manager in this diagram, but there's not a controller manager in this diagram. This is a loosely coupled system. I have to find a pod, which is already the lowest level abstraction. This is already the lowest level primitive. So there's nothing to break down into smaller primitives. There's no need for a controller manager. If I had specified within the pod spec in no name. If I had populated that field, I could take the scheduler out of this picture as well because there would be no scheduling to do and we're going to show what this means and minus might be like a security service for in general. That was my five minute Uber need to talk. That was what it is. So next I want to talk about like what is a container? How are you, how many people actually in the room? I should have asked this already, but like how many people in the room are already pretty familiar with Uber needs? Right on. Well, the rest we just got like kind of a crash course override of what of what it is. How many people in the room are familiar with containers? How many of those people can tell me what the container is in reference to like a Linux or a process. We're about to expand your horizons when you log into any Linux host and you type LSP or slash pro the pro file system and then some pit and then the directory NS you'll actually see a mapping that represents what a con is in general, all of those things within that mapping. And we'll, and we'll show this when we do the demo part, we'll agree on the particular name spaces that they're associated with it come. But effectively every process within the Linux kernel has the primitively necessary to define isolation on a process level. All container is all containerization is, is basically just making use of all of those pars to provide a different mountain file system or a different container file system that's shipped with your container or a different network name space associated with just that one pot or a different pin name space. So that when you're inside the container, the only process I DS you see are the ones that are associated with that particular process, right? This is just process isolation. And this is the kind of the aha moment when we think about like what the difference between virtualization and containerization is, right? Containerization is process isolation. If you start the process within a container and that process dies, the container dies. If you start a process within a BM and that process dies, the VM doesn't care. It's gonna keep going in perpetuity. This is the key difference between the two that we think about. We're also gonna talk a little bit about doctor and do so many people here know what Doctor Docker is a few more of them than I expect. How many of you think that doctor and doctor is exposing the underlying doctor socket on a container on a container host to the container? How many think it's actually exposing enough privilege to the container to create container primitives directly here? All right. All right. OK. OK. So this is actually a description of what a container is when you think about like all of the different things we talked about like what they mean a container is a mount file system. This represents the actual doctor image that file system that ships this part of your dark image, a container file system, any shared house. Like if you actually have like persistent volumes or anything else like that, those sorts of things are actually associated with a with a particular container. All of these things are limits kernel primitives that are together represent what a running container might be. And you can do things like you can do things like limit things like my cio things like create different spaces. You can share all of these things within, between containers, within a particular pod inside of it is and then there's also a set of constraints that you can apply like you can define an external capability. So whether you want to be able to provide to the container, the ability to manipulate the network of that container, you, there's quite a lot of constraints that you can define. And we're going to talk about what that surface looks like this. So that was the quick crash course if you're ready this in containers. Now I'm gonna show you some kind of like moving on some things I creeping in front of the stand like right in front of the speaker so that I don't, you are, you are yeah eem shield. So this is a one liner that actually allows any authenticated user without in the absence of admission control to take control of the underlying node. And this is a, this is leveraging the Q Kel command line tool that shows what is and what it does effectively is it will actually just deploy an alpine lix image. It will use N enter to mount the underlying nose files, hit name space and file system and then they'll actually use an essential inside of this container to basically take over the underlying note. This is possible for any authenticated user inside of a Cooper news, but in the absence of admission control. So let's show that off a little bit. What I'm gonna do is I'm going to show you the actual manifest. I'm going to use this and basically, it's just like that one letter, but I've actually just created a container that will actually allow me to schedule this directly on a host. And just like we were talking about before with the direct scheduling attack. If I a no name, I don't have to have a schedule to run it. I can actually pick any note that with the buster to target with this particular man. And I'm going into that container and look at the result of of center is a tool that can be used to enter into any of the name spaces associated with any new container. It's a great tool that really exposes a lot of capability for interacting with all of those primitives that make up an actual container. I center to solve the mystery of how con is actually configured. Like if I wanted to understand what part of actually going to be bound to a specific pod, I might want to actually see like, you know, the results of FSLN look and see like what actual process is actually found. So in this case, what I want to do is jump into an NSX an engine pod and I'm going to look at what process, what is found. And so in this case, the interceptor command basically allows me to enter the network space of that running container inside of my sheet here, basically only by identifying process ID or P ID that is that is working on this particular note. Once I enter it in, I can do things like my cell and which is kind of like the equivalent of that man, I can see that this particular contender is this binding on game. So that explains my mystery, right? It wasn't working because it wasn't designed to work the way I expected. I can also mount to the file system and look at the configuration of these things. Now, what's interesting about this is that I can do these particular things even if the container itself doesn't have a batch or doesn't have a shell. So this is actually that mapping that we were talking about before. So this particular container, we can see the mapping between the seafood, the I PC network, all of those things and a particular number, these numbers are specific only to this pot or this container. If you look at, if you as a host and you looked at all of the rest of the processes, you might see that they all agree on this. But what else do we do here as we talk about Lexus name spacing all of these, all the time. We can also use any center to take over the host without post path. We talk that we haven't talked about those path, but we, so this plan, I'm actually gonna use any center again, I'm gonna enter all of the name spaces associated with page one. Now, what's neat about this is that basically what this allows me to do is get rot on the underlying host. This one liner allows me to actually completely take over the underlying host, right? I can, I I can see the underlying host, I can do pretty much anything inside of the space I have complete ownership of the underline host. And this is possible in any group of news cluster in the option of Yeah, sorry. Yeah. So, so that's what an authenticated user can do inside of without the mission control. They do pretty much anything to be. How do we fix it? How do we actually stop that from happening? One of the only service that we have to actually control? This is a mission control and this gives us the ability to validate or mutate resources as they're defined inside the API S. So in this case, I'm actually going to show off pod security policies and how those things can actually allow us to limit the scope of this particular type of attack act as an emission control for your community. Customers and they allow you to limit things, contra things in a reasonable way about security policies, give you the ability to define things like within the, within the pot. And then, so I set the flag, I could set up pod security policy globally with an investor that says nobody can define privilege flags. And if I needed more than that, then I can actually allow only those pods within a particular name space to have that capability but deny everybody else that particular capability. Same thing with the underlying post file system or host network or capabilities. All of those things I can to train all those with pod security policies. And a now the ordering of prose security policies is a little tough to get around and the US. But generally, what happens is that the ordering of prose security policy application will be validated before new cases, which means that if you, if you have defined multiple policies within the group of the whatever entity it is that's creating that policy. If they have access to multiple pot security policies. The first one that allows admittance without mutating an object will win and only that policy will apply. They not a they don't, they don't accumulate or aggregate. It's just the first policy that applies, that doesn't change the object will win. No validating object, no valid policy applies. Then we will go through the mutated ones and again, even the mutated one, only the first mutating policy will actually affect the pot because at that point, it has been accepted, security policies are tough to work with because they actually tie into our back. We have to think about the entity that we create in the pot and associate security policies with that. I think cooper, we talked about the controller manager, the controller manager is responsible for running things like that. The controller controller actually in those sorts of those engines are responsible sometimes for creating pods and we have to associate the permissions to those pod security policies that we want to apply to them to those specific controllers or to groups of users. So let's do all of this. Thank you Duffy. So as Duffy pretty much showcased, this is pretty right for picking, right? If you're looking to attack a coon buster without having default P SPS in place possibility policies, you're pretty much out of luck. So the beauty of that is that, is that in every single provider, what essentially happens whether I'm running on Azure or they're running eks or if I'm running on G, they all come baked in with their own security policies and most of them are pretty wide open. So they allow blanking kind of policy that anything goes, that is not something you want to roll out of the gates. So the good thing about managed clusters like E and GE and A is that they give you a cluster, but that is a vanilla cluster, you cannot actually use them, right? Not in production. So the cool thing about Pulumi is because we support many SDK across all of our platforms. I've literally defined free clusters and free lines that is pretty powerful. If I want to actually go into the actual actual clouds, I can go and inspect that based on our SKs like on the column on the right, you my E I define my VC that's going to be deployed. I deploy some rolls. I set the properties of the cluster itself, the version of cabernets. And then I'm mapping some roles. If I want to actually take over as a different user. When I deploy this, what actually is given to me is three different few config one for one for AKS and one for G I also deployed a pine cluster. It's a cabs and docker cluster and that is just local. So essentially all you need to work with the is a file. We wrap a provider around that and in bloomy, that is the object that you work with. So this is pretty neat. I don't want to bore you the detail to setting a up, but this runs and you get a couple of customer. If I switch terminals here, I'm not going to deploy something into that. So because we have all these FDK available and because each people fig is wrapped up in what we call a provider, you can now simply put this in a for, hey look, you actually have a real for and in that for loop, we can do a couple of things, we can define our own default security policies out of the gates that replaces all of the blanket allow that all these writers put in and then replace that with something more coherent. That actually makes sense, not to mention, deploy a real application into that. So this kind of shows up how I can deploy engine with its flow balancer into all these different clusters. And so if I run an update, which I've already done for all of us, I get all this output and I can literally go to engine X on Azure, I can go to X on EKS, I can go to X on GE I can go to on my kind cluster. But wait, I'm getting yelled at because I don't have the authority here as an anonymous user to do this. Why? Because my default security policies have revoked all of those open. So the neat thing about that is that we can not only set our own defaults, we can now showcase that we've provisioned all of our infrastructure. I am all the networking, the cluster itself. And then once the cluster is up, we can hook into that cluster and deploy anything else that we need to have. So in this case, we have biosecurity policies pou me by default, ours do not manage cloud security policies. But we have this notion of what's called a dynamic writer, something that I can create a crud interface and I can manipulate to my heart's content. So in this, I've gone in and I said, look for each of these providers, some of them come with default that I cannot strip away. G for example, if I were to remove all these pot security policies from G all of their stuff would break. Likewise, in a very kind of wonky manner. AK comes with its own security policy. But if you try to delete this root privileged pot security policy, they have an add on manager that want it right back in seconds. So what do we do? We have to at least scope it to our capabilities to make sure we have the least privilege and that we have some control of what comes in after the fact because again, the lego blocks that we were given at the provider, they stop once that cluster is up. So it's on us to take the ball from there. So I decided to say these are the required cloud security policies. And if you've worked with KTIS, I decided I'm going to create my own prosecutor policy. I'm going to create my own demo security policy and my own restricted policy. But they are each going to be gated to different groups within COTIS. In this case, only the service accounts and the Q system name space. So what does that look like? So if I go ahead and go into looming, one of the neat things is the code is the state. So just like I deployed the engine next app, if I were to comment that out here, that code is no longer live and oh, and if I were to rerun an update, what will happen is that because they removed the code, that deployment will go away. So if you have a much easier way to work with the infrastructure with the state that belongs to it, and it feels more intuitive than what you want to work with it over these. You're not in a situation where you can just continue to it, right? And so the cool thing is you can do that on. So if you're testing, they say you want to do a testing across all discriminated clusters, you can realize what that would be. And again, the code is the desired state. So that's going to go off and delete and all the deployments are gone now to talk to what Duffy was really getting to is by default, we don't have the greatest security policies. So we make on our own. I created this cluster as the user in AWS and by default, whoever provisions the cluster gets super roots on all of this. So the super route can do whatever they want now to show that because the provider is just ac file, I'm going to show what it takes to deploy a roof privilege pod and the docker and duffer pod that Duffy was talking to here and show that the super route user can in fact deploy this as we expect. So that will go off and deploy in about 15, 20 seconds. And as we expect because I am the super roof user, all of these pods will come up and again, I'm leveraging my cup big file that I created as Super Roof and that's just defined as the provider and it just the property and how you deploy the actual pods or the employment for that matter. So as you see the pod comes up as we expect, I am Subaru, let's go ahead and blow that away and try that as a different user who scoped to more finer grain are back and who's not allowed to do this because they are forced to be restricted security policy. So I'm just going to blow that away real quick just to kind of show you the clean slate, give that a couple of seconds. But the cool thing about that is because we can just mix and match all of this because in Pernetti's, everything is a few config file that gives you so much flexibility on how you manipulate, how you manage and how you actually work clusters. And again, because we can look into a cluster after it's up, that gives us the ability to actually set our standards to set all our best practices to limit actual capabilities or even set proper defaults that we may need. So that will go off and delete the pods from the super root user and that will be done in just a second. And I'm going ahead and set this up for the next user who's finally scoped. So I created an additional user called better mic because better mic knows better than to run in a privileged setting and better is actually scoped with I am. And to be only using this finer rules here, they can only create pause deployments with the sets and PV C within the default name space. Anything else beyond that they're going to get yelled at. So if I go ahead and save this and I run it as my better mic provider, which is my cup file that the better mic has, I will get Hami to show me a little bit more different output. This time around that shows that we can not only bake in our popular defaults and that on top of that leverage our back to tell us that we are not allowed to be doing these things. So give it about 10 more seconds where and shortly enough, we should get a warning. There we go. And it says sorry, we cannot run these containers because the better mic user is not allowed to run a privileged container, they're not allowed to use the host file system. And that by itself is miles ahead of what we would ever have the capability of doing with any other tool. So with that, let me switch back to the slides. So that pretty much does get it on our end. If you have any more questions, there's a QR code, just put your camera at it and I'll show you the link to all of our slides to all the res and look for us after if you have any questions. Thanks. You still here. I'm working in this space. You can, you know, meet anybody at this table. Right, right over here or for or Joe here at the. Thank you very much. So, next up, we have Christian. Christian works at. So and so was one of our earliest customers and Christian is gonna sort of give the, the dos and don's of working with Pulumi. So sort of best practices overall. Uh and some, some of the battle scar, hard fought, hard won lessons they've learned over the past couple of years. Actually, it's been a long time. All right, everybody. So. Ok. Um So I'm actually my name is Christian Timan. I'm a senior software engineer at and uh which is like a 50 person startup. And as Eric was already saying, we are one of the relatively early customers of uh we started using it in August last year and we use it for almost everything on our infrastructure in action since about January. And I just want to talk a little bit how we use it, why we use it and what are some of the best practices that we had at the time? So about me, yeah, super soft engineer. I focus mostly on infrastructure platforms. I make most of our tool decisions, uh provisional infrastructure and make the rest of our engineers a bit more efficient. But by background, I'm actually like a full engineer. I work a lot on web apps and stuff like that. And I just happened to work a lot in the infrastructure in the last year. So um so obviously, we have 50 groups of startup. We do a bunch of machine learning to automate it all in our customer service. Uh Some of our customers are booked up a link um When you get a website of link and you ask a question here uh is actually going to our U I and our back end, which is a bunch of learning and a bunch of websites to go to know what it was to me. I mean, not answer some of these questions on the our stack is primarily on GCP. Um And then the back end we use for the most part um and for and the infrastructure but also the application. So that's, it's kind of important we don't like just get set up. Um But we also play into our economy, you're interested about our stack in detail, you can actually work out. Um how do we, how and why do we use money uh over anything else? Uh So in 2018, we started projects to modernize our infrastructure uh because we had a lot of cost stability and velocity of deployment. And this was not just because we were using, there was a lot of things wrong and the reason for that rise to ya, which for the most part managed by hand, lots of custom backs, a bit of Jenks and some steps which were documented or not or in some way is great. Um A key decision at that time was it was beginning in 2008. So was fairly popular at the time. So we wanted to do something which looks very promising and that was, and to run, uh we decided to use G PE on GP because it takes away a lot of managing the cluster itself. Um And we started uh you know, setting up a bunch of GCP projects uh and this clusters terra form and it looks kind of working fine for the basic things, however, where the things were a little bit more ambiguous applications into your classroom. And we started just using, we looked at found when the audience actually has this out before and who of you actually likes it. So that that's kind of the feeling we have though. It, it introduced a lot more complexity than problems it solves. In my opinion, we looked at case there, which is conceptually great, but it is kind of a language obviously Reflux even tried a terra and they think it's not that great for two minutes. And then we used to actually put one for which is like similar to build with containers, a bunch of Y and poison the cluster. But it's super opinionated and it's really extended for some cases. So in August, I believe me, I tried it out and I was from the get go be impressed waiting on how good it was working for a product that literally just came out of private al two months ago, not too many. And it was honestly at a time, I already be looking better than a lot of the tools which had been out there for 45 years. Um and gave a lot of confidence in the company that it's just like a good platform in battle and completely, I think some of the, the, the two key advantages that I've personally seen is over most alternative tools is that it has removed the best class point. And one reason for that is like you look at the team, they have a bunch of folks which have been working for for a couple of years. They have a lot of language technology. And what they do is actually the o to generate a large part of the STK uh by introspecting the open EP I spec from. And that's when 1.60 comes out in the future, it's very likely that it's soon, very soon in but not in a lot of the other. So uh I mean, just to give you like an example of actually put some links for some also as a provider. However, when the audience just change, you can see they added now supported for a service and somewhere down here they added support for deployments and things like that. In 2018, telephone provider did support his deployment, which is so essential to. So it was not practically usable. They added it in the meantime, but you know, there's always a large delay compared to uh because all of them every day like hand code support for each resources. Um So and, and the next the other benefit is basically, yeah, the other big I just record program, I just touched it and uh you can also spice it but we used to touch where a bit and you know, it's so easy, right? We have to know and use it already before. It's not like we need to learn this, this like I'm a my, my background actually software engineer and I'm used to these like like nature high language and when I started doing all this infrastructure stuff, I was like, why does everybody, we just didn't make any sense is, is great and just to be all the existing ecosystem. So package management for support, you know, jobs well supported by any get instead of and you also get to use a lot of real libraries and a lot of those real libraries, they are not really written for us. Some things are really handy. So if you just get to convert, for example, a toile, a yellow file can actually be super easy job in library. And then before it is yellow file, um you know, sometimes you have to use cases where you have an habitation where you need to your tunnel file in the run of. And yeah, most importantly, kind of basically use the same tool and language both for the core infrastructure and the deployment of the applications. Um And in our case, even the application for itself. So basically one language for oftentimes three different things. Um So with that in mind, just wanted to talk a little bit about some of the practices that we established in our company over over the last couple of months on how you structure your uh projects and deposits. So typically speaking, what you have is a core infrastructure, a positive, you print your PC and clusters and some stuff like the database, basically kind of resources which are shared by a lot of your services and a lot of and I can actually show you pretty looks. So this is actually this infrastructure and we have a bunch of sub directors here, for example, we have like this so that project here which underneath like a project which is actually a project uh which creates the staff and some iron rolls and things like that. And I remember how the project we had this API class that, that actually sets up a class and installs a bunch of like common services. One such service is, for example, cloud, which actually you, it's just like a small uh which deploys a few external DS which is the extension and to a customer and, you know, it looks very young ish, but it's, it's, it's kind of type skip, right? Um And uh I use actually this thing that's called manager class and all of our clusters and I just put it through and say, well, this has made filter and that um and this is quite the actually manages our DNS for applications that we have to point into that. Um the next thing. And then when I wanted to mention what issue, the next thing is you should be aware of this feature called stack reference in polo. So what you can actually do if you have multiple projects or stacks, which just somehow have some dependency in the group, you can actually pass reference information from one to another. And commonly, you have this, for example, in our case, we have like here one project called based environment which sets off our database. And what I'm here exporting is actually like the IP address of the database and some connection strings for certain in the database. And then in our application report, I can just import that connection string in my application. And so that way I don't have to somehow something changed. Maybe I change the IP of the database server in the future. I don't have to create another commit on top and get a, I just read it for that reads a new value from that and I don't have to copy it around. Uh The next thing I wanna talk about like um Jerry speaking, you should create like your own video library for your internet usage. Uh which basically that's a few c best practice and few things that you often use. So in our case, we use GP and GP. So we, a lot of the library stuff is around that. In fact of actually just uh upload it kind of cleaned up. Yeah, example, let me just show you here. So basically we have this library, it's actually really just an M package that we have for roll. And this free library, for example, has something called no app and it has less than the app. Let's look at, let's look at this one. So the app one actually is just a competent resource uh which pulls together a bunch of things. For example, it is a dark, it builds a darker image, it uh creates a container, uh puts that container somewhere down here to deployment. It also creates like public option documents if we are like specify arguments. Um and it also creates servers uh depending on if we have said for example, exposed to and like this kind of, because this pattern happens very often. We work that you have to build, create deployment and creating service on stuff. We've just forwarded that into this app class and inside our, you know, applications, we just import this class for the sake of time we got to skip but we have some GKE utilities uh which are actually pretty handy. Um But I've actually published the whole library. Um So anyway, but in general, I would say don't over engineer this library, don't like to like, don't wrap everything of into some giant class or something like that. Just we just always like once every once or two weeks, we add a new property to this new library which had been used in our case all the time. I'm doing that. But, and last thing, at least you have application of policy and those are basically your repositories where you know, where is your P or not or I code office happens to be deployed as well. And who knows? We actually use this. So you can actually show you that real quick. And um so for example, here, I'm actually a very small demo app that somebody could pull the microphone. So I've actually prepared application and I haven't read any information for that yet, but I'm just gonna create the code to the widest using to be So the first thing I have to just create basically the front. Um And now I'm actually creating a subdirectory in. So that's kind of the convention we had free subdirectory in which I can see. Um Now I'm actually installing for the publishers just on MB. I do want to be able to access it. It's a private MBM instance. But uh as I said, I in that and now I'm in a subdirectory, I'm just being a small index where we say uh if and that out, you know, it has a lot of families. But for the default case, the only thing you need to actually is the your context, which is kind of the working directory from which we will build a dock image and last one last. But on this, we also need a co image. Uh and I could go here and just create a dog. But honestly, the not so some of I know a lot about this is that we put a bunch of normal files into that M PM package itself and I can actually just reference it by saying, you know, no, no. And then reporting that the and this is that predefined and last but not least, the only thing I need to do I need to do which to, I wanted to play a T shirt. Which question our Yeah, that is basically everything I need to do. I think I created the II I to write uh I in the wrong place. And so, so last thing I did is that basically, now I would, the only thing I need to do is like I need to run. And now it's actually one of the play, the play into her Buster and add a bunch of very useful this part and you can see the source. So, so that's basically quite a down point. I guess that's one of these, you should really think about some naming conventions for your projects and stacks. So we, for example, always name our stacks, the dash environment like stash staging or dash product. And we'll use this convention. We introspect it in a lot of cases, the context of which stack we are in and depending on that we deal with some else branch. So yeah, last but not least I think when you work with and also still with, you still oftentimes have to deal with. And it's actually very useful to know that you can convert the very quickly into uh Colombian configuration just by converting it to a javascript Jason, which he time to get the file know from that. So for example, if I wanna use, I don't know like let's say this example here just popping some random stuff from the internet. No straight up you, you probably often find samples in for, for uh for vanilla and in order not to just use this in my sorry. So basically now, so I just used this kind of one liner which is a bit of Python, uh, to convert into a javascript. Very nice that Jason and I think I can save you, you, uh, K as for the five. And basically, yeah. And, you know, basically this is not already for any confirmation. Um, so that's what I do actually. Right. Last, but not least, I think the last best I had shoes, IC I CD system I've tried out about, I don't know, 20 C I CD systems over the last 4.5 years, they all suck in one way or another, to be honest. Um And uh the better ones like the some important future is for me to have like many pro so because we deploy everything, we have new jobs sometimes when you want production, you wanna have the man, like you wanna have the manual staff, uh you do manual, right? Um And that's something called environment tracking where the C I CD system has able to actually track which jobs have to, it's actually a production or a staging environment and then being able to roll them. And some of the better C IC system since I've used the conjunction of or give up C I as your works. And certainly I actually the third, which is kind of an exotic one, but it actually works pretty great for us. Check it out if you have some curiosity and be satisfied with your system and I think that is basically the essence of what I wanted to deliver today. I say here is the repository uh where it contains uh basically this library which I've shown you before. Uh There's a bunch of good goodies there that you can use your own company. All right, thanks. OK. So next we have Mr from Microsoft Mal is kind of do his own thing and they're going to talk about global applications, but on cosmos, so it's kind of cosmos seems beyond global like in the cosmos. And yet they're going to talk about large applications at scale. So please pay attention to some great stuff in here. Thanks. So my name is Rima. I'm an architect and a product manager on Cosmos TV. We are going, I'm gonna speak for about 15 minutes and just give you a little bit of an idea of what is cosmos TV, how does it actually work, you know, behind the scenes? And then Mika is going to show how to build apps, land scale apps using coms. So roughly, you know, introduction system model, global distribution, we'll talk a little bit about the resource governance conclusion. That's my part. And then so we started Cosmos TV, actually about nine years ago inside Microsoft and originally it was known as Project Florence. Those of you who have ever been to Florence, there is this famous dome which which is viewed as basically the beginning of the Renaissance. And we wanted to start the Renaissance for data. That's why we named the Florence. We made the service generally in 2017, it is classified as a Ring zero or foundational service inside a what that simply means is it's automatically available in all of the Asia regions by def so whenever we announce the region, whether it's a public, it's a sovereign cloud, it's a government cloud. We have to be automatically there because there are other services that are built on top of us. So we about tens of trillions of requests per day. Given that we already announced a region in Africa. You can actually take your cosmos account and span it across all of the seven continents worldwide. So basically create a database that is spanning across North America, Africa, Asia Pacific and still view a single system in the data inside Microsoft. It's also became ubiquitous, you know, services like office 365 Xbox Universal Score teams, linkedin and now we're also on boarding. So when we started, we wanted to build database designed for the cloud in a sense, if you were to forget about 40 years of legacy called relational databases, how would you design and cloud the data platform? And ultimately, we wanted to provide capabilities such as global distribution because cloud is ubiquitous. So your database, your data platform should be ubiquitous wherever your users are. Second is provide the elasticity and unlimited scalability with respect to two dimensions, both storage as well as. So you should be able to elastically go from just a few gigabytes into terabytes and eventually scale into petabytes. And talking to that your computational needs may vary over a period of time. So in retail phenomenon like Black Friday, all of a sudden you have a when you launch a game, you want the game to be successful. So you got a lot of users coming in and you want to scale up. So you want the data platform that will last and last but not the least is provide the cost efficiencies with very fine grained. And this is the to basically take multiple, basically back them on the commodity hardware. This resource isolation, we've adopted resource governance and performance isolation and provide the guarantees in terms of performance. So these are kind of like the three core principles that we had in mind when we started the service, the design goals were number one is to elastically scale throughput on demand across any number of Azure regions around the world within five seconds at the 99% time. The other thing is also to provide a near really fine access to your data. So what that means is deliver less than 10 milliseconds and clients over read and write on average. It's actually a lot faster for reads. You will see somewhere between 1 to 3 milliseconds for rides, it will be somewhere between 4 to 5 milliseconds offer five nines read and write, provide tunable consistency models for developers. So this is known as the trade offs or pass up trade offs being able to precisely the right trade off to in the trade off, not just with respect to high availability but also as well for no brain obviously. And then last but not the least, this provides strict performance isolation between both transactional as well as analytical work. So something that we will announce very soon, it already pre is being able to run analytics and global scale has also built the scheme agnostic engine to support basically unbounded scheme. So this is ultimately what result? So given all of these design work at the end of the day, we have the core of the service which is providing the capabilities like global distribution elastic scale out guaranteed low latency anywhere around the world consistency models. We also give you the multiple data models to work with because they're built on top of the same core. All of these properties are applicable to graph to Jason documents, to call their family key value and what not. And ultimately to access your data, you should be to pick the choice because we want to meet developers wherever they are coming from and give you the experience or the applications they experience as if they're talking to native mong or Cassandra. But ultimately, they are taking advantage of the cloud native. So if you're coming from relational background, we have a S A if you are coming from mong background, if you're using a Bo Caan dry, you Casandra. This one on the, this is the ability to take these capabilities and take them to. So cos becomes globally distributed with automatic feel over with the multi master capabilities. And this one is the to be able to run operational and real time analytics anywhere around the globe because spark is coated with your globally distributed weapons. So what is the system model? Let me quickly walk through it a beautiful site. But ultimately, the tenants, the customers, they bring their data could be depending on the data model or it could be either tables or collections of graph cosmos containers, disclaimer. It has nothing to do with the GREs containers, it has nothing to do with darker containers. We are not very creative in terms of naming. So we call them cosmos containers. Ultimately, this is your unit of horizontal scalability. You pick a partition B A in terms of local distribution within the region and then global distribution across multiple regions. So customers can associate so customers can associate one or more as regions with their cosmos account. And then we perform the global replication across multiple regions supported A PS are mobile, the sequel gambling and all of these capabilities are applicable to all of them. The containers themselves think of them, they are completely schematic bags of. So whatever the data you throw with cosmos will happily absorb it, it automatically index all of the. So what does the system actually look like behind the scene? This is actually the physical view so that you don't think it's magic. But so if you take earth, you know, these are regions we have, we're running today plus 54 regions worldwide. And the control plane is basically strapped in the data center, the control plane, the control plane itself is fully decentralized and its state is also replicated inside the pa increasing scale. You if you zoom in on Azure regions within the regions, you will find the data centers. This is where we have the data center within the data centers. We're running across by 11 to 20 frames so that there is a partial outage or somebody has to something happens. Tornado hurricane, your replicas and your partitions, racks of clusters, you'll find here clusters, both compute and storage. If you zoom in on the individual machine, this is where containers that belong to different tenants are core residing on exactly the same machines. So this is multi tenancy at play with the proper resource isolation. We can again sleep back containers that belong to multiple tenants on the same machines and perform and give the resource isolation. So the containers themselves of replicas, the replica basically has a complicated database engine with mission control research government. And within the database engine itself, you will find the typical components of the database engine plus a lot of other stuff that one is the actual storage engine. The fact that we are storing index growth on the column store optimized for both operational as well as so the global distribution. This is the experience that we provide to the users. You can come in click on replicate data globally. We provide you with the world map and you select where you want your data to be, then you click the save button and it's so at any point in time, you don't need to pause your application, you don't need to redeploy, you select the regions where you want it to be. And that's you can also specify the priority list of the regions. So that in the unlikely event, if there is a unless there is a regional outage, basically all of your rights will automatically be the secondary in the priority list. So this is what we call global distribution by virtual, just turning on what you want to say. This is a little bit of the insights, but you can think of it. The partition is basically basically for your data. When you go partition your containers, we shard them using partition keys. Each partition is represented by a forum of replicas that is represented by where the data is stored in both analytical and transactional data store. We dynamically self adjust membership of forums. We have built in resource governance leader over election and we enable basically very flexible split and merge the reputation on these partitions. Well, this is um what the actual global distribution looks like. We go and look at our telemetry. Basically a graph that and you can see basically all of the weeds and rights are always local through the region where where your patient here is. So you should always look at your or your a functions with the region where what is associated with the cosmos. That's how you get the lowest it's multi masters. So your weeds and rights are always happening locally. So RR zero, the SDP that we provide are mobile forming aware. So if there is a regional outage, the client is intelligent enough to be able to go into a secondary region and be able to run them across. So as far as the applications here oblivious to potential regional failures or you adding and removing regions, you may only notice the impact of the this is the consistency models. So if you are really fascinated by this subject, if you look at the traditional market of operational databases, you will typically notice what we call a very extreme binary choice. On the other hand, most traditional databases give you strong consistency with its own trade offs of higher latency, lower availability. But you get the most traditional most ey give you also the three intermediate consistency models. We actually have a plus stacks for all of the consistency models we develop. So you guys can go check them out and it basically gives you a very, very well defined options in terms of navigating the precise trade offs in terms of performance, latency, consistency and availability with failures or network conditions. So you can get the right resource governance. This is another very important component of the system ultimately. So Cosmo DD goes with a provision throughput model very similar to what dynamo DD offers. But the difference here is our request units, abstract, abstract, basically physical resources, like percentage of memory, percentage of CP percentage of bios. And then there are operations that you're running. So you don't need to do separate capacity management for reads or for rights, all of them are applicable to all of the database operations that you're running behind the scene. Basically your replicas that belong to the partition, get a lot of the budget of our use and we provide basically resource governance to guarantee that when you ask that you want to perform 10,000 transactions per second, you have these resources fully allocated to you and they are completely ring fenced from all the, all the time. So you are getting the resources that you got. OK. This is actually just I took the snapshot from about two years ago now. It's a much, much larger numbers. But you can see just the scale that we're riding in within a matter of three days. We're supporting services running three trillion transactions in just three days. This is a tele view zoom in on one of the clusters. This is multi tenancy in real life. So think of it this way, these are the clusters that are in region central us with the replicas and partitions belonging to different tenants all located on the same cluster serving. This is zooming in on the individual machine, how we only partition resources and allocate them to different tenants and within a single machine, how the resources are properly isolated to serve. So in conclusion, you know, Cosmos is Microsoft's globally distributed database service is one of the foundational services. We've battled massively with our internal work flows, multi master replication, global distribution, fine grain resource governance, partition management are the core foundational principle of the service. And then they are precisely defined multiple consistency models providing a clear trade off the and the latency and the throughput availability is something that we as the engineers inside cosmos are super, super proud of it took almost 10 years to get to that level. But this is really exciting we're also hiring. So if you are excited about this mission for cosmonauts, you can go to the site cosmos TV dot com would show how to use Cosmos to build and applications. Thank you very much. Uh Yes. So what so now I want is our greatest database of all time and then show in just 15 minutes how we can use it to actually build the application in Asia globally distributed and using all the power of customers. And of course, so I will start simple. Let's say I'm developing a service and a micro service and ecommerce application. I want to show a product on a web page or something like that. And I started simple. I only have two components in my application at the moment. One is and one is as functions which is a service service in Asia. And for starters, I just deployed to one region here in the US. I use for this on. So that's how my typescript program can look like I start with the resource group which just says the name, it's like a container for resources in measure. And then the location of that resource group is going to be defined by the configuration of my staff so I can to deploy different regions if I want to. And then I have a resource component for Cosmos. And the important bit here is how I can figure locations you can see based on the name, the locations that it could be in a. But for now I just use 11 location I deploy somewhere in the US. And then I need to define as a function for my app. And I chose to use a component called Even subscription. And the cool thing about this component is they can define the code of my application right inside my Polo program. So I just have a javascript function I do whatever I want. And Polo will take care of packaged as Asia expects it to Asia. I can figure my application exactly the way Asia wants it to be. And I just read Jaws can function, request a response and do whatever I want there. So another cool thing there is like in the body of that function, I can just use the variable customer account that I just find the and use it as a just any other Jaco variable. I can get keys from my connections to just out of there without taking much care. And Colombia will be smart enough to start deploying with Cosmos and getting the keys from that account. Pass to my function package it to a zip file, put it on someone's storage and then load that function up to use the file. Hold on for me, I just write function. So this is a first version of application I run for several minutes. I get to resource group Cosmos account and then several resources which are required to run a fun app, a storage account, a zip file consumption plan and function itself. So now I can write my public page works and I'm sitting here in the US. I'm a happy user. I sit close to the application so I'll get the best possible experience. But my ecommerce side is global. So I have customers in Europe, I have customers in Asia from those locations. It's not going to be that great like from San Francisco I might get 100 milliseconds but from Europe, from Germany or Hong Kong, I get like 400 milliseconds. If I have multiple subsequent requests, it starts to build up a week and I get worst experience for, for the outside world. That's not what I want. Of course, I want my customers to be happy. So let's make the distribution, the application distributed. So I started with and I will not be clicking the portal to make it distributed. I'll just change my poli I make one extra configuration which list all the locations where I want it to be. And then with typescript, I create Jan area with three. And then the other thing I need to do is to map this area, an area of objects which will be compatible to the Cosmos account resource. That's again just the one called the map function. So I do that I run up again, but it doesn't help. Of course, because my customer still talks to Function app, it doesn't talk to my customers to be found directly. So it still makes around 32 us and then us function app talks to cos that seems to be a terrible mistake. But when the infrastructure is being, when cosmos be managed by one team and managed by other team, it's quite easy to make this mistake and real production application. So let's see how we can fix it. Instead, I want to deploy function app every region, the same reasons that I deploy cosmos and then I want to configure routes and so on. Use them from Asia to the closed location, use it from Europe to the location and the U and so on. So I can take so many reasons I want, I just need to copy the infrastructure and make it front and round and choose the better one. So my plan for the second iteration of my app, I have this on the back end which takes care of the hard stuff of the, distribute in the data. And then I copy my function at 10 times and I put the Asia traffic manager which is like the space service in Asia in front of it. And I say please route it based on the nearest location in terms of Colombia, I already changed degree. So I need to change function app. Now I just have a look I look for and for every location. I create 11 subscription. The subscription is gonna be exactly the same except that it's in a different location and then it connects to a different region for the cost. And in the end I get an end point and I need to use this end point to configure the traffic manager and they find the profile which is like global thing which has the last name. And, and then again, I have a loop which goes through the same location area, face an end point and traffic manager and then it links this end point to the proper function just by using the idea of the result. And that's it. When I deploy the problem is fixed, the latencies are good forever, the world just where my customers are. That's not the end. I go to my other team and say, I explain what I need. And they say, yeah, we need exactly this. But we don't use as functions for this. Let's say shopping cart service is using go and then they package it is a doer and put somewhere and as you can, but they want the rest of my application to work the same. They still want to be global. They still want to use cos maybe my price team has like a million lines of Java and the application on Iran in one specific version of operating system. So they have to deploy the vision machines and they put it on the scale in front all the network security that's already hard enough. But now they have to make it global. It's quite easy to make a mistake there. So how can I help them? Well, I can notice that there is a there configured for customers degree. I have a partner with project manager and then I have some compute which does stuff in their room and provides ad P part which I should connect to so I can make an abstraction. And as I would do it with normal generic purpose programming language, I can create a component for this. So I can create a component which is called it's a really simple one. I can pass a resource group, a list of locations and then like a come back function to provision the customer services for the use case. So the callback will do two things, create some global resources specific for the use case. And then for every reason, it would create a copy of infrastructure in that region. So to explain, I'll give an example for my fun obvious case, I don't need any global infrastructure. I just can go back again. So instead of going myself, I delegate is the component and I just, I functionally sprays in one location, right? I guess it's not available but I have two blocks here. One great Asian container results registry makes the image, put this document in the container registry. I can still do everything inside my program and then I have a call back which again uses the javascript closure to use that container register. One cost has to be called to create as container instances and configure it properly to connect the right region, then use the right open and the end it just returns against my component has to know. And for my pricing team, they create all the infrastructure which I but if you consider it's like a dozen of resources, just it's still it's much if they have to do it themselves on a global distribution. So a couple of benefits that I get from this, I can also evolve this component separately from my end users. So let's say I didn't need the right distribution for my product service because products they are not changed by, by, by end users. But then my shopping cart team asks I just create an extra option and I can distribute a new version of the application of component of the and then everybody can benefit from it. And let's say I want to swap traffic manager with like another service and as a front door which is like a city and they have locations around the mountain, they get traffic and throws through Asia networks so it might be faster for my space. So I just from one service with the other inside the component and make a new version and then everybody who can, who wants to use it can just update their packages and basically don't have to think about this that much. So perfect. So we looked at the power of cosmos to be to solve the problems of the students and data around the world and we can, but it's still not to create applications which would be globally distributed that would embrace this power of the. So we can use infrastructure to this configuration properly written once and that is still doing multiple teams. And we can also create extractions which will codify some best practices in life. So not everyone will have to think of them the day. Just share it. I feel like a so if you want to know more, there is a blog post which outlines this a little bit, there is a component of my which does exactly this and then if you want to start with, there is not good. So, thank you. I think we are done and good for beers, right? Definitely. II I don't have much to say. I want to thank the speaker again. I think it was definitely tough to speak over the the do, but I think he did a fantastic job. I definitely learned a lot and we recorded it. So it's gonna be up online. Um Anyway, just thanks for coming. This is the first of hopefully many, I mean, we're planning on picking up, you know, around the world. We're already seeing a lot in Europe already. Um So hopefully see you pick up that you give it a try. Download the slack. We'd love to have a part of the evening Thanksgiving coming here.

---
