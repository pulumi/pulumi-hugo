---
preview_image:
hero:
  image: /icons/containers.svg
  title: "What is OpenTelemetry? | Cloud Engineering Summit 2021"
title: "What is OpenTelemetry? | Cloud Engineering Summit 2021"
meta_desc: |
    Microservices have broken monitoring tools and practices. Traditional methods of application logging and host-based metrics can’t provide accurate ...
url_slug: what-is-opentelemetry-cloud-engineering-summit-2021
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "What is OpenTelemetry? | Cloud Engineering Summit 2021"
  description: |
    Microservices have broken monitoring tools and practices. Traditional methods of application logging and host-based metrics can’t provide accurate and timely signals for issues impacting production. OpenTelemetry solves this dilemma by providing a single set of APIs, SDKs, and automatic instrumentation tools that give you the ability to understand your distributed system and the performance of individual services within it.  Talk by: Liz Fong-Jones Watch more sessions at https://pulumip.us/37bqXgl   0:00 Introduction 1:41 What do we still struggle with? 3:08 How does observability help? 5:13 Observability isn't just the data 6:31 Metrics, logs, and traces, oh my! 8:12 Vendor-neutral instrumentation 9:20 Components of OpenTelemetry 10:33 OpenTelemetry creates the needed data 10:59 Regardless of which language or tech you use 12:36 Context Propagation 13:32 Automatic Instrumentation 15:37 The OpenTelemetry Collector 16:09 Start with auto-instrumentation 16:58 Pick a telemetry backend with confidence 17:26 Instrument manually to get the most value Add custom attributes relevant to business 18:21 Use observability data to level up 19:04 Ways to get involved
  sortable_date: 2021-10-20T23:00:49Z
  youtube_url: https://www.youtube.com/embed/2Dg2m5a-RHo
transcript: |
    Hello and thank you for coming to my talk about open telemetry and observ ability. One of the problems that we have on our plate as software developers is that we waste a lot of time on break, fix work, work that is reactive rather than proactive and doesn't necessarily move the business forward. We waste over 17 hours out of every single week because of technical debt and bad code. According to the stripe developer coefficient report from 2018, why is this? It's because we're tasked with doing three different things all at the same time and it can feel really, really overwhelming. We want to both ship features, more of them faster scale to meet you or demand and decrease down time all at the same time. And we're really afraid that things are not going to work correctly in production, that things are going to break and that we're not going to be able to figure out why. And when we have slow feedback loops, the problem gets even worse because when you've forgotten about what you developed because you turned it over to another team to do integration testing on and it didn't actually release production until two weeks later bundled with 100 pe other people's changes. You'll have no idea why the release was broken. You'll have no choice but to roll it back. But the good news is that progressive delivery culture and dev ops have solved a lot of these challenges by enabling people to have faster feedback cycles and get code deployed to production faster. But that doesn't necessarily solve the problem of what to do once your code reaches production. How do you keep your code running successfully in production? This is a problem that we still struggle with. We struggle with a lot of technical complexity where different microservices are releasing at different rates, faster and faster, creating unknown failure modes that we couldn't have predicted in advance caused by interactions between users and services including noisy neighbors and multi tenant situations and our traditional monitors no longer work. So this is where observ ability and open telemetry can come to our rescue and help solve the problem. Hi, my name is Liz Fog Jones and I'm the principal developer advocate at Honeycomb. But the hat that I'm wearing today is actually that of an open telemetry governance committee member. I am part of a vendor neutral body that is aiming to standardize how we measure and report data from our production systems to enable developers to move faster in production. I'm an elected member of that of that governance committee and I act on behalf of all of those inflammatory rather than just my particular employer. So today, we're going to have covered what's painful in software development, what observable is and how it can help and how open telemetry plays into the observable picture. And then finally I'll give you some starting points as to how you can get started with open telemetry to advance your observ ability journey so that you can get back to shipping code with confidence faster. So let's talk about what observ ability is and isn't observ ability helps us answer questions about our systems questions that the previous generation of technology did not allow us to an answer. Things like what are the common characteristics about the queries that timed out taking more than 500 milliseconds and not just the dimensions that we thought to break down by previously, but new combinations of dimensions, things like language pack browser plug-in, software version data center and to combine all of these things together along with the full execution path of the request rather than being confined to measuring only by predefined dimensions. In order to achieve observ ability, we need to have both instrumentation data and the capability to query it. And I'll tell you more about that in a minute. So there is research done by independent researchers that shows the impact that monitoring and observably have when you advance to observably and not just have monitoring. The accelerated state of Davos report reports that 2016% of teams are elite in the year 2021 and those teams are 4.1 times as likely to integrate observ ability into how they run their services rather than just attempting to passively monitor their services. Solving that problem of the 17 wasted hours per week. Teams with good observable practices, spend more time coding according to the uh Dora research groups accelerate state of DEV support. Additionally, open source technologies such as open telemetry is part of the journey towards becoming an elite team, elite performers that meet reliability targets are 2.4 times more likely to use open source technologies. According to the Dora reports, accelerate state of DEVO report and what do they mean by an elite team? They need a team for which they're able to deploy multiple times per day on demand. And that the lead time from writing a change to having it running in production is measured in hours, not days or months. So you may have heard this before that observ ability has something to do with logs, traces and metrics. The answer is that telemetry data is part of the picture, but it's not the entire picture. Observ ability is about our socio technical systems. It's about the ability of our people to gather the systems to answer important business questions and to get back to doing proactive work. This means that our developers have to be able to write high quality code that's instrumented correctly from the beginning to measure its progress through the systems to debug it in production and then to understand how users are actually utilizing the feature and finally to manage and eliminate tech debt so that we can make sure that our systems are scalable and maintainable into the future. So yes, we do need instrumentation in our code. In order to produce that data, we need to be able to store it. But most importantly, we need to be able to query it to answer questions such as why is our bill taking 15 minutes rather than 10 minutes or why is this particular set of users seeing it should be five hundreds. That's what really matters to our business and not necessarily having a Pokemon situation of collecting all of the different telemetry types just because we can. So let's talk about those data signals though, because they are important to consider your data comes in many different formats. In particular, you may be used to measuring metrics which are aggregate summary statistics that tell you data points such as how many requests did you have in the past five minutes that took between 506 100 milliseconds. You also may be used to diagnostic logs which contain detailed debugging information emitted by processes. But what distributed tracing aims to do is to provide a causal relationship to give you insights into the full life cycle of a request time together the relationship between different bits of your code that are running at different points in the request workload, regardless of how you wind up with data in the end state. The reality is your systems are executing code in order to execute user workflows of some form. And you can have those workflows emit structured data corresponding to those units of work. And then we're able to synthesize metrics logs or distributed traces out of those pieces of structured data. So that's how I encourage you to think about it is that you can choose to imitate it in the format of metrics or in the format of logs or in the form of distributed trace or maybe potentially in the future to utilize new things like continuous profiling everywhere. So you have to be able to generate the data, send the data somewhere, store it and then visualize it. And that's where open telemetry comes in as a vendor neutral solution that enables you to correlate multiple types of telemetry together. So open telemetry arose as a vendor neutral project whose goal is to make telemetry simple for end users. So you don't need to change or re instrument your code every time you decide to change back ends or change vendors. It's also the combination of two previous open uh open standards, open senses and open tracing that were both trying to achieve the same thing and realized that they would be better off if and the ecosystem would be better off if we joined forces together. So open to Lary supports tracing context propagation and metrics today and has emerging work around logs as well as continuous profiling signals. These projects have been in the works for a while. Open Tracing was founded over five years ago in 2016 and Open Telemetry is formed as the merger of Open Tracing and Open Census in 2018. As of when I record this in October of 2021. Open Telemetry's specification has reached general availability and open telemetry is available in some of the most pop popular languages as a generally available product. So open telemetry consists of a number of moving parts that I think are all important and all work together to produce an optimal result. The most important part about open telemetry is that it's a cross language specification that no matter how many different languages you use in your project, you can use open telemetry as one seamless set of API S that work consistently together and transmit data in a consistent manner in order to measure the properties of your system and get the debugging data that you need. Additionally, there's also the open telemetry collector which is a Swiss army knife that enables you to ingest telemetry in any format, whether it's produced by open telemetry libraries or not and to emit it in a variety of different supported formats. Third, for every language, there is an implementation of the specification both in terms of an API that code can call by end users or by library authors as well as an SDK that actually takes those incoming API calls and turns it into pieces of telemetry that gets sent out over the wire. Finally, there's automatic insert patient libraries that make it very easy to get data flowing without writing very much code. So referring back to that earlier diagram, open telemetry creates all of the necessary data so that you can understand your systems, understand where they're going wrong or where they're working correctly. But we're relatively agnostic about what you do with that data once you've produced it, and that way we preserve interoperability and flexibility for our end users. So open telemetry also works regardless of what language or technology you use. Not only do we support light everything from java and dot net to node in Python, but we also even have emerging things such as a cli integration. So you can wrap your shell scripts with open telemetry. And that will enable you to understand the performance of your shell scripts and performance of any applications that are called via shelling out. So what do I mean by the API S? Well, let's demonstrate what the open telemetry API S look like in Gola. For instance, what open telemetry provides is a mechanism for you to say I want to trace this particular library and I don't need to care whether or not the end user has set up open telemetry or not. If the end user hasn't set up open telemetry, it's still safe to call the API and it won't do anything. But if someone has configured the SDK, then they'll immediately start getting useful data from my library. So within one of the functions of my library, I could take the languages, standard context object. And then I can say I want to create a span and close the, the trace span when this function terminates and I might set parameters and attributes in order to provide instrumentation that lets people know what a what function uh calls were made and what their attributes were that were passed along. When you initialized that call, this enables people to have a fine grained understanding of what your library is doing and what it's spending its time doing in case it gets slow or starts submitting errors open to lure also implements the W three C trace contact standard, which is a worldwide web consortium standard that says this is how we define what a trace id is or a request id and what an individual uh trace span id is. And how do we transmit that information from process to process and from web server to web server, API server to API server so that we can understand the full flow of execution. And not just that we're also able to understand things that we want to pass along all the way down to individual end points. For instance, where did this request come from so that we can make smarter decisions about routing along the way. So, distributed con propagation is a crucial part of open telomere because it enables us to correlate requests no matter where in the system they're happening. So that when we collect that data in the end, we're able to understand and retroactively figure out what happened. But you don't necessarily have to manually write instrumentation if you're an end user. Because open telemetry aims to make instrumentation ubiquitous. So we've written wrappers or directly integrated with common frameworks in each language to make sure that you're able to always have an active trace context if you're using for instance, express and no Js or the H GDP libraries and go. So you're able to just say, hey, by the way, wrap this function with this automatic instrumentation handler and we'll take care of the rest. We'll take care of recording the HTP status code, the duration, the uh method name. These are all things that will just automatically record. But if you have something additional, you want to add on, we're always happy to give you direct access to the trace spas, you can add in your own attributes. So when you define the API calls, that doesn't necessarily mean that you're emitting open telemetry data right off the bat. That way it's safe for anyone to compile into their binary. But the SDK implements trace and span collection and actually starts collecting that data and then you can configure exporters to send data wherever you like. And that could be anything from an existing format or the new open telemetry format, which is a common language that open telemetry speaks regardless of whether it's the collector or of supporting back end or whether it's the uh SDK producing the code and the collector is important because it, as I said, it's a Swiss army knife. It proxies data between the instrument code and the backend services. And that means it's just a config change, not a code change to recompile if you want to try out a different back end. So in terms of export, as I said, the open telemetry protocol is our native native proto buffer JSON format that enables people to have one common way to import and export telemetry data. But we're also cross compatible with things like Jaeger, which is an established tracing product or prometheus, which is an established metrics product. The collector is able to receive things in whatever format, potentially enhance it with information about the current resource. And it can even do things like drop personally identifying information or t the data to multiple different backends. In order to make it as easy as possible for you to have data portability, you can also run it as a side card in order to collect local information about the currently running container or pod. So what do I do? Knowing about open telemetry components. Well, let's talk about what the journey of an open telemetry adopter looks like if you choose to adopt open telemetry. The first place I encourage you to start is to start with open telemetry's automatic instrumentation, especially if you're using a language like Java, you can go ahead and install the open telemetry agent which will automatically hook into your server and will automatically start measuring your GR PC server or your HGEP server. So attach the agent to a few tightly connected services within your infrastructure. You don't have to deploy it everywhere and configure the exporter to send data somewhere immediately useful. For instance, you can send that data to an open to entry collector that's located on as a sidecar on your local host. And then you can immediately route that data to a back end. For instance, you could choose to use Yeager and Prometheus which are open source software or you can use the generous free here of a large number of vendors that support open telemetry. And it's just a config file option in your collector to choose where to route the data. And you can try out multiple providers as your needs grow because it's just adding additional stanza to say, hey, I want to try out lights step or I want to try out honeycomb. And then after that, the thing that I recommend is adding custom attributes that are relevant to your business to start measuring smaller units of work if you discover that request is taking a while and you don't have visibility into why you may want to add a trace span covering part of that unit of work to understand where is it spending its time and how can I optimize it? You can also start implementing across multiple services beyond your initial seed set of services and potentially do conic propagation for everything ranging from your Amazon load balancer or Google cloud balancer all the way down to things like event driven services C FA or even use open telemetry's newly adopted project SQL commenter which enables you to correlate your application traces to the slow query log in your database server. So there's a wide number of ways you can go with open palm tree because it's so extensible. And as you evolve, you'll be able to use observ ability data to make your systems run better and to make your teams function better use observable data to level up with things like service level objectives. So you can measure the impact to end users as measured by real user monitoring and then to be able to use uh distributed tracing to debug those issues all the way down to the service emitting errors. You can also graph the chart of services to understand. Do I have dependency cycles? Do I have single points of failure? And then finally, challengery data can be useful in order to understand, should I potentially automatically remediate and drain from a bad availability zone or roll back a problematic release. So that's what I have to share with you today is how open telemetry relates to observ ability and how observably can make your life easier by making it possible to understand who is impacted and why they're impacted in the event that you're having unexpected or turbulent conditions in production. If you're interested in learning more about observ ability, you can go and visit the C MC F Observable Technical Advisory Group on the cloud native computing Foundation Slack. You can also check out Open Tole Mery on cloud native computing Foundation slack or visit our website open telemetry dot IO. I also encourage people getting into service level objectives to check out open slo which is a new standard being developed to use telemetry data to measure service level objectives. Finally, if you're interested in seeing me speak more about open telemetry, you can go and look at my Twitter or the open telemetry, Twitter or Twitch. And I'm always happy to meet people over a video conference if you'd like to spend 30 minutes pairing or asking questions about how to adopt open telemetry and how to make your life as a developer more productive and to sleep better at night. Thank you very much and I look forward to taking your questions and to conduct the uh fires site chat. You can find me at honey dot co slash Liz and I'm Liz Liz the gray on Twitter. Thanks.

---
