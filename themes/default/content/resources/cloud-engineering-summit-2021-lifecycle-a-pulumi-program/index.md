---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Cloud Engineering Summit 2021: Lifecycle of a Pulumi Program"
title: "Cloud Engineering Summit 2021: Lifecycle of a Pulumi..."
meta_desc: |
    Infrastructure as Code provides a way for teams and companies to standardize the way they manage and secure applications. In this talk, Fariba Khan...
url_slug: cloud-engineering-summit-2021-lifecycle-a-pulumi-program
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Cloud Engineering Summit 2021: Lifecycle of a Pulumi Program"
  description: |
    Infrastructure as Code provides a way for teams and companies to standardize the way they manage and secure applications. In this talk, Fariba Khan and Stephen Van Gordon will share how they leverage a custom state backend with SSO, RBAC, and programmatically configurable pipelines powered by CICD tooling and the Pulumi Automation API to drive IaC at Apple.  This framework of tools enables teams to provision secure-by-default Compute, Storage, Identity, Ingress, and other components available in multiple languages in very little time and without any manual interventions. This experience is complemented by operations-friendly workflows previewing infrastructure changes between deployments, as well as cost and policy violations directly in Github comments. This results in reduced cognitive overhead when making changes to a deployment. Finally, by providing the state store for IaC stacks our team gains insight into usage patterns, security issues, and compliance via rich data and analytics.  Talk by: Fariba Khan and Stephen Van Gordon
  sortable_date: 2021-10-20T22:59:34Z
  youtube_url: https://www.youtube.com/embed/5U7q9miKWj0
transcript: |
    My name is Steven, my colleague, Riba and I are excited to talk about the work our team at Apple has been doing using Pulumi. Here's what we're going to talk about today. First, we will share a little about our team with Apple cloud services. Then we will discuss some of the challenges to maintaining engineer velocity when working in the cloud. Next, we will talk about some of the foundations that we provide to help teams be productive on their cloud engineering journey. Then to make things concrete, we will look at a case study of a few plumy programs that bring up a service managed by a small team that service also happens to be our state back end. Finally, we will share a few recommendations based on our experiences free. And I are members of the hybrid cloud engineering team. A part of Apple cloud services. Apple cloud services builds managed services, first party cloud resources, build automation tools and developer experience products for Apple engineers. Basically, we focus on taking the complexity out of the cloud. We use infrastructure as code principles to increase engineering velocity and help Apple engineers follow security and cloud engineering best practices. Next, we'll talk about some of the challenges to maintaining engineer velocity while working in the cloud. Based on our experience, we saw that teams were encountering similar sets of challenges on their cloud engineering journeys. When resources are fragmented across many different cloud providers and types of resources, the number of tools and interfaces that engineers must learn increases, exponentially velocity is increased with repeatable processes without any one offs or bespoke infrastructure which fragmentation doesn't allow. We want to treat our stacks and resources as interchangeable. We strive for ephemeral stacks and environments. We want to be able to bring up and tear down stacks and environments without side effects, security is hard and infrastructure is hard and putting the two things together is harder. There's a lot that people need to do to make infrastructures care, but they also need to know how to make their infrastructures care. This becomes harder with fragmentation. When engineers are working across different interfaces, as the number of stacks increases and as the relationships between them becomes more complex, we need tools to manage these relationships with searches on reputability. We want engineers to be able to drive updates from the ID. It also touches on fragmentation. We don't want engineers to have to cobble together a disparate set of scripts and L for making critical production changes. And when engineers are making that critical production change, they want to be confident that their change is not going to have any side effects or break something unexpectedly, we don't want teams to be in a situation where there's a risk that a small one line config change could bring down a production cluster. Best practices are not always documented and they are rarely trivial to implement. It's difficult enough to learn a single cloud ecosystem as we work across multiple cloud providers. The problem only becomes more pronounced. Our team wants to enable engineers to run their infrastructure without having to be an expert in everything. Now, let's look at an example of some of these challenges in practice. Think of a typical scenario for a service having to rotate its enterprise DNS load balancer certificate. This is critical to keep the service not only secure but live. If the certificate is short lived, frequent rotation becomes a maintenance burden. If the certificate is long lived, it is easier to forget when it has to be rotated. If the certificate is mis configured or expired, the service will go down. So for personal experience, this is something that can keep you keep you up at night certificate. Rotation should be simple, but it can easily involve a number of complex steps. An engineer might need to go to a dashboard, create another certificate and then make sure the new certificate has the correct properties to match what's running in production. For example, San D and then the engineer must upload the certificate as a co any secret and wait for redeployment throughout this process. If a mistake has been made, even a simple typo, it brings the service down with the tools and automation. We have set up using Pulumi the program and the pipeline automates this to be risk and toil free frigo will talk about this workflow in more detail later on. But first, let's go to the foundations of our ecosystem. These are what we consider the foundations for maintaining engineer velocity. Our team has built higher order components for common patterns. We have brought first party cloud resources under plumbing management and we also provide templates of common resource patterns. As a starting point for Apple engineers, we publish policy packs that reduce the cognitive load of following best practices and helps teams follow security guidelines. Now, while our published SCKS and policy packs solve some of the problems within a stack like reputability, we need tools to help bring together different stacks. The plumy automation API has turned out to be a powerful tool for a team to orchestrate engineer workflows both for program development and for CCD environments and production pipelines. As teams grow their portfolio of resources and services. We've found it is important to think deeply about how to design and architecture programs and stats. Let's take a closer look at these foundations. Starting with the SDK. Our SDK contains security reviewed and production ready cloud resources. Examples include security compliant multi region object storage buckets or clusters, the components, our team has built, have safe insane defaults. Engineers don't need to learn the implementation details or become experts in a given cloud environment to follow those practices. These components also work together and can be reused and composed to create new components. Our multi region bucket, for example, is built from our team's bucket and bucket replication components. These components are also used as building blocks by other teams. For example, the Kriti cluster is used by other teams to build their own clusters tailored to their specific use cases. Another thing we've done with our SDK is bring first party cloud resources under management by custom plumy resource providers within Apple. There is existing work on using terraform providers to manage first party cloud resources. Our team brought those resources into the Pulumi ecosystem and we've also written new providers when needed. There's much existing tooling around integrating terraform providers into plumy and we've used that to build upon. We use TF bridge to generate the language SDKS and we use the existing plume tools to generate documentation. Some of the resources that teams are now able to manage using pluming include global server load balancers and enterprise certificates. This allows Apple engineers to use our SDKS to manage both their first party and third party cloud resources using plumbing. There's a single interface for managing these resources. Engineers now have a repeatable and safe process for updates. Teams are able to manage more resources and we are able to support more teams. We provide a set of starting examples for Apple engineers which we've made available to teams through the Pulumi new command engineers can quickly get up and running on Pulumi or quickly get a piece of infrastructure up and running. For example, we provide a template with an Apple Cooper Dennis cluster that is available both in Python and typescript. You can go from zero to a real life case cluster in 30 minutes, which I think is pretty cool. We also provide multi stack examples as reference implementations for orchestration of complex resources. Providing these templates reduces the cognitive burden for learning. A new resource or provider enables us to scale out and support more teams and helps disseminate best practices. The hybrid cloud team has created policy packs that we distribute to Apple engineers. The SDKS are secured by default but they cannot validate runtime usage. The policy packs provide checks and balances for stacks at runtime. One thing we do differently is making policy packs available through internal N PM registries. This provides a central place for internal Apple engineers to share and contribute to policy packs. There's a few examples here. The cost estimate policy provides an estimate of the cost of the cloud resources in a stack. Engineers can see if an update would cause a major shift in resource usage and teams can avoid surprises if something is going to cost more than they thought. There's also the object storage policy which checks that storage buckets followed best practices. And the end config policy which sanity checks an engineer's environment for reputability of updates. These are building blocks. Our team provides for infrastructure provisioning. They can do a lot to help engineers with the management of the resources in their stacks. But the truth is in the real world, if we work with projects that are more than just individual pieces of infrastructure or even just individual stacks, teams need to orchestrate these stacks, they need to have deployment strategies and they need to have testing strategies to talk more about the tool that we use to help manage these challenges. Free is going to take a look at a complex service that we've brought up. Thank you, Stephen and hello everyone. I'm Fariba. I'll share our teams uh how we share program stack and pipeline design principles and walk through our usage of the tool built with Pulumi automation API through this service. This is our state and policy managed backend architecture. We deploy hybrid cloud service with the cloud SDKS. We have built in-house using components from the SDK and deploying on first party and third party cloud across two regions. We run in an active standby configuration. Our goal is to automate everything so that we are able to bring up an ephemeral environment for testing at any time. That meant we couldn't have one offs in our programs or stag design and we had to have repeatability across regions and deployment environments. We had to develop our testing and promotion strategy, consider safe database chema migrations and manage communities and data store life cycles. Based on this experience, our team developed a set of recommendations for program stack and pipeline design and built a tool for Apple engineers with Pulumi automation API to make this a lean continuous deployment process. Let's go through these recommendations and see how we apply those to our service. Our tech team came up with few strategies that added to develop our productivity related to designing Pulumi programs. The first recommendation is group things with a similar life cycle together. If the service architecture has both resources that are frequently updated and that are rarely updated, they should be grouped into programs accordingly. This reduces the blast radius. If an update goes in an unexpected way. For example, our team recommends that Kris clusters with infrequent updates and Kris deployments with frequent updates reside in separate programs otherwise they'll create toil and risk as the stack is regularly updated for deployments. When we group resources with similar sta cycle, we know that an update could only have affected a certain subset of the infrastructure which makes it easier to reason about things, grouping resources with similar life cycle results in more meaningful depths. Core changes are targeted to a specific part of the infrastructure with a quieter depth and fewer resources in it. It makes it easier to understand what the changes actually are. This also provides more confidence when doing an update. Engineers don't have to scan each dish to make sure that they are only making the changes they intend to and related to this use stack references, the stack outputs and references should build an explicit contract between stacks. For example, a cube fake referenced in a deployment program is an output of a commodities cluster program. Let's see how we applied this to Harbi cloud service programs. The first program provisions are global load balancer object store and certificates. So it's very rarely updated. The outputs of this program are referenced in many other programs. The structured data store has an active passive configuration. So we decided to put it in its own program, accumulative cluster has a slightly different life cycle. We rotate the images pretty aggressively for security reasons. The regional load balancer is its own program. This imports the regional endpoint domain names, the monitoring parts for vector part cori cluster in a program and they depend on the program. And finally, the service which is updated the most frequently and depends on many of these components is its own program. We also have a process for making safe schema changes to the data store. As part of this service program, we recommend to make CMA changes along with your deployments. So in this case, you wouldn't want to have a separate pipeline for this. Our team also has recommendations on how to create stacks from these programs we mentioned earlier, we recommend stack references to pass information between stacks including the environment name in the stack name can provide a way to parameterize stack references. For example, a config variable can be used for environment and it can have values DEV test prod. Now to reference the DEV data store, the stack reference can be to the stack DEV dash data store for resources that are regions specific. We recommend to also encode the region for our service. We also encode if it's a Canary or a primary deployment for hybrid service. The GS LP and the data store have their own stacks. The regional load balancers, the Karri clusters have separate stacks for each region and the service has even more stacks with cannery and primary in each region. Once teams have their programs and stacks, they actually have to think about how to orchestrate them in continuous deployment pipelines. We keep a linear relationship between stacks and deployment pipelines stack may have downstream dependent pipelines. If so, triggering a stack update will trigger updates in all of its downstream pipelines. Trying to determine which pipeline need to be triggered can be a challenge. The trick here is we do not try to determine whether an update is necessary in a downstream pipeline. If a pipeline is downstream from an updated stack, we'll always run an update in that stack. If there is no changes, it's a no up. This simplifies the orchestration logic. For example, if the load balancer certificate is updated in a stack, the stack will trigger the downstream regional load balancer stack. But the load balancer in point does not change. Uh So the downstream stack update will be a no up for hybrid curve service. We have three production environments. So the set of stacks are repeated in each of them. We have a pipeline for each stack for each of these environments. That means we have 40 stacks for one service and same number of build pipelines. There's a lot of stack to configure in a built environment. Our team has a solution for that. Enter Pulumi automation. API our team has built a tool with the Pulumi automation API to help Apple engineers manage the life cycle of their stacks. Apple engineers can point the tool at a source repository with the Pulumi project provided some conflicts and it will run through a planned Pulumi update. For example, in the sample config file here we have few parameters. The refresh stack will make sure that there is no drift in this stack before any upgrade is run. The preview stack option will enable Pulumi preview and corresponding pull request commands for the update. And after update, it will run update again with the expect no changes option to ensure that the code and the stack are stable. The automation API allows us to use code to define things like the exact sequence of Pulumi steps to be run and config and environment variables to set, taking the person out of the loop at run time helps ensure repeatable bills. Using the API. We also post the results of an update back to the PR internally. The team uses the same tool for automation. These tools makes it easier for us to support Apple engineers because they're using the same set of built tools as everyone else. We also use the automation API to drive testing of our SDKS and examples. And a fun little aside because the service we are running is a state back end. The automation API also drives the acceptance test of our own service. Let's look at the developer workflow for updating resources. With this first, the Apple engineer raises a peer to make changes to the program. This is a simple change as seen in the depth rolling the version of the service forward, the PR triggers a pull request belt, the pull request belt is set to initialize a Pulumi preview with the preview. The hybrid car service gets a new state file with the corresponding proposed changes along with policy back reports. The TF and the policy pack reports are posted back to the source code repository as apr comment. Now the team members can review this along with code changes. Not only that the review also gets update that resources for other programs like the global load balancer, the monitoring pods and the K cluster resources have not changed. This builds confidence in fearless deployments. The Pr March pipeline finally triggers the changes reviewed by the team and the resource changes are provisioned in cloud providers. Now, let's see how the hybrid card team takes this all to production environment. Each environment is tied to its own git branch and every change that could eventually end up in production has to always go through the previous environments whenever we want to change something, we create a branch off of the deaf branch. We do our work and open a pull request. The pull request triggers the corresponding de branch pipeline as we saw in the deployment workflow. Let's look at the case through reading search in the global load balancer object store and certificate program. After the death of the stack runs all the pipelines depending on this will be triggered. And you remember there were plenty. The GSLB program is the one that almost every other program is referring to through stack references. But the good thing is each deployment goes through a validation test. Once the test passes, the next pipeline is triggered. If the test fails for some reason, the changes are not propagated anymore. For example, here, if the GSLB stack failed, other pipelines will not be triggered. The incorporation of the validation test to be part of the pipeline has been a big automation gain for us. We recommend this highly the service deployments are also triggered in order if the deaf service in cannery in region one failed validation. The other service pipelines would not get triggered to get these changes to test. We have a promotion pipeline that merges the Deaf branch to the test branch and then the test pipelines can be triggered with validation in both regions in depth. There is confidence to promote this to test and then to correction to get to production. We use another pipeline to March test to prod while this process may take time with preliminary refresh, previews updates long pr B build times it's worth because we are confident through our repeatable develops process to wrap up. Let's summarize some of the learnings we had in infrastructure as code. We suggest making an explicit goal for building ephemeral environments and identify automation opportunities build best practice components according to the needs of your organization, encode environment name and stack name. This is a big value add to recognize what environment and stack is impacting just with the clients group resources with similar life cycle in a stack to reduce blast radius and have meaningful depths. This is a big developer, velocity gain defines stack contracts with outputs and references. Our team highly recommends the Pulumi automation API for C I CD and finally avoid one of items to make your programs and pipelines uniform. For example, we didn't have to have two regions for our DEV environment. But this allowed us to have uniformity in programs and pipelines across DEV and production environment that wraps up our presentation on behalf of our team at Apple Steven. And I thank you for listening in.

---
