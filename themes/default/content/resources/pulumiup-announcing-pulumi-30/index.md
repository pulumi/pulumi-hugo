---
preview_image:
hero:
  image: /icons/containers.svg
  title: "PulumiUP: Announcing Pulumi 3.0"
title: "PulumiUP: Announcing Pulumi 3.0"
meta_desc: |
    PulumiUP is for anyone who is interested in cloud engineering, cloud infrastructure, software development, modern applications, or Pulumi. Whether ...
url_slug: pulumiup-announcing-pulumi-30
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "PulumiUP: Announcing Pulumi 3.0"
  description: |
    PulumiUP is for anyone who is interested in cloud engineering, cloud infrastructure, software development, modern applications, or Pulumi. Whether you’re a seasoned cloud engineer or just curious to learn what’s all the fuss about cloud engineering, PulumiUP will teach you something new about the future of building on the cloud.   Hear from Pulumi CEO and Co-Founder Joe Duffy and CTO Luke Hoban as they share several new announcements, present demos, and discuss the future of cloud engineering and what it means for teams building cloud infrastructure and modern applications. You will also hear from technical leaders of pioneering companies that are using cloud engineering practices and cloud providers like AWS.   You will learn about cloud engineering practices, how leading engineering teams are implementing best practices with Pulumi, and the latest developments from Pulumi. After the broadcast, dive into hands-on learning with on-demand content or join one of our scheduled workshops or talks.
  sortable_date: 2021-04-20T20:58:48Z
  youtube_url: https://www.youtube.com/embed/Zko70KVGcgo
transcript: |
    Hello and welcome to Pulumi. I'm Joe Duffy, founder and CEO of Pulumi and I'm really excited to welcome you here today to talk about modern cloud applications and infrastructure. And here's how the day is gonna unfold. We're gonna start with uh a little bit of context why we're really excited about everything we're seeing in the modern cloud. We're gonna bring Adrian Coro from Amazon, uh joined by Aaron Coo here at Pulumi to talk about what it means to build for the modern cloud era. We're then gonna invite some industry leaders Justin Fitzhugh from Snowflake, Keith Redman from, and I'm gonna talk a little bit about how the Pulumi platform is enabling teams to adopt uh modern cloud applications of the infrastructure in their organizations. We'll then turn it over to Luke Hoen our CTO at Pulumi to talk about what's new. We have some really exciting announcements to make today and uh some demos as well. So we'll invite some folks from the team to give you a tour of Pulumi and what's new then we'll have an amazing panel of industry leaders um with Kat Cosgrove from J Frog Charity majors from Honeycomb, Dana loss from github and Justin Fitz from sto flake. And what we'll talk about here is some of the patterns and practices that are working for teams trying to move to the modern cloud and then I'll wrap it up at the end. It's gonna be an exciting day. Can't wait to get started to start. We're actually gonna talk a little bit about cloud engineering and what cloud engineering is and why we think this is the secret to really tapping into the modern cloud cloud engine is a new way of building. And to see why we're excited about this, why we got here, how we got here and why this is the next natural step in uh the modern cloud journey. We look a little bit about the history of of uh the cloud. You know, over 15 years ago, we got started with the public cloud with AWS and you know, Azure Google cloud, we really moved beyond sort of the lift and shift era of moving virtualized applications into the cloud uh without, you know, really rearchitect them or revisiting the architectures. Along the way, we've uh started practicing devops, which has helped developers and infrastructure teams work better together, uh which has been an amazing journey over the last 10 years. Uh really taught us a lot about how developers can, can leverage the cloud in new ways, but also how infrastructure teams can use automation to solve some of the challenges they're having with the, the cloud. Um And a bit more recently, we've, we've started uh incorporating containers with Docker and service architecture, starting with Aws Lambda. Now, Azure functions, Google functions uh and Kubernetes, of course, over the last uh five plus years. And really what, what these technologies have done is they've really fundamentally reshaped how we build uh and ship our applications. Um We're really thinking more about applications that is distributed architectures uh architectures that span cloud services and leverage the cloud in new and exciting ways. And the old models really don't work as well. Um They don't, they don't allow you to get the most out of what the cloud has to offer. Uh And so that's what we're calling cloud engineering. We'll see a little bit more about what that means uh in a few in a few minutes. And I think before getting there, really, it's important to, to note it's still early days for the cloud, many companies still have private clouds of their own, they're still doing hybrid workloads. Um So I think it's important to, to ask ourselves some questions and we'll try to answer these throughout the day. What do modern applications architectures look like? Uh What does it mean to move beyond lift and shift? Um How do we embrace these new capabilities? Not as afterthoughts but as first class concepts as we're building our applications to begin with. And second, what tools and approaches are necessary to adopt those new architectures. What got us here is not going to necessarily get us there. Um Third, how do we work together as a team? I mentioned, Dev ops, that's been fantastic. There's clearly variants of that Dev ops and, and so on, but really, it hasn't necessarily brought the cloud as close to development as we'd like. And so how do we break down some of these barriers even further in unlock collaboration across the team? And finally, how do you empower our builders to innovate and ship customer value faster? That's what it's all about, right? Um And so I, I think we have some, some good answers at least to, to, to get started with throughout the day that we'll, we'll talk about and add all that together. Really. The question is, how do we tap into the modern cloud? I think to, to really start answering that question, we can look to some of the most innovative companies in the cloud space. Um We've got AAA collection of companies we've, we've talked to and worked with here um and spanning many different industries and, and company sizes. Uh You look at uh source graph web flow, superba and fauna. These are relatively early stage startups that were born in the cloud and really are leveraging the cloud to build their, their technologies. Um The cloud is really part of how those technologies uh work in a, in a deeply fundamental way. You look at snowflake lemonade, uh Tableau Atlassian Fango, these are, you know, mid stage to IP O scale companies that really have used the cloud to completely uh change their business model and, and, and change the the industry that they're operating in um completely by leveraging the cloud. Um and then, you know, finally, Mercedes Benz, a very well known company that, that has a rich history that is continually reinventing itself as part of innovation and has brought, brought the cloud to, you know, their, their automobile through the connected car efforts. And that's been really the cloud has been an essential component of that. So a lot of different stories here, a lot of different industries, a lot of different team sizes and makeups. But I think if you, you look at the commonalities amongst them is is kind of where it gets interesting. They've all adopted a self injuring mindset, they've applied software engineering practices to how they're managing their infrastructure. Second, they've empowered developers, they've enabled the developers who are building customer value to actually just leverage cloud capabilities directly sometimes with guardrails, depending on the risk profile and uh how, how expert, you know, developers may or may not be. Um but in all cases, they want their developers to be empowered to build. Um The third is that they've really embraced cloud architectures. They've gone beyond the lift and shift as we, as we talked about, they've deeply integrated the cloud into all all aspects of how they operate the entire software development life cycle. Uh It's not something that happens at the tail end of development where you take something and throw it into a virtual machine and ship it to the cloud. It's really part of how they operate on a daily basis and you add all that up and these companies are able to use the cloud as a competitive advantage. And I think that transforms the business and that's the change that we're seeing and that we're really excited about here at Pulumi to summarize the old way is to really think of the cloud as an afterthought that has not worked well. It, it worked fine in the early days when a virtual machine was, you know, you had two of them in a database and you shipped every quarter and it, but these days, we're trying to ship every day every commit potentially. Um And the old way just leads to slower de iterations. In fact, many developers have to wait up to a month to get access to new infrastructure that's required to build and, and ship a new service for the business. And ultimately just disappointment, this isn't a path to tapping into the modern cloud. The new way is to recognize the new reality, which is that all software is cloud software. And what that means is all developers are cloud developers and that infrastructure teams are really central to enabling that innovation to happen. The cloud becomes a competitive advantage, not a tax, not an afterthought, it becomes part of the R and D budget for a company. Not, not something that you think of as a, a tax, but instead something that allows you to have higher development velocity, better returns for your shareholders, higher operating margins. And frankly, you know, the builders like you and I are just much happier here too. There's less red tape, we can ship faster, we can just get stuff done and cloud builders. When I say that term really, we have to understand that that cloud builders come in very, you know, different forms, many different forms. There's infrastructure experts, we might call them site reliability engineers, systems engineers, it and ops often say that these folks are practicing dev ops, the infrastructure experts are provisioning automating and managing infrastructure. And then of course, we have developers, developers are the ones building the customer facing value. In most cases, sometimes internal customers, building shared services platforms or web services, but really the responsibility of the developers to build and deliver a scalable cloud application. Uh And, and that might be a, you know, application developer, full stack engineer, you know, many different titles here, but uh developers pretty broad ecosystem of, of folks and finally, the security and compliance experts, you know, security really is important to think about as we're talking about cloud engineering, security needs to be front and center in everything that we're doing. Um we've all heard the stories of when that doesn't happen, you know, the the consequences and you know, security engineers and security architects are, are the ones really securing the entire stack and ensuring governance. And when we say cloud engineering, what that means to us is all of those different forms of cloud builders coming together and working together and using standard software engineering practices to tame the complexity of the modern cloud. That's to us cloud engineering and to talk more about cloud engineering and what it means to build for the modern cloud era. I'd like to invite Adrian Cockroft VP of sustainability architecture at Amazon and Aaron cow from Pulumi to share some of their thoughts and experiences together. Thanks for joining me, a Adrian and Aaron. It's great to have you. All right. Well, hey Adrian, thanks for uh joining me. Um This is sort of like our old uh monthly one on ones, but we've decided to broadcast this um again. So thanks. Yeah, it's good to see you again and we've both got different roles now, but it's good to catch up and keep the old discussion on modern application development going. Yeah, definitely. So, um you know, since we, you know, since I was last at A BS, you've got a new role. Uh So tell me a bit more about it. OK. So what's happened is um I spent about the last decade talking about digital transformation and telling the stories of what we did at Netflix and what other customers have done along the way. And it's really kind of got to the point where everybody should have heard those stories by now. And I was looking for, I want to do something different and figure out a new area and opportunity came up to join the sustainability organization. So think of this as moving on from digital transformation to sustainability transformation. What does it take to rebuild the company so that it's uh it's generating a smaller carbon footprint and it's mitigating the effect of climate change on that company. So, you know, you were one of the pioneers of like modern cloud architectures and building those applications. How, how exactly does that play together with sustainability? How how should we view that? I think there's um partly if you're looking at buildings more sustainable products, then what you're doing, it's all about developing new products more quickly and cloud has always been about speeding up development for a long time. I did a talk just called speeding up development. That was what it was about. That was the point that was why we did microservices, why we did cloud, why we're doing all of these things and that lets whatever your business was doing is about be more successful. So that's really the core thing. And if we're trying to accelerator change to be more sustainable, then it's the same thing. We've got to develop new products. We've got to develop new services and it's going to help us speed up development. So that's fundamentally it. And then if you look at the trend as we've got some more modern application development, if we move, if we look at instances which are fairly heavy weight, if you, they take minutes to spin up, you tend to keep them around for hours or days or forever. Um Then we go to containers where we started seeing people using containers really just by minutes at a time in some cases. And fairly, it was quite a surprise when we looked at the usage patterns that there were a lot of containers that really only lived for a few minutes. And then we've gone to functions that only live for a few seconds or a fraction of a second. So these lighter weight ways of doing things are more responsive and that's a way that you can effectively depending on exactly what you do. But in, in, in principle, they're more sustainable by being more lightweight, right? So um you, you just talk about instances and functions and so clearly, there's like a a evolution um of things and what we're I I'm seeing and you know, we definitely saw that at Aws is with customers is a blurring of the boundaries between infrastructure and applications. So what, what are the boundaries between these two today? Um you know, serve functions, you know, are the infrastructure or the applications do, do you see these things merging, becoming more closely intertwined over time or how do you see this? Yeah, they, they all get used together for different things. So if every time you add a new tool to your, your tool bag, you've just got to learn how to use it, right? So that, and it's not that you stop using the other tools. So there's still places where the right thing to do is have a, you know, even a bare metal instance and or a or a full Ec2 system um EC2 VM. And then what, what we're seeing right now is a move so that the primary development patterns we're seeing are kind of serverless first, in many cases with containers for the things that you can't do in serve and stitching together services which are running on whatever they want to run on um as you build it. But the thing that's really different now from the decade or two ago is that applications used, we used to spend forever building an application and getting it deployed and then not touch it for a long time. Nowadays, everything is completely dynamic and that's where infrastructures code comes in and being able to define the thing you're building. But then also there's event driven thing. So even if you're building your complete containerized environment, um there may be events that happen when you and a container goes away, you want to do some clean up, so you schedule an event and you trigger lambda function to clean up the U BS volumes or that were attached to it or something like that. So there's a mixture of event driven in the application and in the operations, which is naturally them, the kind of things and there's lightweight scalable services using containers or maybe there's third party applications which you, you've containerized or things, you've moved from your data center that you containerized to just kind of give them the environment they want to live in. And those are really the two mainstream sort of patterns. But it's how do you, how do you use them together? I think that's interesting. Yeah. So you just touched upon briefly, you know, how people should think about choosing things. So um I, I guess the question is how should people think about when to use building block services and then sort of when to use more abstracted level services? So what, what's the right level of abstraction for different use cases? What, what's a sort of quick rule of thumb type of thing? What I've seen one of the formative experiences I had was judging a hackathon a few years ago where in 12 hours, the teams of people that had really barely used the services that they were putting together built incredibly functional applications. It is like how did this happen this quickly? And these were functional scalable applications and it's because they were gluing together services, web services, which are just there to be used. You don't have to install them or debug them or make them scale and then they were using Lander and containers to stand up everything else they needed and just getting on with it. So it's quite possible. The way I'd look at it is build the fastest thing you can first, right? So just take all the shortcuts, build something that isn't exactly what you want. But uh it's going to get you the first iteration of this. This is this even the product you're trying to build, right? Don't try and opt over optimize first. So then once you've got something built for test purposes, then you think, OK, maybe I want to optimize the instance type I'm using. So I, I have to move from servers to containers because I want to put a GP U in there or something with a terabyte of memory or something. Um Or you want to take a uh an off the shelf web service that's giving you the rough idea of what you wanted, but you want to do some more custom thing. So it's more like treating the, you know, it, it's the rapid development iteration and start the ability to put things together in a day or two, really a day or two for a completely new application up to something that's running. And there are definitely people out there doing that and some people can't figure out how you could possibly do something without having weeks of meetings before you, while you're discussing it. As like in the meantime, somebody's already finished building the app and is off on something else, right? That's kind of the difference right now, if you're, if you're building modern applications, got it. Um So with, with, with all these things, right, like the development cycle using the cloud could be pretty simple, but once you get into production, things get increasingly more complex. Uh So a lot of companies create um abstractions and forms of like, you know, shared services platforms or developer portals. Um And it leads to a boundary between app developer, app developers and platform engineers. Um I I is this the right interface between the two or is there a better model that you've seen um on how to do these things, how to, how to essentially bring cloud to all developers and you know, enable all of them faster? Yeah, I think one of the things that's got harder over time is there's too many services and takes too long for people to get their head around all the different things. So the the solutions we've been developing for that. Certainly a w there's the slash solutions part of the website which has entire solutions and components. And if you want to hook up, you know, two or three different services, there's some common patterns there and we've published the patterns and it's becoming more compos and you can, you know, they're using various configuration languages, you can put these things together. Um It's something we've also seen around the well architected framework, uh Liberty Mutual, for example, has published a whole lot of well architected patterns and they implement them, they put them on github and they've just started sharing them, but this is what they use internally for building out their, their environments, which is, you know, mostly servers first. So there's, there's serve and you know, a bunch of lambda, a few bits of containers in there, but they're, that's kind of their world they're living in. Um And then we're seeing and that's mostly um in that case, it's like CD K patterns dot org, I think is the site where they've been putting that. But we're also seeing this as a more not just a specific to sort of AWS kind of thing, which is a lot of land and CD K, but as, as a more general purpose kind of environment where we're seeing these ideas spread across different platforms, you know, in the data center and in other environments that uh beyond cloud. So beyond any one specific cloud. So I think there's a place where these ideas are becoming quite generalized. Right. Right. OK. So, you know, you're, you're seeing that that is the thing, right, where um there are platform teams that are building out sort of generalized patterns that are being distributed across many different people. They're using infrastructure code tools like, you know, or CD K to sort of um distribute this out. They're also sharing this open source. So it's commonly usable and uh shareable and things like that. Yeah. And the, the well architected patterns uh Aws started publishing quite a while ago, have been adopted by, you know, the other, other cloud partners as well and the cloud platforms as well. So we're seeing similar kinds of um well architected patterns. So the solutions are becoming just better understood across the industry. So you can sort of decide, OK, I need to build a thing. There's a relatively straightforward path for building most things now and you, you're getting preassembled components. So it's kind of like we're, we're building out of LEGO bricks, but some of the LEGOS are coming preassembled into little clumps that are maybe can be plugged together a bit more, bit more easily. And that's kind of the, the development then we're seeing some higher level services as well that pull together lots of different things, but uh got it, got it. So, um so how does uh Kubernetes fit into all of this? Right. Um You know, we got different, different clouds, different, you know, different services, um different teams building on things. Uh then, you know, um as a um a different cloud interface that people use. Um you know, how, how does this all fit into the different architectures and abstractions that we just talked about? I think it really comes down to the control plane. So the codes control plane is this is distributed with the cluster. So if you want to run a bunch of containers at the bottom of a cell tower for a five G deployment, you're gonna do that with a local KTIS control plane and cluster quite, quite likely, right? Or um and there's a lot of cases now where cloud is becoming much more distributed in general, it's not really just the central, you know, few locations with huge buildings, with everything in, it's pushing out to the edge with things like outposts and various other other technologies. So we're starting to see the tech, the idea that you want to have programmable infrastructure become pervasive into data centers and edge locations as well as in the center and for applications that need to run and manipulate their environment in particular, um then gives you a good API for doing that it's portable, but it's also you're manipulating a control plane that's specific to that particular application. Uh Whereas the, the large scale Aws services, for example, there is a huge control plane running all of ECs or all of EC2 or all of LAMBDA. And um you know, it doesn't really scale down to something that you could run on a couple of machines in a, you know, in a warehouse, you have to be able to connect to it. So they each have, you know, it's in some sense more efficient and more scalable to have a big central control plane. But you've got less control and you've got the network latency to get to it. So that's kind of different. I think we're seeing a mixture of these just as, as the technologies just are becoming more pervasive like this stuff needs to be deployed everywhere. And there are different ways of deploying with, with different strengths for different use cases. Got it. Yeah. So 11 of the things um that you, you, you just talked about is the in because of um the the vast range of types of applications and services that are being used. Um The and just the complexity of these cloud applications now um having uh programmable infrastructure is increasingly important and then we're seeing the blurring of the boundary of the infrastructure and an application and these applications are increasingly dynamic. Um So what, what does that mean for the future of the sort of the software delivery pipeline? What changes are we seeing there? Yeah, I think what you're really seeing is that the proportion of the code that you'd be writing in YAML is increasing, right? So you write your application and it's now, you know, 20 lines of javascript and you know, 5000 lanes of yam or something is to try to deploy all this stuff. And, and one of the good things with a Polian CD K is like if you're, if you're drowning in the sea of Yarl, you should, you're doing it wrong, you should, you should be using a higher level language to generate this stuff directly. And that just makes programmable infrastructure just a much better way of looking at it. Um, in terms of the pipeline, a lot of what we've been doing in the last few years is really getting into continuous delivery as the default behavior. So you writing some code, you hit, save, you checks it in the, saves it checks, it's in, it runs whatever tests are needed to show that it works on your laptop or wherever it checks, checks it into the repo, the system just kicks off and starts delivering it, whether you know, whatever kind of pipeline you're using. Right. And as it goes through, um, the stages, you know, obviously various types of build stages. But if you're delivering a function or a micro service that contain something relatively small, the build time is fairly quick, it goes out, then you run. Um, let's say it's in a test environment, you'll do some stress testing on it there, you'll do some chaos testing on it there. You'll, you'll mess with its inputs to make sure it doesn't fall over and, and just make sure it's, it's behaving itself and we're starting to see people adding chaos testing as part of their delivery pipeline now. Right. And then it goes off to production where you're doing blue green testing and you've now got to the old and the new code running side by side in production and you're looking at the metrics to make sure that looks ok. Uh Maybe some feature flags or some A B tests in there as well. And then finally you decide, do I need to run this version alongside the old version? Um, or am I going to shut down the old version because it's, you know, board compatible? So that's kind of that pipeline is fully automated and a lot of people now and there is things flowing through it, hundreds thousands, tens of thousands of times a day, depending on how big your, your organization is so perfectly normal. Now, for your pipeline to be running continuously and changing your application as fast as you need to go and any one piece of the application may change, you know, every few days or every week or whatever. But overall the application is being changed um with a huge number of updates uh that are being rolled out. I think that agility is really important, especially with things like, you know, some somebody finds yet another zero day in a library and you have to go to redeploy everything because there's a security exposure, right? The ability to respond to that in, in hours or a day or so is revolutionary compared to the sort of typical way we were dealing with this 5, 10 years ago. So to do that, right, like baking compliance as part of that unified delivery pipeline is also pretty important to being able to work for different types of policies and things like that. I think, yeah, there's a lot of ways that you can automate the policy, the policy compliance. There's all sorts of tolling around that. And then there's ways you can look at it from the point of view of how do you get auditability. And one of the things cloud does, which is really quite difficult in the data center is you can build a tamper proof log of everything that happened in an account. And auditors will look at that and say, yeah, I believe the temper proof mechanism that Aws uses works. Once they believe that then you can look at these audit logs and see everything about the state of the system. And that's a kind of a new approach to auditing by this is continuous compliance because you can prove the system was always compliant was the way it was done before was more like asking people awkward questions. And um you know, saying asking, did you follow the processes carefully and all this kind of very manual approach, which is very difficult to be highly confident about basically. So everything we just talked about is um you know what I would call, you know, cloud engineering practices. So what do you see as the ultimate business benefit of um adopting, you know, putting software delivery um our software engineering into your infrastructure doing infrastructures code building it as all part part of one pipeline, doing continuous compliance with all of that. Um What, what's the ultimate business benefit of uh of all of this? Yeah, I think it's always been about speeding things up, right. The competitive time to market for, for ideas and applications and features. Um There's a, you're in a race and if you can roll something out in a week and your competitor takes three months to respond to it, you'll just keep running away with things. So that's kind of the um that's always been the the the main driver and then people are finding that the systems are actually more reliable and the they're getting fewer errors and the the customer experience is better. It's easier to measure customer experience and you're just building a better product um that uh and then finally, if you do it right, if you, if you look at a lot of uh older style environments that are very static, they are sized for this, the biggest spike in traffic that they're expected to see, so they're running idle a lot of the time and there's a lot of wasted capacity there. And the other thing with cloud and dynamic infrastructure is things and things only exist for as long as they need to be there and you scale up and you scale down and you're running effectively. If you do that right, you can save a lot of cost as Well, because you're, you no longer have people wandering into a data center and racking system. So you take out some of the manual labor and then also you're uh you're just dynamically order scaling at whatever level it is at instances, containers or um or with lambda, it's, it's, there was some, there were some workloads which would not be possible without cloud, which people are running with huge spikes, very, very concentrated uh flash sales, for example, which just don't make sense. You couldn't build, you couldn't run your infrastructure at the scale needed for the flash sale, which is generating a huge amount of interest for, for a particular business. And then they run their entire environment for a few hours and then shut down like 90% of it. And so those kinds of things become uh not only possible but they become um they enable new business models and there's a whole bunch of interesting things happening there. Sounds good. Well, I think we're out of time. So thanks for dropping in and chatting with me. Well, thanks and uh great to see you um best wishes in your new role. And um likewise, yeah, hope you have a great event today. Cheers. Well, awesome. Thank you, Adrian. Thank you, Aaron. That was a great conversation. I definitely learned a lot. And next, we're going to build on that conversation and talk about how to bring cloud engineering to your team, how to put cloud engineering into practice. And to start answering that question, I'm super excited to announce the general availability of the Pulumi cloud engineering platform. The industry's first platform for cloud engineering. The Pulumi cloud engineering platform really builds on all the technology. Pulumi is delivered to date the open source infrastructures code project, the SAS for teams and enterprises and the goal of the cloud engineering platform is to really help you put cloud engineering into practice, to help infrastructure teams, developers and security teams work together to build, to plan and manage modern cloud applications and infrastructure. It includes the plumbing console, the Pulumi cli policy code engine infrastructures code, all the plumy packages we we know and love. Uh It really enables you to use your cloud, your language, your way. And we're visiting that set of industry, cloud innovators uh that we were talking about earlier um at in the in the conversation. I'm really pleased to say that we've actually worked with every one of them as a Pulumi customer and I learned a lot from them in helping them put cloud engineering into practice. And as a result, we've been able to distill many of the things we've learned into the cloud engineering platform in the form of automatic best practices so that you get a cloud engineering platform that works for you right out of the box. And we've seen a lot of traction with Pulumi to date um to date since launch, we've done over 18 million deployments through the Pulumi uh online we've also seen in the last year alone, a 290% increase in cloud resources managed by Pulumi. Uh Every one of these is a cluster, a managed database, a serverless function that's actually live in a cloud as we speak. It's phenomenal growth, phenomenal progress, really seeing a lot of happy customers and to talk a little bit about some of those specific customer use cases. I thought it'd be interesting to look at a few uh from different vantage points. First, is Snowflake who approached this more from the infrastructure side where Justin really needed to ship a new data cloud platform. Um Snowflake had an amazing IP O last year and as part of that, they, they put together this new data cloud platform, they had to get to market really fast. And Justin knew that the only way he was going to be able to do that was to adopt more of a software entering mindset into his team. Um He also wanted to empower developers to be a bit more self served so that for the parts of the infrastructure that made sense for developers to manage, they could do so and adopting go a familiar language for him, his team and his developers was the way to go. And as a result, they were able to get to market much faster than they could have otherwise, what used to take over a week and a half can now just take under under a day. And it's been an amazing partnership helping snowflake out and learning from them. Um in terms of cloud engineering best practices, approaching from a slightly different angle is Fungo Keith at Fargo was tasked with transforming his product from uh more of an on prem installation for his customers to something that was hosted as a a in Aws. He had a team of developers didn't have a budget to build a, a shadow it or platform infrastructure team alongside. And really the only path to success was to empower his developers to build and his developers were typescript experts. And uh so Pulumi allowed him to give them typescript, allow them to do infrastructure as code on their own. And what he saw was subject matter experts emerging from his team, uh the developers really took to learning different facets of the Aws cloud platform. Uh It really improve their time to market, they were able to be successful and spend a lot more time focusing on making their customers happy. And then finally, a very similar but slightly different story uh is it's lemonade Igor at lemonade was managing a small nimble infrastructure team and noticed that increasingly they were becoming the bottleneck for developers delivering customer innovation. He also wanted to apply more of a software engineering mindset and more automation to enforce governance to catch things like drift and adopted Pulumi and as a result was able to empower developers to be self provisioning their own infrastructure while also improving the way that his team was doing infrastructure, great success stories. But to go a little bit deeper, I'd love to invite Justin Fitz VP of Cloud engineering at Snowflake on stage to talk about their journey with cloud engineering. Welcome, Justin. Hello, my name is Justin Fitz. You. I'm the vice president of cloud engineering here at Snowflake. I want to start off with a short overview of of what Snowflake is. We're a data cloud and data platform with near infinite scalability where we allow you to easily load, integrate and analyze uh and securely share your data. We've been born uh natively in the cloud. So we've never been on premise and so our focus has always been how do we leverage those components of the public cloud? Um and take advantage of them. A key concept for us is around how do we disaggregate compute and storage allowing them to scale independently? So storage can scale up and down independently of compute as needed and vice versa. We also allow permanent billing which allows you to scale your workloads up and down independently and and use it on an on demand basis. We provide the same experience across all three public clouds uh with the same user experience and no change to features between the three and our core core fundamental value of ours is that the platform should be self managing. Um It should not have, we don't want a lot of knobs and dials and such to uh allow you to, to tune it. The system kind of manages that on itself. So with that, I want to start off with a short overview and talk a little bit about kind of what cloud engineering is, how we got here and kind of what our, what our path forward is and what it really means to us. So as a background, um I think the operations and development worlds have historically been divided into two different organizations where you have ops and you have DEV and very much oftentimes we saw the developers building the code and throwing it over the wall to the, to the operations folks um and, and the operations folks going and operating that and there was lots of back and forth where there was maybe confusion or a lack of understanding of the kind of concepts that each one cared cared about and it didn't lead to kind of quality as quality product as could be as could be had. And so we think this is flawed. The ops people often lack context on the code and why the code was developed and what the functionality is where the DEVS kind of lacked concepts and they lack, they lack kind of understanding of how to run a scalable infrastructure and scalable system. Um The infrastructure and scalability and operational concerns often were ignored at design time and only came out once the software was pushed into production and, and a large amount of load was, was put on it. And so I think that, that the concept was really flawed around having a separate infrastructure built from the software. It's stuff like we've always embraced the DEV OPS model where we have kind of developers and operations folks co located and embedded. But what we saw with the team was they were performing when I, when I first came in probably a few years ago where we saw a lot of perform heroics being performed in a very reactive fashion. So the team would have a lot of inbound dependencies. They felt like they were always under the gun and didn't have a say in the kind of how things were being built and how the software was being designed. Um As I said, they were, they were kind of a constant bottleneck because they had three times the number of dependencies of any other engineering team um that was there and, and, and always looked like they were kind of not in the way but, but certainly overwhelmed and not able to kind of keep up and, and operate in a strategic, strategic way. So to solve some of this, we shifted towards kind of cloud engineering. And what does cloud engineering mean to us? Cloud engineering is a software engineering organization that's focused on building, that's building software to manage maintain and deploy kind of our technical infrastructure and why do we need this? So we really needed the ability to scale as, as has gone through a really rapid growth in our infrastructure and our and our customers and our software based. Um And as that infrastructure grows, there was a lot of manual tests that were happening from individuals. And so those were scaling linearly as the infrastructure grew, which, which really doesn't scale over time when you're seeing massive growth. But more importantly than the people growth is we really wanted to focus on how do we improve quality and consistency of the code and the, and the product that we're delivering. And when you have a lot of human and manual processes, there's, there's lots of errors that can introduce, there's variability, it's not repeatable and it's difficult to test in a, in a consistent, reliable way. So we really wanted to codify these human actions into kind of software and build them as software software projects. So our goal here was really to follow the same engineering practices that the rest of the same software engineering practices that the rest of our engineering teams did where we any, any mutations or builds of infrastructure were traced were started with a code commit. Those code commits go through code reviews that are reviewed just like any other software commit would be code comes with tests and c processes that consistently test those commits. We have a continuous deployment pipeline for execution. So those deployments are all pushed through via having a computer do those as opposed to a human for consistency. And that all summed up to kind of any infrastructure change was traced back to a commit going through a pipeline. We really wanted the cloud engineering team to be viewed as as as another arm of the software engineer of the engineering function. But just another software engineering team with a focus on infrastructure um uh the infrastructure side of things. So to do this, we really had to shift kind of our culture. And one of the first ways we did this was by looking at hiring software engineers with an with an opposite infrastructure slant as opposed to C admins or operational folks with the ability to, to to script or to um to try to automate small parts of this so that we really kind of took that software engineering piece all the way from start to finish and started to build solid software to run our infrastructure. So a few key values that we, that we have as part of cloud engineering, one of which is the features and projects should be designed as a single system software and infrastructure together. So how do we kind of understand that software engineering mindset and embed with those software engineering teams to make sure from this project from the very initiation of a of a feature or a project that the infrastructure concerns are thought of and that we can build automation around it. Another key value is uh that every kind of every action taken on infrastructure should emanate from a code commit. And this is kind of a, a North star that we continue to, to point to and, and uh we, we try to, to hold very strong to but to get there, we really needed to automate everywhere. So we needed to look at every manual function, every human function and see how do we automate that? Even if that meant, you know, large scale projects to get there, you know, having any kind of manual intervention or human intervention as part of this generally introduces error and variability and doesn't kind of align to kind of our vision of where cut engineering is going. Lastly, we need to build scalable and and easy to use platforms. So if the developers, if this isn't easy for developers to use, if this isn't easy for the rest of the engineering team to use, it's not going to be adopted. And so we focused on building a next generation platform focused on containers um to, to for our next for all of our new applications and new functions that are coming out. We built this using Pulumi given from the start so that we could use um software languages, true software uh languages as opposed to uh config or scripts to allow us to have a true programming interface. And and to manipulate and review and test this platform and it's been very, very successful. So in summary, you know, cloud engineering is really a shift from the reactive kind of operational team to a proactive software engineering focus team. Um Our infrastructure is 100% managed through software commits and we're really embedded with other engineering teams to help design the software and the infrastructure. From the beginning to the end, I will say this has been a journey for us and it is a culture shift and a hiring shift and just a, just an overall shift for the organization. Um And we've been at this, you know, a year, I think we've made really great progress. We're not 100% of the way there, but this will require kind of a uh time and effort and, but I do think it's, it pays dividends at the end. Um And we've seen really great uh success from it. So, thank you very much for your time today. I hope this was helpful and speak to you soon. Thank you so much, Justin for sharing that story. It's been an amazing partnership and incredible to see all of the success that you and your team have had. Next, I'd like to invite Keith Redmond on stage VP of engineering at Fungo to talk us through that slightly different approach to cloud engineering. Coming more from the developer side. Welcome, Keith. Hi. My name is Keith Redman. I'm the VP of engineering at Urgo. We're one of the world's leading providers of client life cycle management, anti money laundering and kyc compliance solutions for the financial services industry. And our mission is pretty simple. We want to digitalize the fight against financial crime. We create frictionless onboarding and compliance journeys for our customers and for their customers. And we really want to introduce efficiency and effectiveness across the financial service. We're trusted by 32 of the top 50 banks in the world who have powered their client life cycle management processes with our enterprise platform since 2010. Now, as we all know, the world has changed dramatically in the last 18 months. And there's been a sharp pivot towards cloud native and software as a service based products. As we've all been forced into a more remote way of working something which has dramatically affected the financial services industry, an industry typically based in face to face communications now driven by the inaccessibility of physical branches and offices. And we've been far, far less likely to deploy on premise software and we're now far more focused on cloud based products. And as a result of this sharp pivot, we at have also had to change our own approach. In early 2020 we made the strategic decision to really build out our cloud made of product stack so that we can empower our customers to work where their customers are in a way that the modern world demands. And with that strategic decision came the need for us to build out our own engineering teams so that we could create the next generation of our software using the latest cloud native technology stacks. And importantly for me, as an engineering leader, I really wanted to build out an engineering culture of ownership, rapid agility and frequent deployment with a limited available set of resources. To me, what I really wanted to avoid was creating teams where responsibilities were handed off from groups built around specific functions. So things like Q A departments who tested a piece of software delivered by a development team or a dedicated DEV ops team who deploy artifacts built and created by another team. The term you build it, you run, it is often used in our strategic and design sessions. But I think there's a key term missing from that statement, somewhere between build and run, we need to deploy it. So when it came to planning the toolkit that would support our cloud native product development, we were determined to find a deployment framework that would both support our continuous deployment goals while also making it easy for our development teams to ramp up and become true. Cloud engineers. We began our journey with cloud native in early 2020 with a simple proof of concept. We tested a number of tools from across the industry including Pulumi and our first test project was really built around some simple LAMBDA functions behind an API gateway connecting into a document DB, nothing particularly groundbreaking, but it allowed us to really focus on the tooling that we wanted to evaluate. We started out with the Pulumi C# library since that's where our engineers were already comfortable and had existing expertise. And because of that familiarity with the language and with the development environment, we use visual studio or visual studio code to write our infrastructures code, our teams were really able to see results quickly. But one of the things that really made Pulumi stand out for us was its ability to help our people to learn the cloud. At the time, we had limited knowledge of some of the AWS stack in particular its servers capabilities and even something as simple as the built-in in intelligence that comes with deep inside our IDS really helped us to drive quality and the completeness of our infrastructure as code. Now, it's fair to say our early experiences weren't without challenges. In the early days, we spent far too much time trying to write our own abstraction layers and help our classes around the Pulumi C# libraries because ultimately, every developer likes to write an abstraction layer. But fortunately, we hadn't gone too far when we discovered Aws X, which is Ploy's package for tight trips. And with a little help from the Pulumi engineering team, we've now pivoted all of our infrastructures code to tight trips and we've never really looked back from there, that first proven concept had two developers building a simple aws infrastructure running Pulumi up from their local terminals. And now one year later, we've got 12 teams actively building our production cloud infrastructure using Pulumi to deploy everything from LAMBDA functions to configuration of hardened EC2 instances, deeply integrated into our seam tools and our info sector. And we're doing all of that from automated building and release pipelines. But Pulumi is a guided step ensuring that we're maintaining the integrity and also the quality of all of our cloud resources, which we're configuring in the point. And that's particularly vital to us as a company working in such a highly regulated industry. If I was to describe some of the benefits that we realized as a result of using Pulumi and cloud engineering, I'd say first and foremost, it's allowed us and myself to focus our investments in engineers who are building feature value at all times rather than building teams for specialist concerns like build or deploy or test. Secondly, I'd say it's really driven greater infrastructural quality and integrity since Pulumi really guides our developers in the work that they're doing and ensures that we're getting closer to correct and complete implementations. First time, things like in teen and developer friendly API documentation, really help with that. And then finally, something I'm particularly proud of is that Pulumi and cloud engineering have really helped us to grow as people and as engineers. I would say our engineering team are now far more proficient in using the cloud to deliver software, given that they're working on a daily basis with cloud infrastructure in an interactive way using Pulumi. And they're certainly far more familiar with the capabilities and they're confident in engineering and designing cloud solutions than they ever were before we started to work with Pulumi. So when it comes to future plans and looking ahead to the next step for Forgo cloud engineering, I think it's likely we'll start to look at multiple cloud products. And we're hoping that Pulumi will help us to take advantage of multiple cloud providers. And as we look to use the unique capabilities of each of the different providers in the market, a natural next implementation project for us will certainly be in the use of Pulumi policy as code as we look to take advantage of the stronger governance and oversight of cloud engineering as we start to scale out our footprint in the coming months and years. And then finally, something I'm particularly excited about for ourselves as a company and also for the industry as a whole, the opportunities afforded by the Pulumi automation API I think we started to become a bit spoiled by the speed at which we can interact with our cloud infrastructure using Pulumi. And we're starting to look for new ways to improve and using the automation API I think we can move away from configuration based build pipelines and the challenges they bring and move more instead towards full end to end solutions where build test run and deploy are all written in first class languages using tooling we're all familiar with. So thank you for the opportunity to take part and Pulumi up today. It's exciting to see where we're going to go as an industry and as a cloud engineering group into the future. Well, thanks Keith for sharing that story. It's wonderful to hear about developers up and running productive, happy and delivering customer value. And I think those two stories really emphasize why it's important that the Pulumi cloud engineering platform supports one way to build, deploy and manage both modern cloud applications and infrastructure. Many companies that we work with to adopt. Pulumi are coming from a world where they've got different technologies for infrastructure than they do application delivery and this just leads to silos and difficulty especially when it comes to securing that entire end to end pipeline plume also supports your way that includes your cloud, your language and your workflow. Pulumi supports over 50 different cloud providers including Aws, Azure and Google cloud also supports Kubernetes and we've really embraced the entire cloud native ecosystem with helm support Cober netti operators, Cr DS. And also it's important to emphasize that it doesn't stop at the public cloud providers. Uh Many others solutions are offering infrastructure uh cloud flare, uh fastly git lab Mongo DB data do all of these bits of infrastructure are often managed alongside your public cloud infrastructure as well. And it's important to have one solution to orchestrate deployments across all of those. Next is your language. Pulumi supports general purpose languages um rather than proprietary domain specific languages or YAML, which is notoriously difficult to deal with at scale that includes any no Js language such as javascript or typescript includes Python go any dot net language such as C# and F sharp. And it's not just about the language itself, it's about the entire ecosystem around the language including ID ES tools and package managers. And it finally, Pulumi supports your workflow. Many organizations have adopted C I CD for you know, application delivery but are looking to, to accomplish that for infrastructure as well, especially as we've heard earlier that the line between the two can often be blurred, especially with modern uh applications and architectures. And that includes over a dozen integrations with uh technologies like Spinnaker, Azure DeVos pipelines, github actions and Octopus. Deploy. Plumy also supports any architecture such as virtual machine based architectures, managed services like data stores and a IML um containers. Of course, whether that's Kubernetes or ecs and Amazon, for example, uh and serverless as well or the combination of all of the above. And Plumy also supports and integrates with your source control provider of choice and identity providers such as Octa Azure Active Directory, G suite or so. So next, let's take a quick tour of the Pulumi cloud engineering platform's capabilities starting with build by expressing your infrastructure in general purpose languages. The Pulumi infrastructures code engine can orchestrate deployments as with many familiar infrastructures, code technologies. With the exception that you get to bring to bear all of that great knowledge and community knowledge uh around these languages uh to how you're doing infrastructure as code that unlocks access to a broad development ecosystem. Uh not only do you get to use the language you've chosen to its full capability including things like loops and variables and, and classes, but also better development tools. So I mentioned editors, what this means is if you love visual studio code for, you know, editing code, now you can apply that to your infrastructure code and get interactive statement, completion and error checking, interactive documentation, refactoring support. Uh You could even use Vim or emacs. Um you get access to great tools as well, test frameworks and um code, search and static analysis and ers and package managers as well. So you can consume packages that already exist uh for common functionality or publish your own for for common functionality within your team or within the ecosystem. This adds up to a modern application development experience where the experience of creating your infrastructure's code I I is measured in milliseconds or seconds rather than minutes or tens of minutes as is common with these other YAML based and domain specific language based technologies and then finally, the ability to share and reuse best practices goes well beyond just code functionality, but also allows you to share and reuse architectural patterns that apply to the cloud. And we've got some exciting news that we'll talk about shortly that brings us to even the next level, the next area is deploy. Once we've built our infrastructure and cloud applications, we need to deploy it and deliver it. And we've got a great C line that's uh got great interactive output. And is it fun to, to work with? But once you go to production, many of our customers adopt C I CD as a solution so that deployments are automated and not run manually. This cuts humans out of the loop ensures that deployments are more robust, reliable and secure. You can focus all of your efforts on securing access to production by restricting access to just the C I CD delivery pipeline. And I mentioned earlier, Plumy supports over a dozen different C I CD providers so that we can meet you where you are or take you to a new solution that's best in the industry. Next is test frameworks. I often ask people, uh how many of you test your application code? Of course, it's near 100% people wouldn't admit otherwise. Um But how many people test their infrastructure code? It turns out it's, it's, it's very uh much lower than that. Um And that's because most infrastructures code tools are done with YAML. Yaml is not testable or domain specific languages. Pulumi use of general purpose languages, however allows us to take all the great knowledge and tools and solutions out there in the industry around testing and now apply them to your infrastructure code. So for example, whether that's moca for testing your no Js code or the built-in go test framework or pi test uh including code coverage tools. This really allows you to bring a new level of engineering discipline to how you're managing your infrastructure. And next, Pulumi goes beyond all of this, even to enable advanced deployment automation scenarios. By using general purpose languages, your infrastructure becomes programmable in new ways. So whether you're building self service portals, custom cli S or your own a product that has to incorporate infrastructure provisioning. Pulumi has got a solution for you here too and this is one of the new areas that we're excited to announce some new functionality. I'll say that for Luke momentarily and of course, everything that's happening here is orchestrating with the Pulumi Cloud engineering platform so that you've got full end history of who changed. What when and why this integrates with your source control provider. It also integrates with your cloud provider so that hey, if something broke at 11 59 pm, Tuesday evening, you can always go to the plume cloud engineering platform and find out exactly what changed down to the individual resource and the individual diff of what specifically changed for that resource. Finally, we'll talk about manage. Um Pulumi has a policy code solution built into the platform so that as you're empowering developers or as you're empowering individuals on the team to be more self serve and have more control over delivering what they need policies are enforced always in often. Um that includes, you know, security compliance, cost controls, configuration, best practices, oftentimes people might get something wrong even if they didn't mean to and catching something before it gets into production is very important. We've seen what happens when that goes wrong. Pulumi supports uh the open policy agent, a uh the Rego language. So any A rules can be harnessed with Pulumi in, in the uh policies, code solution, you can also offer rules in no Js or Python. Next, Pulumi integrates with your identity provider of choice, whether that's git lab git hub at Sian, uh any Sam Sso uh provider such as a active directory Octa G suite and what this means is it gives you fine grain role based access controls to, to enforce who can do what within your organization and really restrict access to just those who need access to critical things like secrets or production stacks, et cetera also allows you to delegate access. So if you're really looking to empower your developers, you can delegate access to them to, to have more control when that's appropriate. Everything that happens though is fully auditable. So any delegation of authority, any operations performed, there's a full audit log available. So if you ever need to go back and find out what happened and who did it? That's always there. And finally, Pulumi has great dashboards, reports and insights built into the platform. So if you want to understand what's happening across your entire team across the entire cloud engineer organization, whether it's developers or infrastructure teams, if you're the VP of engineering and you're trying to understand uh how many clusters did I spin up over the last month? And how many am I likely to spin up next month? We've got a great dashboard, great reports for you to get insights across the entire team. So in short, the Plumbing cloud engineering platform is really the easiest way to adopt cloud engineering in your uh team. It allows you to harness the modern cloud, both applications and infrastructure. It brings the cloud much closer to application development. So we can really build powerful modern cloud applications. It allows you to apply engineering practices to your infrastructure to really tame the complexity of the modern cloud at scale and add all those up. And your team will be able to innovate and collaborate faster across the entire team and just focus more on business value and making customers happy. And with that, I'd like to welcome Luke Homan, the CPO of Pulumi to go deep on some of what's new with Pulumi. Hi, my name is Luke Cobin. I'm the CTO at Pulumi and I'm really excited to be here today at Pulumi up today. We're announcing the availability of Pulumi 3.0. Pulumi 30 is the next major version of Pulumi and the foundation for Pulumi Cloud engineering platform. Three builds on all the work we've done since we launched Pulumi back in June of 2018, including the releases of Pulumi 10 and 20 over the last couple of years. Ploy 30 includes dozens of new features and hundreds of improvements and fixes both developed by the Pulumi core team and by the broad Pulumi open source ecosystem. P 30 provides industry leading support for building deploying and managing cloud infrastructure and applications over the next few minutes. We're going to do a deeper dive into some of the new features of Pulumi 30 and see some demos of what Pulumi feels like in action. First, let's talk about building with Pulumi. So Pulumi 30 supports your language. Pulumi use the program languages that you know and love to define and deploy your cloud infrastructure by using these languages. Pulumi offers the broader way of software engineering benefits. These languages have to offer things like ID tooling and reusable components, package management test frameworks and so much more. In Pulumi 30, we've significantly improved the Pulumi go SDKS with four times smaller binary sizes and SIM small and faster build times and a simpler API we've also enhanced our Python SDKS with more natural API S and stronger typing, which means that all of your tools inside your I DS and other tools for Python Light up and you get that benefit for Pulumi as well. Finally, we've introduced the ability to take Pulumi components developed in any language and make them available as Pulumi packages to users in any other Pulumi language. This is a really key building block which enables us to unify all the different language ecosystems supported by Pulumi into a single Pulumi ecosystem of reusable components of infrastructure. Let's talk for a few minutes about and we'll go bit more detail about Pulumi packages. Pulumi packages allow you to package and distribute libraries that manage cloud infrastructure resources. There's a whole bunch of different ways to do this, but no matter how you do it, they have become available to users in all Pulumi languages. And we do this by providing a Pulumi schema that describes the Pulumi resources that are part of a plumy package. And from that schema, we're able to automatically generate the SDKS for all the supported languages that go into the package managers, you know, and love every day. Plume packages are published and consumed the same way. So any user of Pulumi can just consume a Pulumi package, whether it's the AWS package that we provide or some custom components that you develop yourself. But there's several different ways to create these Pulumi packages we have the support for native Pulumi providers which are packages that use the complete Pulumi resource model and have access to all different capabilities. And these are useful for when we want to expose a new cloud platform for use inside Pulumi. We also now as part of Pulumi 30 have the ability to take Pulumi components programs. You've written that describe how to combine a whole bunch of capabilities inside your cloud provider and use those to build a new abstraction that makes it easier to work with some aspect of the cloud that you work with. These components can now be published as Pulumi packages and again made available to users in all languages. And finally, if you have a resource provider for some other ecosystem, for example, a Terraform resource provider or a crowd api for an existing cloud provider, we make it easy to bridge those and bring those into Pulumi and use them from Pulumi as Pulumi packages as well. One example of Pulumi packages in action is our Pulumi Eks package. This component is something that we made available for no Js developers nearly two years ago and has been incredibly popular. The Pulumi Eks component makes it really easy to stand up a production ready EKS cluster by just writing one or two lines of code with same defaults and a whole bunch of capabilities for customizing the the features of your Eks cluster. The Pulumi Eks provider builds on our AWS provider and KNAS provider to take all the common things you want to do with EKS and make it incredibly easy to use to date. This provider has only been available for no GS users. But by using Pulumi packages, we're actually able to take that component written in no Js and make it available as an as a Pulumi package available to all users of Pulumi by itself. This is really exciting having access to this EKS component is going to enable a whole bunch of new use cases for users of Pulumi and Python dot go and dot net. But we're really excited about this enables for additional packages that we develop both for ourselves and as part of the broader Pulumi ecosystem. Now, when we talk about building, we also, it's really important to talk about what we can build with polluting. And one of the key things with Pulumi is that Pulumi supports your entire cloud. So with providers from more than 50 cloud and SAS platforms, Pulumi lets you build for your complete cloud environment that could be working with Azure and Kubernetes or maybe Aws and cloud flare and Octa or maybe Alibaba and Postgres, whatever you're using in the cloud. Pulumi provides a single pane of glass that has access to all those capabilities from all the providers you're working with. In addition to this long tail of providers, we've also been investing in what we call native Pulumi providers, these providers offer automated access to all new features of the target cloud platform on day one with the exact API defined by that cloud provider. One of the key definitions of a native provider is that we auto generate it from the the specification provided by that provider. So for example, our first native provider was the Kubernetes native provider for Pulumi. This provider was derived directly from the Cubers Open API specifications and had access to 100% of the capabilities of Kubernetes with support for new releases of Kubernetes being delivered within an hour of the release of the upstream Kubernetes project's new version today as part of Pulumi 30, we're making available the G A of the Pulumi as your native provider. The Azure native provider is itself deployed derived from the Azure rest API specifications that are maintained and evolved by the Azure service teams themselves. This means that as those service teams deliver new capabilities into the Azure platform, Pulumi has access to them immediately on day one and with all the details of the new features. So for example, if a new preview version is published or a new G A is released or a new property is added as part of a new feature capability, those will be available in Pulumi immediately. This new Azure native provider has doubled the surface area in terms of resources and has access to all the versions of all the resources. And so it is a much more complete provider for the Azure platform. In addition, today, we're announcing the preview of our Google cloud native provider. This provides all the same benefits of having day one access complete coverage and, and being directly mapped to the API S as defined by the cloud provider that we talked about for K and Azure, but now also for Google cloud users of Pulumi. And finally AWS uh is coming soon and we'll have more to say about the AWS native provider uh in the second half of this year. Now we've talked about building, but it's also important to talk about how we can help with managing your cloud infrastructure. Pulumi three R really supports your entire team. And the core way that we help you uh collaborate across your team is via the Pulumi service. The Pluming service offers a home for your team's cloud development, enabling you to manage application infrastructure delivery from code all the way through to cloud. Teams can work together to see the deployment history for their stacks to see deep links to the cloud resources that are being managed by Pulumi in their cloud provider consoles to manage team access so that they can have different access for different parts of their organization to their cloud capabilities and to enforce organizational policy so that organizations can define what their best practices are and what their compliance policies are going to be. Along with Bloomy 30. We've also enhanced the pluming service to offer organization level activity insights, highlighting recent stack updates and resources under management for increased visibility within your team. We've also introduced the Pulumi C I CD assistant which makes it easy to integrate your Pulumi projects with your favorite version control and C I CD providers to take a deeper look at plum in action. Let's take a lap around Pulumi 30. Hello, I'm Lee, a member of the Pulumi engineering team. In today's demo. We'll pretend I'm a developer at a company called Upstarts. Upstarts uses kubernetes to build cloud native applications and deploy them to multiple cloud providers. Let's see how Pulumi helps them build and manage their cloud infrastructure. Let's start by looking at what projects they have here. We have four projects, one for an A KS cluster, one for an EKS cluster and two for the workloads on those clusters. Let's take a look at the A KS cluster project and dig into the code. Upstarts uses Python to deploy managed Kubernetes clusters. Note that Pulumi also supports the node run time for javascript and typescript developers, the dot net framework for C# and F sharp developers and go for go developers here. I'm using VS code for my ID, but you can use other I DS such as Pi charm or Adam for no Js or go land for go whatever you prefer. I want to highlight a couple things about the code first. We're using several Pulumi packages including the new native Azure provider. The Azure native provider provides functionality map directly from the Azure resource manager. API as you'd expect of all Pulumi packages hovering over a class brings up the relevant documentation and as you'd expect of any ID, you also have auto completion available as well. And because we are using the native Azure provider, if a KS as a new feature, we have same day access to that feature here, you can see the completion list for the parameters for this class. Whenever new capabilities are added in the R API, they'll show up within the same day. The native Azure provider also supports enos which means we can easily discover and validate all the legal values for our resource. For example, we can pick the container of VM size by bringing up a completion list. Because the native Azure provider supports the full arm surface area, you can also pick and choose which versions of any given API you want to use or just use the convenient top level name space we've already built out that brings in the latest resources. Let's walk through a bit of the code to see how we end up using this cluster. At the end, you can see here, we create a few resources such as a password and a resource group. Eventually we build this cluster out. And finally, we take the output of this cluster which includes a cube config we take that cube config and we have the base 64 decode, the resulting value we get back from the rap. This is easy and gloomy because we can take full advantage of standard programming languages. For example, here we're importing base 64. Thus lets us extend our code in a way that configuration languages can't. I'm going to run a Pulumi preview because I want to check for any errors before deploying the stack. A preview shows what changes will be made to your cloud environment. If you deploy this Pulumi program, it also validates your program including syntax type checking and resource validation, shifting left as much validation as possible before you actually deploy. It also checks the code for policy compliance using rules set by your organization that enforce best practices, security cost and governance compliance. Let's take a look at the stack in the Pulumi console here. We can see the recent activity for our preview. Let's take a quick tour of the Pulumi console which is the front end of the Pulumi service that manages state for our stack. So what is state Pulumi stores metadata about your infrastructure so that it can manage your cloud resources? This metadata is called state. Each stack has its own state and state is how Pulumi knows when and how to create read, delete or update cloud resources. In addition to managing state, the Pulumi service, lets us manage the people, teams and policies and our upstarts organization, we can group organization members into the teams, depending on their rules, you can grant access over resources to team members. Here, we can see the policies that are applied to this organization including the policy pack that was run against our stack. The Pulumi Council now provides an organization dashboard, providing all the most important information you need about your organization in one place. The dashboard lets you quickly see which stacks were updated, who updated them and the number of resources created over time. You can also customize the set of widgets on your dashboard. Another new feature is the C I CD assistant which lets you configure C I CD for your pluming projects in just a few clicks. Instead of building your builds locally, you can push your code to your version control system and have your C I CD build pipeline, build it for you and all that we showed deploying from the CL earlier for our production stacks. We want these to be managed by C I CD. Instead to configure your C I CD, we can connect to our version control provider so that we can trigger deployments on our pushes to a specific branch. Let's outline a high level how this works. First, we'll configure a Pulumi access token as a repository secret so we can access the Pulumi service from our C I CD provider. Next, we have to configure our workflow such that upon pushing to this repository, we will trigger the build and deploy workflow here. We configure the working directory and copy a template to paste into our CS C I CD provider configuration. Let's make sure it's actually going to deploy on the events. We expect it to. We're almost done. Normally we'd set up any additional environment variables. But since we're not doing this for real, we'll just click through to the end for the final validation step. Let's go back to the preview we had earlier. It looks OK, except for a policy violation, policies are enforced as part of an organization to ensure best practices and organization policies are being followed. In this case, we are not using a new enough cities cluster version policies are rules for managing the infrastructure control access or setting resource configurations. Policies can be either mandatory which stops deployments if not met or advisory, which warns you that the deployment is not in compliance. OK? So let's fix the version and run Pulumi up to deploy the cluster while this is running. Let's look at how we deploy a humanities cluster in A S. You can see that our EKS deployment is much simpler, just a single line of code. This is because we are using Pulumi S AWS EKS component which lets you deploy a production ready Coiner's cluster with same defaults. However, previously, it was only available to no Js Pulumi users as part of the 3.0 release. We can now use a component authored in one language such as typescripts in another language such as in this case, Python using the Pulumi component model. This enables teams and the open source community to work across different languages but build up shared IP as with everything else, we get the rich type completion along with the in line documentation and everything else you'd expect from a Pulumi package. If we run a preview, this works as expected. In fact, it shows how many resources would be created with just a single line of code. Thanks to the abstraction that the component resource provides. The popular EKS package is available now for all languages and starter packages are available for building and publishing your own Pulumi component packages. Today, Pulumi packages are the foundation for building a unified ecosystem of Pulumi components across first party, second party and third party ecosystems. And we're just getting started on growing this ecosystem. EKS is the first component to be released, but more components will be released later this year. Let's take a quick look at deploying a workload to our cluster. We'll deploy our workload to the cluster by referencing the A KS cluster stack we looked at earlier, I've deployed this workload before. So let's take a quick look at what the application looks like by visiting the endpoint. We see the guestbook application. Let's enter some input and see how it's persisted into the reddest instance that's running in the cluster. Let me quickly summarize our tour of Pulumi 3.0. We deploy the Kubis cluster on Azure using the Azure native provider which provides full access to Azure and day one support for all new Azure features. We took a tour of the Pulumi console including the new dashboard and C I CD assistant which helps organizations collaborate and adopt robust devops practices. And we saw how the Pulumi component model enables components authored in one language such as typescript to be used in another language like Python. And how this enabled us to create an AWS EKS cluster with just a single line of code in Pulumi. Thank you for watching and joining me, that's a great look at how all the features of Pulumi and Pulumi 30 come together to provide a great modern infrastructures code experience for cloud engineering teams. But there's one more thing in Pulumi 30. I'm really excited to talk about. So let's talk about deploying. Now, Pulumi three supports your workflow. And traditionally, when we talk about workflow, we've kind of had two workflows in mind. One is as a developer sitting at your laptop and building out some new cloud capabilities using Pulumi and running the Pulumi cli. And this is what you do while you're developing a new capability. Uh and is one of the core ways we want to integrate in inter interact with developers. But once that cloud capability becomes something you want to deploy to production or to some long lived environment you just want to take that and move it into a continuous delivery system. And we saw in the last demo how Pulumi the Pulumi cli can be embedded into your Pulumi your C I CD uh solution of choice. But teams we worked with over the last few years have told us they're looking for richer ways to integrate their infrastructure delivery more deeply into their own custom workflows. And with Pulumi 30, we're launching the G A of Pulumi automation API, the automation API lets you embed Pulumi infrastructures code platform and deployment engine into your own software. That means you get all the same reliable desired states, cloud infrastructure delivery, you love from Pulumi. But now as an API that you can embed into your applications in any supported Pulumi language. So in no Js and Python and go and N dot net, being able to embed Pulumi engine inside your own projects unlocks so many exciting new use cases. For example, we've worked with teams building their own cloud provisioning web services, either rest API S that they embed within other systems inside their software or full web applications. They expose directly to their internal consumers to point and click and provision infrastructure provided by that platform team. We've also seen users and open source projects building their own custom cli s for vertically integrated workflows, for example, to manage deployment of single containers into their own platform as a service, these kind of vertically integrated cli experiences can define their own user experience that really lights up their workload, but still gets to sit on top of all the capabilities of Pulumi desired state infrastructures code platform. This makes it much easier to build your own custom CL is. And finally, we've seen teams building their deeper customizations into their continuous delivery pipelines on top of automation. API for example, we ourselves have taken our Pulumi github action and updated it to V two recently which builds on top of the Pulumi automation API to offer improved performance, increased reliability and a reacher feature richer feature set for that C I CD integration. I've been blown away by all the different things. We've seen folks in the community building on top of automation API since we previewed it a few months back and to give you a quick demo uh to show you what Pulumi automation A P looks like in action. Let me hand it over to my colleague Kama Ali. Hi, my name is Kal Ali and I'm a software engineer here at Pulumi today. I'm going to show you a few of the infinite possibilities unlocked by automation. API modern day platform. Engineering teams are often in a position where they are bridging the divide between cloud providers and their internal customers. As a result, they often end up implementing their own internal infrastructure platforms where they set up cloud resources following internal and security best practices while exposing a user friendly interface so that their users can focus on what's important to them. Let's take a look at how we might implement something like this with Pulumi automation. API first, I'll show you how a user might interact with the platform and then we'll take a look at the code that the platform team would write to enable this experience on the left, I have my terminal and on the right, a web browser first, we'll start up our application by running our flash run command with a few variables set. In practice, the code running in the terminal would be running on a back end server. Now, as the user, all I have to do is navigate to the to the website to access the portal. Welcome to Pulumi self-service infrastructure platform, a website that lets you deploy databases, virtual machines, PP CS or static websites. We'll take a closer look at static websites. Sweet. So we don't have any websites deployed. Let's go ahead and create one. We can pass in either a URL or the content that we type in ourselves. So I'm gonna just type in some content and we'll click create on the left. You'll see Pulumi start running its update. It'll create a bucket, put an object inside that bucket, attach your policy to the bucket and then put out some outputs and we'll be back to the start. Sweet. Now let's check out our website. Awesome. Looks like that worked. Let's go ahead and create another one. This time, I'm just gonna pass in a URL because I already created this website. Once again on the left, you'll see Pulumi, run its update, creating a new stack for my new website and it's running through the same process that I just described. All right, we're back. So let's take a look at the new website. We just made awesome it deployed, but I've definitely spelled some stuff wrong. So let's go ahead and fix that up. I'll click edit. You'll notice that the html is already there. So all I need to do is edit what needs to change. I'll hit update this time. As Pulumi starts running the update, you'll notice that Pulumi sees the difference in the content and it only updates the resource that is related to the change. In this case, the bucket object. You'll notice that the other three resources remain totally unchanged. All right. So let's make sure that that worked. We'll refresh the page. Awesome. That worked. So we don't really want this test website hanging around. So let's go ahead and delete that this time. You'll notice Pulumi deleting our resources and then as it updates, you'll see, test disappear from the site directory on the right. So let's make sure that those resources were actually destroyed. So we'll go ahead and refresh this page four or four, not found no such bucket. Awesome. That means our resources were destroyed. There's also this view and console button that takes us directly to the Pulumi console. The console gives us access to all sorts of useful information about our stack. So for instance, you can see your outputs. In this case, you'll see that the outputs are the html and that's the html that we use to edit the content. And you'll also see the website URL which we use to create the links. You also see the configuration that the stack uses tags associated with the stack, all of the resources that go into making the stack as well as links to the cloud provider and all of the activity that the stack has seen. So in this case, we did our first update where we created the four resources and then the second update where we only updated the one bucket object cool. Now I'm gonna show you the code that goes into making this experience. So to create this, the platform team wrote a simple application using Python and flask within the application. We've registered handlers for each of the cloud components that we can deploy. If we take a closer look at one of those resources, you'll see crud handlers to create update list and delete each of the component resources. These handlers are implemented using Pulumi automation API automation A P allows us to write a normal Pulumi program to describe the desired state of our infrastructure either in line in this case, in this function or externally. In this case, our program describes the desired state of our infrastructure including taking in an input, which is the content that we want to deploy to our website. We can then drive the deployments of the Pulumi program from our crowd handlers. So for example, to create the website, we create a stack using automation API we then set some configuration values on that stack and then we run stack dot up, we wrap this code in a little bit of flash boiler plate to provide inputs from the web app and deliver error messages and outputs back to the user. A similar process applies for list update and delete handlers in just a couple of 100 lines of code. We've created a self service cloud infrastructure platform by building on top of Pulumi and Pulumi Automation API. So this web app is just one example of how you could use automation API to create rich experiences on top of Pulumi. Now that your Pulumi code can be embedded within your application code, the possibilities are kind of endless. You could even create an Alexa app that you can narrate your html to and have it deployed to a website using Pulumi. I'm not suggesting that you do that. But now at least you have the option as a former data scientist. I love my Jupiter notebooks. So let's take a look at how we might run automation API within a Jupiter notebook. So I've got this notebook that I created here. Notebooks are great because you can write prose right alongside your code. And so all I have to do is run some cells and you can deploy your website from within your Jupiter notebook. And there are so many more things that you could do in the automation api examples. Repo we've got examples of all sorts of stuff. We've got examples in each of our languages. I've done my demo in Python because I'm a big Python nerd. But we've got examples in each of the supported Pulumi languages. You could do a multi stack orchestration. So doing multiple stacks that are dependent on stack outputs like this example here, you can chain your deployment of your infrastructure with database migrations like this example here or you can build a totally custom cli that is specific to your domain and create a rich user experience on top of Pulumi like like Ploy, which is a cli that deploys local Docker images to a Kubernetes cluster in the cloud. Hopefully, this demo has gotten you excited about all of the things that you can do with automation API and I can't wait to see what you all come up with. Thank you. Great. Thanks, Kamal. It's amazing to see all the things that you can do with Pulumi Automation API with that. It's been great to, to show you guys all what we've been doing with Pulumi 30, really excited about all the new capabilities. I'm really excited to see what you build with it. Thanks a lot. Well, thanks, Luke and team. That was amazing to see all this new functionality available to the open source community and our customers. Um And with that, I'd like to move on to something really exciting. Uh I'm happy to have Kat Cosgrove from J Frog here to moderate a panel with some industry leaders to talk about. How do we move into this future of modern cloud applications? Uh panelists include charity majors, CTO co-founder of Honeycomb, Dana Lawson VP of Engineering at github and Justin Fitz VP of cloud engineering uh at Snowflake. Welcome everyone. Really excited to have you here. Hey, everybody. My name is Kat Cosgrove and welcome to planning for the future of modern cloud applications. We're gonna be talking about best practices for building and managing modern applications in the cloud where the industry is headed in terms of practices and tools and what practitioners need to know to stay ahead of the curve. Fortunately, we have some experts here to help three of them. In fact, plus me the moderator, let's get in there. So y'all, what are the key stacks and frameworks that you see sticking around over the next five years? And which ones do you think are gonna kind of disappear? Team? I all the way. Yeah. No, not at all. And, and also this is the year of the Linux desktop 2021. Sorry, we can dream, we can dream, we can dream or we're going to keep making that joke until it becomes true until the end of time. Well, I mean, I think is still on an upward trend. Um It just the, the benefits just become clearer and clearer with time. Um And the nice thing is that you don't really have to give a shit about the rest, sorry about the rest of the stack if, if you can, you know, just compile and pass the Fighters around. So I mean, I'd put a lot of chips on that. I, I think a lot of people are picking up things like rust for a fairly niche um applications. Um But you know, II I think that is, is definitely the winner. Yeah, we, we're definitely see a lot of, of, of go and, and still persisting is a lot of Python. But I think in terms of frameworks, I think the one constant is that there will be iteration and evolution here. And, and I think that looking for kind of the right tools and the right platforms to, to look at kind of whatever it is that the team may be looking for is is important. I think we saw a lot of things like terraform and such and obviously moving much more towards functional programming languages. And I think we're shifting quite a bit, especially with the cloud engineering focus towards um soft using proper software uh languages and such as opposed to things like DS L. But what I've seen is, you know, if we're looking at a five year horizon, there's going to continue to be evolution. And so as long as we're grounded in kind of, um software engineering foundation, I think we'll, you know, that, that's, that's a good bet to make bash is also never going away. That's true. Yeah, I, I would say exactly. I feel the same things but Ruby, you know, I, I may have been a, a Ruby cynic at one point in my career. Ok, a lot of points in my career and I keep working for Ruby shops and, you know, it's like the death of the monolith. I don't think there ever will be the death of the monolith. If you have a monolith, there will be death of pieces of the monolith. So I, I believe that Ruby is gonna continue to persist, persist. And then when we think about really the persona of how, you know, digital products are made, it is gonna be more of that traditional type programming languages. We see these um advancements in open source which lets you use just program languages of your flavor. And so javascript is gonna continue to be a pretty powerful thing outside of, you know, like go and the things that um and Python that, you know, it is, there are traditional DEV ops for SOA, but I'll say traditional, I don't know if it's been long enough to have the acronym to say traditional but um kind of gravitate towards and, and less DS L I think there will be less of that. I think we want to have common, common patterns across the board as we've seen um with our cloud native technologies. Is there anything that's like kind of low key right now that y'all think is going to be like really a big thing in five years, like something maybe we don't hear as much about right now. So what one area that we're that we're spending a lot of time in is is we have, we have a lot of tools like Pulumi that kind of for infrastructure, automation and such. But what we're finding is what is the, what's tying the business process together in terms of orchestration. So what sits on top of this, what coordinates the cloud provider apis with Jira with, with, you know, with Pulumi and, and kind of what brings it all together. And so one of areas that we're spending a lot of time in and we're not seeing, we're not seeing great stuff in the market just and we're having to build quite a bit, but it's kind of this overarching orchestration frameworks to tie a lot of these components together. If you think about Mule soft for kind of what they did and you apply that towards kind of a cloud engineering or, or infrastructure platform. I think that's an area that I'd love to see a lot of advancement in and I certainly hope we will come tool. Anybody else got thoughts there? I've been thinking a lot about, you know, just so nerdy five G uh the internet things, the way that we build app, distributed clouds, the future of like network API S and how we're gonna, it comes back down to orchestration again, like how you're gonna orchestrate these new ways that you actually hold and host code. It's not gonna be and this is maybe not even, I, I would say it's, it's starting to happen right now, but definitely the adoption is going to be five years plus for really, you know, this big nirvana of edge computing for everything, but we see that happen. Um I think that's one of the things that is going to continue, continue to rise. I think there'll be more serverless. I mean, I think we thought three years ago, serverless everything and it's been kind of a slow roll. I don't know, I know, I know a lot of places use it, not every place uses it. I think there's gonna be some things that are going to sneak up like that, but more so than anything. I think it's this sense of how, who is the persona of who builds, right? Like it's the SRE mindset versus, oh I'm gonna have this SRE team and we're kind of in this crosshairs of, of, of really what is the discipline behind that and what are the tools that are gonna be used? And so really just kind of to gravitate towards on the software development practices and what started as DeVos, but the persona is now morphing into like this one entity. Uh But, you know, I think we've been saying that for a while, but I feel like it's finally happening. Well, I feel like it does kind of feel like it's over the past five years, there's been this massive sort of gravitational shift away from, you know, putting all of our energy into preproduction stuff, you know, all the staging environments, all the, you know, all the tests, all the, all the shit and, and, and finally, like we, we reached a point of diminishing returns and, and, and the sort of gravity has really become, you know, around in production right now, like building your guard rails, building your tooling, building, you know, whether it's observ ability, whether it's, you know, feature flags, whether it's, you know, chaos engineering, whether, you know, there, there are a whole bunch of companies that kind of sprouted up around five years ago, um from which you can, you can compose basically the full uh development pipeline that you have at Google or Facebook just by, you know, you know, if you start up together with a little bit of glue, which I think is, you know, what, you know, I think this is a good thing. I think that this invites more people into production. This gives you a, a bit more of a constant conversation with your code and as it's live in production, which, you know, kind of to what Dana was saying, it, it enables there to be, you know, this one persona that, that, you know, writes the code has the original intent ships, it watches the users use it and, and then is able to iterate incredibly rapidly on it, which, you know, I've been, I've been yelling a lot about how it's time to fully highly fulfill like the mission of continuous delivery, right? Where that that feedback loop should be less than 15 minutes. You know, you should, you should be short enough that it's still all in your head when you're looking at users use it in production, it's so powerful. It's like an entirely different profession um than writing shipping code with, with a, a lag of days or weeks or months. So, on the subject of personas, do you think that Kubernetes is a thing that developers like just developers need to care about? Do they need to understand it? Do they just kind of need to know vaguely what it does? I, I think they need to understand containers not, I don't know why is, is the focus like to me it's, it's how are you gonna package your code? And what's that, what's that gonna look like? And how are you going to the concept of a container and including dependencies and having kind of a, a unit, an atomic unit. That, that's what matters to me whether it's Kate or, or anything else. I, I, I guess they need to care a little bit but I care, they, they understand the concepts of, of what a container is more so than that. Everybody should know what happens when they merge their code domain, right? How it gets out to users, like you can't be a good engineer, I think without, without basic knowledge um of, of the of the life cycle of your code. And, and that means just like you said, knowing where, which is increasingly containers. II I don't know. The like nerd in me is like, yes, they should know, you should know. Um But the practicality is like you should understand, I'm, I'm with Justin, you should and charity, you should understand containers. Um And we look at like what's happening now with all that, you know, is a service deployment is a service, everything is a service and it, it's gonna be interesting times when um you know, you are just focused on the innovation, right? And where these things become second nature. But I do think to know how it behaves in production, you just still can't get away from that. You, you're gonna have to still know the really basics of like how it's packaged, how it's built, how it runs under load. And um you know, it's, it's so I would say you should understand it. You should understand orchestration, but you don't need to know it. Um You don't, unfortunately. And, but it's cool, you should know it. You'll be a more powerful engineer if, if you, if you know it, I'm, I'm biased. I love Kubernetes. So I think, I think they should at least come up try, but I'm biased. So we've seen the application infrastructure, lifestyle life cycle merge more and more closely over time from like cloud engineering to get offs to infrastructures code. Do you think this is gonna like continue on the same trajectory? Like just merging closer and closer or are there cases where you would want to like maintain some separation? Sorry, some separation between what and what between any piece of your like infrastructure life cycle management? Like do you think we should just keep merging things closer to like one amalgamation, one blob the way we have been with like going from cloud engineering and get ops and infrastructures code or do you think there are situations where we should try to maintain like some kind of delineation between parts of our life cycle? Maybe I don't fully understand the question but um like you, you don't want to have more complexity than you absolutely have to. So it would take a pretty compelling reason for me to have something be owned and managed separately than, than the main way like you want to be there there to be a source of truth, a way that things are done and any deviation from that way is incredibly costly. Fair. Um And I think I, I'll add, you know, I, I think that one keep complexity down. Two use patterns where patterns make sense, but don't like box yourself out from that old, like, you know, right tool for the right job. But I mean, really think about it holistically and I think if you really care about like your pipelines, it should be to reduce friction. How are you reducing friction? So I think they should continue to be as close as possible and we should be using tool sets and automation and error detection to say like an this doesn't work. We should be using orchestration to pull those pods out and it should just work. And I think we have the power and tools behind us, you know, charity mentioned feature flags, you know, I'm a firm believer if everything goes out behind a feature flag and then it's going into production immediately, right? You should have good, good, good visibility and analytics and observ ability. That one is gonna tell you something's up immediately and hopefully you should have known that before, but like there's no better place to test your code than production. So I think you should just, I think it has to continue to be one stream, but it should be not complicated and there may be some disciplines where you want to because you have compliance needs. Like it's real, like there's still privacy needs. There's still these areas where you're like, maybe we should have something different depending upon your industry. But you're gonna have to find those uniqueness. But as long as you have the abstraction the same, it doesn't matter. It's about building the right level of abstraction in my opinion. Yeah, I, I, I'd echo what you're saying, Dana, which is, is we really focus on how do we build platforms and, and tools in a consistent way across the organization to make it very easy and streamlined for developers to push and manage their code and manage, like you said, feature flags and of ability in uniform ways and well defined ways. And so all that complexity is abstracted away from them and they have a very simple platform to develop to push and iterate as quickly as possible on. So in some sense, from my perspective, you kind of look at that platform a little bit segmented from and operate that a bit differently than maybe where the actual, the the application code is going because you have kind of a platform you're building and you want to manage that in a very consistent streamlined way and make it super easy for developers to push their, their code out. So I think there is some a little delineation there but those can't deviate too far. I mean, they're, they're linked at the hip. Got you So Dana mentioned testing and production and I would like to touch on that. Do all of you believe that testing and production is the way to go. You don't have a choice. Everyone don't have a choice. The only question is the only question is whether or not you admit it or not and invest in the tool to do it safely. I love that. I, I mean, obviously you're gonna do like you're, you're, you know, you're gonna write unit tests, you're gonna do due diligence, but like I have not seen in my multiple too long years in this business of having accurate load testing like ever, like ever ever. Uh The closest thing is is when you can like ship dark data, you have a feature flag where you can but still you testing and production, you're still, you know, you're still testing in production. I don't know, maybe somebody's gonna come up with some amazing, but I remember having to capture replay capture, replay is the gold standard, right? You capture 24 hours of the traffic and you replay it with a bunch of different, you know, with different concurrency and different flags or whatever. That's the gold standard. It's still like you've got to still the Michael Jackson problem. Michael Jackson could die tomorrow, you know, I mean, not again, but you know what I mean? Something new will happen tomorrow. You, it's not the same traffic and, and so, and so you you, you just get diminishing your turns when you're always planning for the, the known unknowns. Right. At some point, you got to start admitting just accepted, they are going to be unknown, unknowns. And I should have some ways of, you know, watching my, my, my traffic and, and, and tweaking it and, and, and, you know, having, having tools that are more scalpels than really blunt instruments, right? Which is what I feel like the the big shift over the past few years has been because, you know, when, when you have observ with high cardinality and high dimensionality and you have tools, you can literally just go, you know, what is different about these errors than all the other requests, right? What 1 to 20 things, you know, whatever is different about those requests, you can just separate them out specifically and go, oh that's why because, you know, it's only happening when it's, you know, running in I OS and this Virgin with this firm firmware, with this language pack and with this region and this build ID, you know, and being able to like specifically see that stuff with all of that guessing and it just makes it so much easier, it's so much easier to just like run it in a small controlled way and observe than to try and guess and ship it out and, and just see what happens, right? But, but I get why it, if people don't have good observable then, you know, shipping can be really scary because you spent a lot of time trying to formulate guesses about what's going on because you can't actually ask the questions or, or see what's happening. Yeah, I mean, my, my thought, I, I ran a, a CD M for my previous life and now obviously it's snowflake where we have kind of the, the amount of, um, just variations of, of testing environments and data patterns and such as it's, it's not testable. And so you're never gonna find all those. So the charity's point observably is absolutely the key. Um And at the CD N, you can never replicate your worldwide infrastructure. I mean, you can just never do it. So I think you do the best you can. But you, you assume that um that's just a kind of a first initial, first kind of smoke test. And then as you, as you go to production, you have to assume you're gonna continuously be testing. And I think when you get to this mindset, it also helps enable much more of a continuous delivery mindset. Right. I mean, you assume that you're pushing this code out, you're testing it live. You have, if you get to observe it kind of forces you down the path of observ. Yeah. It, it always have fun. Yeah. Right. It's like, it's like if you look for bugs, you shall find them. So just expect them to know we, when, when people are rolling out honeycomb. This, this always happens where they, they start rolling out some traffic and they go, oh, there's a bug. Oh, my gosh. These are, and we're like, you have no idea how long it's been there. You know, there's like, you're going to find 20 others. Like, we just power through. Like, but, but yeah, they aren't there until you go and look for them. Yeah, there's an actual statistic for that. I think it's 25 bugs per 1000 lines of code. So, yeah, they're there more in mine. But that's why I don't production code anymore, you know, so they don't teach any of this stuff in school though, right? Like, even if you, you go to a four year university, you're not gonna learn about any of this stuff, you're not gonna learn a lot of these concepts. You're certainly not gonna learn any of these tools. So, are there any particular tools or technologies that you would recommend a new engineer, whether they got a degree or came out of a boot camp or taught themselves that they should learn now to make sure that they're, you know, effective and marketable. Well, I, you know, obviously I've got my bias, I've got a tool to sell you, et cetera, but I do feel like it starts with observ because it's like putting on your glasses before you go and start working on stuff. And, you know, there's honeycomb. We have a free tier, it's all very easy. But, but if even if you're just like using S trace and L trace and you know, the Linux command line tool is like anything that you do to enhance your tool set of inspecting what is happening under the hood when your code runs is going to make you a more powerful engineer. And it will always be to your benefit to me. I I really focus on the, the concepts of, of what is a feature plug and why is it important of what's A B testing? What is, what is, you know, what is a canary deployment look like? Why, why would you need those things and, and just understand some of those concepts and then I think you can, there's lots of tools out there that, that can, that can implement those concepts but, and the same way that we're shifting away from things like DS L and focusing on kind of more programming languages um like and, and, and instead of contain more containers as a concept again here, I'd look at like, what, what are the key concepts and why would you apply those concepts and what are you trying to achieve with them? And then I think you can, you can, you can go and find kind of the best tool that implements those, those various concepts and that's where I would start. Yeah, just to add to that like find a language you love, just find what you love. It's so easy to be like, oh, I'm gonna learn how to write this and I'm gonna learn how to write this and then I'm on hacker news and this is a cool new language. I don't know what happens. Uh It does and you're like, I'm, I'm guilty. I'm like, been a professional junior go developer for four years. You know, III I just keep, keep running around. Um But II, I wish somebody say, like, learn how to deploy, learn how it runs in production, learn observ ability. Like the first thing you need to know is like, how do I debug this shit? Like what am I writing? Um And, but, but find the language you love. Like, I guess I wish somebody would have told me that because I just would flirt around between all of them. But you'll find something that resonates for you. Javascript does not resonate for me. So, like I found the language I love and, and that's what I always say to, to an engineer and then learn those concepts of how it actually works in the wild. That's good advice. I wish I had gotten that advice too because I, I did the same, spent a lot of time bouncing around between languages that I didn't really like until I decided I just actually do like Python quite a lot. But uh so we've talked about the future. What about the legacy? So what would you all say the legacy of the Devops movement is like, what, what lessons specifically or contributions does it have as the cloud and application world continues to evolve? Like what are we? I feel like the first big wave of, of des was like odds people learn to write code and we're all like OK, OK, you know, message received and I feel like the last year has kind of to the other direction. Now I was like, OK, software engineers, like it's your turn, like it's your turn to learn how to build operable apps to learn how to understand how you know what's actually going on in production, learn what the building and deploy pipeline actually does. Um So, you know, it swings both ways, hopefully, I think that's the point of Deb ops that ultimately it doesn't swing anymore. Right? Well, I, I mean, I, I talk the way I, I'm thinking about this now is devops really is kind of a culture of trying to bring developers and operations into kind of a single stream. Um And I know we used to call them, I'm I'm guilty as well calling a team A des team. And so like, we kind of moved past that, I think, but what, what at least where I, I think that it at least brought both sides of the, of the, with the ops and the DEV teams to the table and kind of understand this methodology and idea of, hey, we could build this system together as a, as a, as a single system, as opposed to throwing things back and over the back and forth over the fence. And I think what charity said is, is really, is valid, which is like the soft bringing software engineers to understand the DEV side. And so kind of we've done one swing, we've swing over the other. And how do we end in the middle where, what if we're all just software engineers with different focuses, right? And that's kind of the way we're running cloud engineering is like how do we build a software engineering organization that has deep expertise and is the subject of experts on infrastructure scalability. It's just another component of the stack, but it's looked like it, it's, it's still, it's just engineering, software engineering. And why is there a difference in terms of how we implement some infrastructure versus how we implement a new feature? And why is it just a commitment? We go through reviews, we go through pipelines and I would argue with you a little bit there. I don't think it's just software hearing. I think that um I, I actually feel like there's, there's a bit of a um a loss right now with deep systems knowledge, you know, people who were, were more traditionally in the stack, um who really understand how systems work most software engineers do not and just knowing how to write code does not make you good at systems all. And, and we've kind of said that coding is necessary for everyone but we haven't, we haven't, like said, systems knowledge is necessary for everyone. And I feel like a lot of people out there feel like they can just hire people who can write code and they've got themselves, you know, a good systems engineering team. That is so not true. No, fair enough. Yeah. Yeah, I'd agree with as maybe I said it incorrectly. I'd agree with that. I think that the concepts around how we, how we can implement, um, can follow a lot of the suffragist knowledge, that networking knowledge, kind of understanding how systems that are scalability. You, you're right, you, you, you can't, you can't just hire a and have them come in and all of a sudden magically you have a scalable system. I couldn't agree with you more. But I think that's what's really challenging in hiring in the market right now is trying to find someone with a deep, deep systems knowledge. But also, and it has, it has, has a, enough of a software mindset, um, and can bring those together and, and that's, it's, it's tough to find. And so I think you, you end up having to either train 11 way or the other, either, you know, people with the steep system side. Really. How do you keep a functional program more willing to train people? I really do. I, I feel like skills can be learned. You know, if, if the aptitude and the desire is there and this whole, like, we will only hire you to do things you've already done before I find that personally insulting. All right, we've got time for one more question. Short and sweet. There's only two choices which model is the best DeVos model. Amazon's two pizza system or Google's SRE system. This is a trick question, right? Because it's a total trick question that how do you, I, I'll answer it. Iii I will say, um you know what my grandma used to do to me and my cousins would say, well, they're my both my favorite. You ask her individually, you were the favorite. So I'm gonna say they're my both favorite. Um But let me, let me explain that, right? I think the sr AM mindset is essential and I think that the principles and the philosophy about being like bullish about availability is everybody's problem and we're actually gonna build constraints to make sure it is your problem kind of mindset. But I also believe that, you know, once you get over the to three pizzas, how many people do you have your room? Are you focusing on automation? Are you focusing on preservable? Are you actually building a system where that you can empower people with an SRE mindset? But you have a small team that own and operate that potential framework. So I cheated and I said they're both because I, I think you don't want to have one without the other. You want everybody have SRE mindset. Well, when shit hits the fan, I'm gonna say, if there is more than two pizzas, then that's probably something to go back and look and say, well, well, how am I building? And where's my automation? And what is my complete, you know, system recovery cadence. Look like Amazon's is best for Amazon Google's is best for Google and yours is best for you. All right, that's uh you know what, I'm happy with that answer. Both. It depends, it depends is the senior engineer answer to everything and I love saying it every chance I get. Uh thank you everyone for your time. Uh My lovely Panelist and for everyone who watched uh enjoy the rest of your day. Bye, see it. Well, that was great. Thank you, Kat for moderating the panel. Thank you charity, Dana and Justin for sharing your thoughts. I had a great time listening in and I want to thank you all for being here. It's an exciting day for the, the team uh unveiling the new Pulumi cloud engineering platform, Pulumi to 3.0. The best version of Pulumi to date. I really think the cloud engineering platform is gonna help you and your team put cloud engineering into practice, whether you're starting from infrastructure starting with developers or trying to do both together, really help the whole team collaborate and innovate faster but I have one more piece of news today. I wanted to announce the preliminaries. This is a new program to recognize Pulumi biggest contributors and supporters in the community. Um A lot of great experts emerging, whether that's experts in modern cloud architectures, experts in how to do AWS. Really, we're seeing community members helping the community, which we always love to see. And we'd like to recognize that we're gonna start slowly rolling this out in the background in terms of uh you know, uh preliminaries that we've already uh recognized, but we'd love to hear what you think. Um And so we're accepting nominations at nominations dot Pulumi dot com. Just send an email. Um We'd love to hear, you know, who are the industry leaders that are using Pulumi that you look up to that you've learned from? And we're gonna have an exciting launch party for this in, in Q three later this year. Of course, don't forget we have lots of great workshops always and often. But coming up in the next couple of weeks, feel free to sign up these free workshops. We'd love to see you there. Always happy to help you get up and running. And if you're listening in live today on 4 20 please join us for the after party on Clubhouse. I'll be there. It's at 4 20 really excited to have an informal chat and just get to know everybody and talk about what you're seeing and what you're excited about and share some of our lessons learned um and A w will be joining us there as well. So, in conclusion, thank you so much for being here today. I had a great time telling you about the new Pulumi cloud engineering platform, telling you about the broader cloud engineering story, hearing from some amazing industry leaders. If you're interested to get up and running with Pulumi, go to Pulumi dot com today. We got free open source, the cloud engineering platforms at your fingertips. And of course, if you'd like to talk with us, we're available on Social slack github anywhere we're always here to help. Thank you again. Take care.

---
