---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Deploying a Data Warehouse with Pulumi and Amazon Redshift | Modern Infrastructure"
title: "Deploying a Data Warehouse with Pulumi and Amazon..."
meta_desc: |
    Learn how to create a Data Warehouse with Pulumi and Amazon Redshift. We'll write a Pulumi program that provisions a single-node Redshift cluster i...
url_slug: deploying-data-warehouse-pulumi-amazon-redshift-modern-infrastructure
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Deploying a Data Warehouse with Pulumi and Amazon Redshift | Modern Infrastructure"
  description: |
    Learn how to create a Data Warehouse with Pulumi and Amazon Redshift. We'll write a Pulumi program that provisions a single-node Redshift cluster in an Amazon VPC, and then we’ll load some sample data into the warehouse from Amazon S3. ► Example code in TypeScript and Python: https://www.pulumi.com/blog/building-a-data-warehouse-on-aws-with-redshift-and-pulumi/  ✅ Get Started with Pulumi: https://pulumip.us/Get-Started ✅ Create a Pulumi account. It's free: https://pulumip.us/Sign-Up-OpenSource  00:00 Introduction 00:12 Where to find the Tutorial and Example Code 00:30 What is a Data Warehouse 01:16 Summary of the project 01:41 Demo starts - create a new project 03:03 Start building the data warehouse 04:07 Stack configuration settings 04:43 Define a new VPC 05:24 Create an IAM role 06:09 Create a VPC endpoint 06:44 Create a single-node Redshift cluster 07:50 Deploy the stack with 'pulumi up' 08:52 Load data into Redshift 14:13 Conclusion
  sortable_date: 2023-01-27T20:56:52Z
  youtube_url: https://www.youtube.com/embed/2v_53eWGrqE
transcript: |
    Welcome to modern infrastructure Wednesday. My name is Aaron Co and today we are going to deploy a data warehouse using Amazon Redshift. So my colleague Christian wrote a blog post about this sometime last year and we're just gonna go through it and uh try to follow along and um you know, deploy data warehouse uh using redshift. So what is a data warehouse? Um It's a specialized database that's purpose built for gathering and analyzing data. So, unlike general purpose databases like my sequel or post, um those are designed to meet the real time performance and transactional needs of an application, a data warehouse, however, is designed to collect and process the data produced by those applications collectively and over time to help you gain insights from that. So, examples of data warehouses warehouse products are Snowflake, Google, Bigquery, Azure, Synapse Analytics, and Amazon Redshift. And coincidentally all these are managed by Polu uh as well. So today, we're just gonna focus on Amazon Redshift. Uh Specifically, we're gonna walk through the process of writing a plume program that provisions a single node redshift cluster in an Amazon VPC. Uh then we'll load some sample data into the warehouse from S3. Uh And then um we will show you that data directly um in the data warehouse. So let's get started here. Um So we're gonna first create a new uh Pulumi project. Um But we'll first create a um new directory to holder pulling project. So we'll use uh my data warehouse. OK. Then we'll uh do it pulling me new and use the A Python template. So we'll call it the project name, my data warehouse. Um It's called project description that uh yeah, we'll take DEV and then we'll deploy the US was two. So is going to go off and set up the project pulling in all its dependencies. OK. Now that that's done um well, uh start building our data warehouse. Um So OK, to launch a like new Redshift cluster, we'll need to give a to BS a few details. So for example, the name of the red cluster, the name of the initial database, the user name and password for the admin user and the no type to use. Uh So I'm just gonna copy this from the blog here. So with that, we can um uh with that, we can actually open up main up pie and we can get started. So, all right, the first piece of code really is uh we're, we're just reading in the configuration value. So all the configuration settings that we passed in on the command line, we're gonna pull pull in from Pulumi conf config um into the stack. Um And then we're gonna create a S3 bucket that's gonna store some raw data. Next were going to defining new VPC and associated network resources for the red of cluster. Um Since the aim is to launch the cluster into VPC, we'll need to define a VPC first and define a PRO if it's subnet within it. And then finally designate a red shift subnet group to tell A to BS where to provision that cluster. So that's each of these here. Now, the next piece of code that we're gonna add to this program is um some I am A I am roll. So what we're trying to do here is we're trying to pull data from a S3 bucket into redshift. Uh So what we'll need to do is give redshift the appropriate permissions to read from Amazon S3. Uh So that's what this I am role is um doing here. Um And you'll see that we're only giving it a read, only access. So that takes care of the permission part. Now, the next part is um to make this all work because the cluster resides in a private subnet that subnet actually won't have access to the public internet. So what we'll need to do is give the cluster a way to communicate with S3 without having to leave the VPC network. So we'll do this by creating a VPC gateway end point. Um And this allows the cluster to read from S3 over the private network. And now that we have all that done, we can get to the cluster itself. So, so here we have a single node, redshift cluster. It is pulling in all the config settings, network settings, im roles VPC um settings that we defined earlier on and the code, it's all here. And then so once the clusters provision, we're gonna export uh the data bucket name and the red shift roll arm because we'll uh use it for uh loading in the data. All right, now that we have our program, let's save it real quick and then get back to the terminal. So we're gonna do a quick Pulumi up. It's gonna show us a preview of everything that it's going to provision and then we're gonna select yes to uh start to update. So deploying this cluster takes about five minutes, five minutes. So we'll see you in a bit. OK? It's finished provisioning everything. So now it's time to load some data. So in a real world situation, you probably already have some data to load like web server logs or some other raw or unstructured data residing in S3 bucket or DIMO DB table or R DS database. But uh we're starting from scratch. So we'll create and publish that data manually. So we're, we're just gonna load a generated text file containing a few lines of json each representing a fictitious event to be loaded into the data warehouse. Uh So sure, copy this over. All right. So a bunch of these event, um events, reporting it into events one TXT. Um And then what we're gonna do is we're gonna upload this file test three using the A to BS Coy. And what you'll see is uh So we're gonna do that and then we're gonna take the output here and upload to that specific bucket. So it's three of the OK. Let's copy this bucket name out here and do that. All right, now it is uploaded. All right. Uh Now that we have our Redshift cluster provisioned and we got some data loaded to be ingested. Uh We can go back according to data. Um So let's go into our Redshift cluster here and we should be able to open up a query editor. So there is a red shift cluster and then we're gonna um uh log in via the database, user name and password. Uh So we have admin and then strong one should create a connection. All right, there we go. So we created a connection and you'll see that there is a DEV database that's ready to go. Um And it just doesn't have any tables. So we'll get to that next. So we are gonna um create a table called events uh which has um two elements, um my ID and a name. So run that and there we go. Uh Next, what we're gonna do is we're going to um we're, we're going to uh copy uh the events that we uploaded into the S3 bucket over. So with that, we need to go back and get our bucket name and replace it here. Uh Then back to the Pulumi outputs. We'll pick up the, the I AM role and then we'll use, we use the US was two region. So we'll do that and we'll hit run. So we'll see that um all that stuff is copied and loaded. And lastly, we will run another uh a new query. And what we're gonna do is just um select all the elements from the events table and order it by ID. So let's run that. And all right, that worked. So we were able to load the S3 data and then we were able to or load the S3 data into Redshift and then be able to query for it. Uh So these were the exact same events that we uploaded on the command line into the events one txt uh file. All right. So um that was a quick example of how to um um create a data warehouse using Amazon Redshift. So just to quickly summarize what we did. So we wrote a ploy program that provisions a provisioned a single node redshift cluster in an Amazon VPC. Then we loaded some data into a S3 bucket and just it into redshift. And then used the Redshift query editor to look at the data. Uh So with that, uh that is the end of this episode and a future episode, we're gonna continue building on the state of warehouse example. Um And do, do some. Um et so, uh thank you all for joining for another episode of Modern Infrastructure Wednesday. Uh My name is Aaron co your host.

---
