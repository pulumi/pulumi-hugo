---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Azure Cosmos DB, Microsoft's Globally Distributed Database Service"
title: "Azure Cosmos DB, Microsoft's Globally Distributed..."
meta_desc: |
    A talk from the Pulumi 1.0 launch event, presented by Rimma Nehme, Architect and Product Manager at Microsoft.
url_slug: azure-cosmos-db-microsofts-globally-distributed-database-service
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Azure Cosmos DB, Microsoft's Globally Distributed Database Service"
  description: |
    A talk from the Pulumi 1.0 launch event, presented by Rimma Nehme, Architect and Product Manager at Microsoft.
  sortable_date: 2019-09-30T20:19:53Z
  youtube_url: https://www.youtube.com/embed/2HMM3ZBFGss
transcript: |
    So my name is I am an architect and a product manager on Cosmos TV. We are going, I'm going to speak for about 15 minutes and just give you a little bit of an idea of what is Cosmos TV, how does it actually work behind the scenes? And then Mika is going to show how to build apps, land scale apps using cosmos. So roughly, you know, introduction system model global distribution, we'll talk a little bit about the resource governance conclusion. That's my part. And then so we started Cosmos DV, actually about nine years ago inside Microsoft. And originally, it was known as Project Florence. Those of you who have ever been to Florence, there is this famous dome which was, which is viewed as basically the beginning of the Renaissance. And we wanted to start the Renaissance for data and that's why we named it. We made the service generally available in 2017. It is classified as a ring zero or foundational service inside a what that simply means is it's automatically available in all of the Asia regions by def. So whenever we announce the region, whether it's a public, it's a sovereign cloud. It's a government cloud. We have to be automatically there because there are other services that are built on top of us. So we tens of trillions of requests per day given that we already announced a region in Africa, you can actually take your cosmos account and span it across all of the seven continents worldwide. So basically create a database that is spanning across North America, Africa, Asia Pacific and still view it as a single system in the inside Microsoft. It's also became ubiquitous you services like office 365 xbox Universal Score teams, linkedin. And now we're also on boarding. So when we started, we wanted to build the database designed for the cloud in a sense, if you were to forget about 40 years of legacy called relational databases, how would you design and cloud the data platform? And ultimately, we wanted to provide capabilities such as global distribution because cloud is ubiquitous. So your database, your data platform should be ubiquitous wherever your users are. Second is provide the elasticity and unlimited scalability with respect to two dimensions, both storage as well as so you should be able to elastically go from just a few gigabytes into terabytes, conventionally scale into petabytes and orthogonal to that your computational needs may vary over a period of time. So in retail phenomenon like Black Friday, Cyber Monday, all of a sudden you have a travel when you launch a game, you want the game to be successful. You have a lot of us coming in and you want so you want the data platform that will last and last but not the least is provide the cost efficiencies with very fine grained and resources. This is the ability to basically take multiple back them on the commodity hardware. This resource isolation, we've adopted resource governance and performance isolation and provide the guarantees in terms of performance. So these are kind of like the 34 principles that we had in mind when we started the service. While the design goals were number one is to elastically scale throughput on demand across any number of Asia regions around the world within five seconds at the 99% time. The other thing is also to provide really fine access to your data. So what that means is deliver less than 10 milliseconds and clients over read and right on average, it's actually lot fast for reads. You will see somewhere between 1 to 3 for right, it will be somewhere between 4 to 5, offer 59 read and write high, provide tunable consistency models for developers. So this is known as trade offs or trade off precisely the right trade off performance in the trade off, not just with respect to high availability but also operate a brainer obviously. And then last but not the least, this provides strict performance isolation between both transactional as well as analytical employers. So something that we will announce very soon. It's already previewed is being able to run analytics and global scale has also built a scheme agnostic engine to support basically unbounded scheme. So this is ultimately what resolvable. So given all of these design works at the end of the day, we have a core, the core of the service which is providing the capabilities like global distribution elastic scale out guaranteed the world file consistency models. We also give you the multiple data models to work with because they're built on top of the same core. All of these properties are applicable to graph the Jason documents to call the family key value and what not. And ultimately to access your data, you should be to pick the choice because we want to meet developers wherever they are coming from and give you the experience or the obligations they experience as if they're talking to native Moul or ca but ultimately, they're taking advantage of the cloud native. So if you're coming from relational background, we have a S A. If you are coming from mo background, if you're using a bag of sand, dry, this one on the, this is the ability to take these capabilities and take them to natives. So becomes globally distributed with automatic fail over with the multi master capabilities. And this one is the to be able to run operational and real time analytics anywhere around the world because spark is working with your global distributed records. So what is the system model? Let me quickly walk through. This is a beautiful site but uh ultimately the tenants, the customers, they bring their data, the data could be depending on the data model, it could be either tables or collections or graph cosmos containers. Disclaimer, it has nothing to do with the grenade containers, it has nothing to do with darker containers. We are not very creative in terms of naming. So we call them cosmos containers. Ultimately, this is your horizontal scalability. You pick a partition B and then a shard in terms of local distribution within the region and then global distribution across multiple regions. So customers can associate so customers can associate one or more Azure regions with their cosmos account. And then we perform the global replication across multiple regions supported A S are mobile gambling and all of these capabilities are applicable to all of them. The containers themselves think of them. They are completely schematic bags of. So whatever the data you throw at cosmos will happily absorb it and automatically index all of the. So what does the system actually look like behind the scene? This is actually the physical view so that you don't think it's magic. But so if you take Earth, you know these are regions we have, we're running today plus 54 regions worldwide. And the control plane is basically in a data center control plane. The control plane itself is fully decentralized and its state is also replicated inside. The impressive was he if you zoom in on a regions within the regions, you will find the data centers. This is where we have the data center within the data centers. We're running across by 11 to 20. So there is a partial outage or somebody something happens to NATO hurricane, your replicas and your partitions are distributed across different means of clusters. You'll find here clusters both compute and storage. If you zoom in on the individual machine, this is where containers that belong to different tenants are core residing on exactly the same machines. So this is multi tenancy at play with the proper resource isolation. We can again sleep back containers that belong to multiple tenants on the same machines and perform and resource isolation. So the containers themselves out of replicas, the replica basically has a complicated database engine with mission control resource governor. And within the database engine itself, you will find the typical components of the database. A lot of other stuff that one is the actual storage engine. The fact that we are storing index rose on the column store for both operational as well as a work. So the global distribution, this is the experience that we provide to the users. You can come in click on replicate data globally. We provide you with the world map and you select where you want your data to be. Then you click the save button and it's. So at any point in time, you don't need to pause your application, you don't need to re deploy it. You select the regions where you want it to be. That's it. You can also specify the priority list of the regions. So that in the unlikely event, there is a unless there is a regional outage, basically all of your rights will automatically be over to the secondary region in the list. So this is what we call global distribution by virtue of just turning on what you want. This is a little bit of the insights, but you can think of it. The partition is basically basically for your data. When you go partition your containers, we shard them using partition keys. Each partition is represented by a forum of replicas that is represented by where the data is stored in both a political and transactional data storm. We dynamically self adjust membership of forums, we have built in resource governance leader over election and we enable basically be very flexible split, merge the application of these parts. Well, this is um what the actual global distribution looks like. We go and look at our telemetry is basically a graph that is spreading across. And you can see basically all of the weeds and rights are always local through the region where where your patient here is. So you should always look at your or your a functions with the region where what is associated with the that's how you get the lowest, it's multi masters. So your reads and rights are always happening locally. So the SDP that we provide are multi Hoing aware. So if there is a regional outage, the client is intelligent enough to be able to navigate to a secondary region and be able to run the request. So as far as the obligations here oblivious to potential regional failures or you adding and removing regions, you may only notice the impact of the this is the consistency models. So if you're really fascinated about this subject, if you look at the traditional market of operational databases, you will typically notice what you call a very extreme binary choice. On the other hand, most traditional databases give you strong consistency with its own trade offs of high lower availability. But we get the perfect consistency, most traditional systems, most E even cosmos D gives you also the three intermediate consistency models. We actually have a plus for all of the consistency models, we get you guys to go check them out. And it basically gives you a very, very well defined options in terms of navigating the precise trade offs in terms of performance, latency, consistency and availability with failures or network. So you can get the right resource governance. This is another very important of the system, ultimately. So Cosmo DD goes with a provision throughput model very similar to what Dynamo DD offers. But the difference here is our request units, abstract, abstract, basically physical resources, like percentage of memory, percentage of CPU percentage of bios and then there are operations that you are running. So you don't need to do separate capacity management for reads or for rights, all of them are applicable to all of the database operations that you run behind the scene. Basically your replicas that belong to the partition, get a lot of the budget of our use and we provide resource governance to guarantee that when you ask that you want to perform 10,000 transactions per second, you have these resources fully allocated to you the ring fence from all the all the time. So you are getting the resources that you got. OK. This is actually just I took the snapshot from about two years ago. Now it's a much, much larger numbers. But you can see just the scale that we're riding in within a matter of three days. We're supporting services three trillion transactions in just three days. This is a delay view. It could be a zooming in on one of the clusters. This is multi tenancy in real life. So think of it this way, these are the clusters that are in region central us with the replicas and partitions belonging to different tenants, all located on the same cluster serving. This is zooming in on the individual machine resources and allocate them to different tenants and within the single machine, how the resources are properly isolated to serve. So in conclusion, you know, Cosmos is Microsoft's globally distributed database service. It is one of the foundational services we fascinated massively with our internal workloads, multi master replication, global distribution funding resource governance, partition management are the core foundation of the service. And then they are precisely defined multiple consistency models providing clear trade off for the and the latency and the throughput availability is something that we as the engineers inside Cosmos are super, super proud of. It took almost 10 years to get to that level. But this is really exciting, we're also hiring. So if you are excited about this mission for cosmonauts, you can go to the site cosmos TV dot com will show how to use cosmos to build and applications.

---
