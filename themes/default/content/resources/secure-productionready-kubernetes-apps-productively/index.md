---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Secure, Production-Ready Kubernetes Apps, Productively"
title: "Secure, Production-Ready Kubernetes Apps, Productively"
meta_desc: |
    A talk from the Pulumi 1.0 launch event, presented by Duffie Cooley, Staff Architect at VMWare, and Mike Metral, Software Engineer at Pulumi.
url_slug: secure-productionready-kubernetes-apps-productively
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Secure, Production-Ready Kubernetes Apps, Productively"
  description: |
    A talk from the Pulumi 1.0 launch event, presented by Duffie Cooley, Staff Architect at VMWare, and Mike Metral, Software Engineer at Pulumi.
  sortable_date: 2019-09-30T19:53:51Z
  youtube_url: https://www.youtube.com/embed/xLH21_fVGcA
transcript: |
    For those of you who may not know us. So I am an engineer at Clomi, I work on ready focus portions of our product. And likewise, Duffy does the same at vmware and we shared a former life with some coworkers of ours here. Shout out to cos and in that time, we've seen so many good and bad things about taking protection more bad than good. And we're here to tell you why we think things are starting to take a different turn for the better. So I'll turn it over to what are you doing? So um I'm Duffy, I'm ma on Twitter. Feel free to call me. I'm trying to increase my account. Um I'm actually, I'm talking about Uber news pretty much all the time in this talk. We're actually going to talk about some of the security, some applications on how to think about them. We're going to show you some kind of cool command line tools, some tricks about things. I'm also give you a quick high level overview of who we need. We're going to talk a little bit about what is in your team we're going to talk about. Um I'm gonna give you a quick five minute overview of what is CODIS. And we're gonna talk a little bit about what is a container. And then we're gonna talk about how you can use uh container orchestration, basically a form of container orchestration that CODIS takes to do some things, maybe that you don't as a cluster operator want people to do. And so we kind of go through those examples and then we'll also show you how you can actually mitigate that. And so this ties into the application security story by actually showing you some of the controls that you have within coo and discuss to actually mitigate some of the some of the risks that are just inherent in. So this first slide just talks about like all of them, I'm right in front of the speaker. So this first slide talks about basically all of the moving pieces of CODIS and we're just gonna talk about it as an application here. So in this slide, so I'm actually gonna step on the other side of the. So in this side, we have all of the moving parts of a need cluster. So when you set up a cluster in in EK or in some of these other environments, you really don't have any idea of what's happening inside of the dotted box, it's kind of happening outside of your control. But if you're actually standing up a co cluster in other environments, leveraging some of the source projects like cluster API or, and you actually get to see all of these pieces working together. Let's talk about what they are just really quickly give you a high level overview. Um The API server, as you can tell by all of the arrows are basically, is basically kind of like the center of the universe where the this API server does exactly what you might think it might do, right? It's going to do things like validate resources that are being submitted via the API it's going to handle things like authorization, authentication, those sorts of things. It's actually also where admission control happens and we'll talk a little bit about that. Like as we move through this, the we have the controller manager, this guy's job is actually to have a number of control loops that basically iterate over and over again and try to break down a higher level of obstructions things is called or stale sets down into pod or for the level of ins and we have the scheduler which does what the scheduler associates work with actual notes. And as we're talking about nodes, we can look over here on the right, see two clits and a container run time. This is actually where all of the heavy lifting happens. Um When we were talking about pods, that's actually the schedule of the minimal viable resource within the 2 m. Those pods are actually realized on the clit and the puli takes care of the life cycle of that real process running on a server somewhere. That's what the job is. Lastly, we have CD, which is basically a key value store that stores the state for all of the Cooper news clusters or all of the state within. That's a quick overview of all of the components. Let's talk about a flow here. So in the previous diagram, we said, keep my pod, we're going to actually be doing that live in a demo. I want to talk about what actually happens under the covers when these things happen. The reason I think it's important is because Cober itself is a distributed system. And at least a couple of months back, once the pod is actually created that pod object or spec it's been submitted to the API server. A P server goes through the process that we talked about before. Once it's actually allowed admission into the cluster, that pod spec is persistent into the key value store and then the API server returns 200. OK? You give me a resource, I've accepted it. We're done. The next thing that happens is that the scheduler upon determining that a pot has been created but is not associated with a node will actually take that pot object, determine where to schedule that object and then persist back to the node name field or the selection that it makes. And again, the scheduler is now done, doesn't do any more work than that. It's basically just populates that node name field based on the predicates that are provided by that next thing that happens is actually the heavy lifting, we're going to start this process on a keyboard somewhere. So CUBIT will actually also have a watch on the API server and we'll see that there's a new pot that's been defined. It's been associated with this note. I'm going to download that spec and start doing real work. I'm going to start those processes that are, that are defined within those containers and run them on a server and manage the life cycle of them. And I'm going to report back up to the API server, things like status. One thing that's interesting about this slide is if we go back to the previous one, we can see that there's a controller manager in this diagram, but there's not a controller manager in this diagram. This is a loosely coupled system. I've defined a pod, which is already the lowest level abstraction. This is already the lowest level primitive. So there's nothing to break down into smaller primitives. There's no need for a controller manager. If I had specified within the pod, spec in no name. If I had populated that field, I could take the scheduler out of this picture as well because there would be no scheduling and we're going to show what this means and minus might be like a security service for community in general that was my five minute Uber need to talk. That was in the morning is what it is. So next I want to talk about like what is a container? How are, how many people actually in the room? I should have asked this already, but like how many people in the room are already pretty familiar with Cooper now? Right on. Well, the rest just got like kind of a course override of what of what it is. How many people in the room are familiar with containers? How many of those people can tell me what a container is in reference to like a Linux or a process. We're about to expand your horizons when you log into any Linux host and you type LS slash P or slash pro the pro system and then some pit and then the directory NS, you will actually see a mapping that represents what a container is in general, all of those things within that mapping. And we will show this when we do the demo part, we will agree on the particular name spaces that they are associated with it come. But effectively every process within the Linux kernel adds the primitively necessary to define isolation on a process level. All container is all containerization is is basically just making use of all of those pars to provide a different amount file system or a different container file system that's shipped with your container or a different network name space associated with just that one pot or a different pin name space. So that when you're inside the container, the only process I DS you see are the ones that are associated with that particular process, right? This is just process isolation and this is the kind of the aha moment when we think about like what the difference between virtualization and containerization is, right? Containerization is process isolation. If you start a process within a container and that process dies, the container dies. If you start a process within a VM and that process dies, the VM doesn't care, it's gonna keep going in perpetuity. This is the key difference between the two that we think about. We're also gonna talk a little bit about doctor and do so many. How many people here know what Doctor Docker is a few more of them that I expect. How many of you think that doctor and dock is exposing the underlying doctor socket on a container on a container host to the container? How many do you think it's actually exposing enough privilege to the container to create container primitives directly here? All right, we'll go in here, right. OK. OK. So this is actually a description of what a container is when we think about like all of the different things we talked about what they mean a container is a mount file system. This represents the actual doer image that file system that ships as part of your darker image a container file system, any shared house. Like if you actually have like persistent volumes or anything else like that, those sorts of things are actually associated with a, with a particular container. All of these things are limits kernel primitives that are together represent what the running container might be. And you can do things like you can do things like limit things like my cio things like create different network spaces. You can share all of these things within between containers, within a particular pod inside of it is and then there's also a set of constraints that you can apply, like you can define an external capabilities. So whether you want to be able to provide through the container, the ability to manipulate the network of that container, you can, there's quite a lot of constraints that you can define and we're going to talk about what that surface looks like this. So that was a quick crash course and we need some containers. Now, I'm gonna show you some kind of like moving on some things creeping in front of the like right in front of the speaker so that I don't feel right. You are yeah em shield. So this is a one liner that actually allows any authenticated user with that in the absence of admission control to take control of the underlying node. And this is a, this is leveraging the QK command line tool that shifts the CODIS and what it does effectively is it will actually just deploy an alpine Lix image and it will use to mount the underlying nose files, p name space and file system. And then they'll actually use an center inside of this container to basically take over the underlying note. This is possible for any authenticated user inside of a Cooper news cluster, but in the absence of admission control. So let's just shut that off a little bit. You, what I'm gonna do is I'm going to show you the actual manifest. I'm going to use this. And basically, it's just like that one letter, but I've actually just created a container that will actually allow me to schedule this directly on a host. And just like we were talking about before with the direct scheduling attack. If I populate the node name, I don't have to have a scheduler run in, I can actually pick any node with the cluster to the to target with this particular. So go ahead and I'm going to exe into that computer and look at the result of center is a tool that can be used to enter into any of the name spaces associated with container. It's a great tool that really exposes a lot of capability for interacting with all of those primitives that make up an actual container to solve the mystery of how a pod is actually configured. Like if I wanted to understand what part of parts are actually going to be bound to a specific pod. I might want to actually see like, you know, the results of like l look at a computer and see like what the actual process is actually found. So in this case, what I'm gonna do is jump into an NSX pot or an engine X pod and going to look at what, what ports are found. And so in this case, the interceptor command basically allows me to enter the network news space of that running container inside of my local machine here, basically only by identifying process ID or P ID that is that is working on this particular note. Once I enter it in, I can do things like assess my cell and which is kind of like the equivalent of that man, I can see that this particular container is is binding on port game. So that explains my mystery, right? It wasn't working, it wasn't designed to work the way I expected. I can also mount to the file system and look at the configuration of these things. Now, what's interesting about this is that I can do these particular things even if the container itself doesn't have a batch or doesn't have a shell. So this is actually that mapping that we've been talking about before. So this particular container, we can see the mapping between the sea, the I PC network, all of those things. And a particular number, these numbers are specific only to this pot or this container. If you look at, if you as a host and you looked at all of the rest of the processes, you might see that they all agree on this. But what else do we do as we talk about the Nexus name spacing all of these, all the time. We can also use any center to take over with the host without post path. We talk that we haven't talked about those path. But we, so this plan, I'm actually gonna use any center again. I'm gonna enter all of the name spaces associated with or. Now, what's neat about this is that basically what this allows me to do is get rot on the underline post. This one liner allows me to actually completely take over the underlying host, right? I can do, I, I can see the underline host, I can do pretty much anything inside of this space. I have complete own over the under post. And this is possible in any news in the sorry. So that's what an authenticated user can do inside of without the control. They can do pretty much anything you need to learn. How do we fix it? How do we actually stop that from happening? One of the only service that we have to actually control, this is a mission controller and this gives us the ability to validate or rotate resources as they're defined inside the API survey. So in this case, I'm actually going to show off security policies and how those things can actually allow us to limit the scope of this particular type of attack is act as an emission control for your community customers. And they allow you to limit things, change things in a reasonable way. Pot security policies give you the ability to define things like um within the, within the particular pots. And then, so I set the privilege flag, I can set a pot security policy globally with an investor that says nobody can define privilege flags. And if I needed more than that, then I can actually allow only those pods within a particular name space to have that capability, deny everybody else that particular capability. Same thing with the underlying post file system or host network or capabilities. All of these things, I think to train all those with top security policies. And a now the ordering of prosecuting policies is a little tough to go around and the U I. But generally what happens is that the ordering of prosecuting policy application will be, will be validating before mutating, which means that if you, if you have defined multiple security policies within the, the whatever entity it is that's creating that policy if they have access to multiple pot security policies. The first one that actually allows without mutating an object will win and only that security policy will apply. They not, they don't, you don't accumulate or aggregate. It's just the first policy that applies, that doesn't change the object for them. And if it, if no validating object, no validating policy applies, then we will go through the mutated ones. And again, even a mutated one, only the first mutating policy will actually affect the pot. Because at that point, it has been accepted. Security policies are tough to work with because they actually tie into our back. We have to think about the entity that we create in the pot and associate security policies with that entity within cos we talked about the controller manager, the controlling manager is responsible for running things like the employment controller controller. And actually in those sorts of those entities are responsible sometimes for creating pods directly. And we have to associate the permissions to those security policies that we want, apply them to those specific controllers or to groups of you. So let's do all of this. Thank you, Duffy. So as Duffy pretty much showcased, this is pretty right for picking, right? If you're looking to attack a coup without having default PSP in place policies, you're pretty much out of luck. So the beauty of that is that, is that in every single provider, what essentially happens whether I'm running on Azure, whether they're running eks or if I'm running on G they all come baked in with their own pot security policies and most of them are pretty wide open. So they allow a blanking kind of policy that anything goes that is not something you want to roll out of the gates. So the good thing about managed clusters like E and G and A is that they give you a but that is a vanilla cluster, you cannot actually use them, right? Not in production. So the cool thing about hay is because we support many SDK across all of our platforms. I literally defined three clusters and three lines that is pretty powerful. If I want to actually go into the actual actual cloud, I can go and inspect that based on ours on the column on the right, you're my E cluster, I define my VPC that it's going to be deployed to. I deploy some rolls. I send the properties to the cluster itself, the version of cabernets and then I'm mapping some roles if I want to actually take over as a different user. When I deploy this, what actually is given to me is three different few FIS, one for EKS, one for Aks and one for G. I also deployed a pine cluster. It's a cabs and Docker Fuster and that is just local. So essentially all you need to work with the Pulumi is a file, we wrap a provider around that and in bloomy, that is the object that you work with. So this is pretty neat. I don't want to bore you the detail of setting a up, but this runs and you get a couple of Custer if I switch terminals here, I'm now going to deploy something into that. So because we have all these sdks available and because each thing is wrapped up in what we call a provider, you can now simply put this in a four loaf. Hey, look, we actually have a real fora and in that for loop, we can do a couple of things, we can define our own default security policies out of the gates that replaces all of the blanket allow that all these providers put in and then replace that with something more coherent. That actually makes sense not to mention, deploy a relig application into that. So this kind of shows up how I can deploy engine with its flow balancer into all these different clusters. And so if I run an update, which I've already done for all of us, I get all this output and I can literally go to engine X on Azure, I can go to X on E, I can go to X on GE I can go to on my kind cluster. But wait, I'm getting yelled at because I don't have the authority here as an anonymous user to do this. Why? Because my default security policies have revoked all open wide privileges. So the neat thing about that is that we can not only set our own defaults, we can now showcase that we've provisioned all of our infrastructure. I am all the networking, the cluster itself. And then once the cluster is up, we can hook into that cluster and deploy anything else that we need to have. So in this case, we have cloud security policies, poll me by default. Ours do not manage cloud security policies, but we have this notion of what's called a dynamic provider, something that I create a crud interface on and I can manipulate to my heart's content. So in this, I've gone in and I said, look for each of these providers, some of them come with defaults that I cannot strip away. G for example, if I were to remove all these pot security policies from G all of their stuff would break. Likewise, in a very kind of wonky manner aks comes with its own security policy. But if you try to delete this root privileged pot security policy, they have an add on manager, they add it right back in seconds later. So what do we do? We have to at least scope it to our capabilities to make sure we have the least privilege and that we have some control of what comes in after the fact because again, the lego blocks that we were given at the provider, they stop once that cluster is up. So it's on us to take the ball from there. So I decided to say these are the required lots of cloud security policies. And if you've worked with KTIS, I decided I'm going to create my own prosecutor policy. I'm going to create my own demo, privileged prosecuting policy and my own restricted policy. But they're each going to be gated to different groups within COTIS in this case, only the service accounts in the Pew system name space. So what does that look like? So if I go ahead and go into the looming, one of the neat things is the code is the state. So just like I deployed the engine next app, if I were to comment that out here, that code is no longer live and take that on here. And if I were to rerun an update, what will happen is that because they removed the code, that deployment will go away. So if you have a much easier way to work with the infrastructure with the state that belongs to it, and this feels more intuitive to what we all want to work with both of these. You're not in a situation where you just continue to it, right? And so the cool thing is you can do that on. So if you're testing, they say you want to do a testing across all discriminated clusters, you can visualize what that would be like. And again, the code is the desired state. So that's going to go off and delete and all the deployments are gone. Now to talk to what Duffy was really getting to is by default, we don't have the greatest security policies. So we bake on our own. I created this cluster as the Mike pou user in a and by default, whoever provisions the cluster gets super roots on all of this. So the super route can do whatever they want. Now to show that because the writer is just ac file, I'm going to show what it takes to deploy a roof privilege pod and the dock and doctor pod that Duffy was talking to here and show that the super route user can in fact deploy this as we expect. So that will go off and deploy in about 15, 20 seconds. And as we expect because I am the super roof user, all of these pods will come up. And again, I'm leveraging my cum big file that I created as Subaru and that's just defined as the proprietor and it's just the property and how you deploy the actual pods or the employment for that matter. So as you see the pod comes up as we expect, I am Subaru, let's go ahead and blow that away and try that as a different user who scoped to one where finer grain are back and who's not allowed to do this because they're forced to be restricted security policy. Thanks. So I'm just going to blow that away real quick just to kind of show you the clean slate. Give that a couple of seconds. But the cool thing about that is because we can just mix and match all of this because in Pernetti's everything is a few config file that gives you so much flexibility in how you manipulate, how you manage and how you actually work clusters. And again, because we can look into a cluster after it's up, that gives us the ability to actually set our standards, to set all our best practices to limit actual capabilities or even set proper defaults that we may need. So that will go off and delete the pods from the super root user and that will be done in just a second. And I'm going ahead and set this up for the next user who's finally scoped. So I created an additional user called better because better knows better than to run in a privileged setting. And better mite is actually scoped with I AM. And to be only using this finer scope rules here, they can only create pause deployments with the sets and PV CS within the default name space. Anything else beyond that they're going to get yelled at? So if I go ahead and save this and I run it as my better mic provider, which is my cupid fake file that the better mic has. I will get Pulumi to show me a little bit more different out. But this time around that shows that we can not only bake in our po defaults and that on top of that leverage our back to tell us that we are not allowed to be doing these things. So give it about 10 more seconds where and shortly enough, we should get a warning. There we go. And it says sorry, we cannot run these containers because the better mic user is not allowed to run a privilege container. They're not allowed to use the host file system. And that by itself is miles ahead of what we would ever have the capability of doing with any other tool. So with that, let me switch back to the slides. So that pretty much does it on our end. If you have any more questions, there's a QR code, just put your camera at it and I'll show you the link to all of our slides to all of the res and look for us after if you have any questions. Thanks. I thank you so much. So if you're working in this space, you can, you know, meet anybody at this table right, right over here or um for M or Joe here at the. Thank you very much.

---
