---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Get Started with Integration Testing: Learn How to Test Your Cloud Infrastructure."
title: "Get Started with Integration Testing: Learn How to Test..."
meta_desc: |
    Get started with integration testing of your cloud infrastructure stacks using Pulumi.
url_slug: get-started-integration-testing-learn-how-test-your-cloud-infrastructure
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Get Started with Integration Testing: Learn How to Test Your Cloud Infrastructure."
  description: |
    Get started with integration testing of your cloud infrastructure stacks using Pulumi. In this example, you'll learn to use the Pulumi integration testing framework to create Go tests to instantiate and validate 2 different stacks:  a) An AWS S3 bucket implemented in Typescript and  b) A GKE cluster implemented using Python.  The examples are in Typescript, Go, and Python but Pulumi works with many modern programming languages including Typescript, Javascript, Golang, Python, and .NET.   GET STARTED: https://pulumi.com/start  Pulumi's modern infrastructure as code SDK and SaaS work on all major clouds including AWS, Azure, Google Cloud, and Kubernetes.
  sortable_date: 2020-06-22T15:25:39Z
  youtube_url: https://www.youtube.com/embed/5SRtDT_RUlc
transcript: |
    Hello and welcome to Pulumi TV. My name is Mike Mare in today's episode. We're gonna show you how to get started with integration testing of your Pulumi infrastructure stacks. Let's begin in this episode. We're going to cover the following topics. First, we'll start off by deploying two different stacks in Pulumi. The first one is gonna be an AWS S3 bucket implemented using text. The second stack is going to be a gkes cluster and an engine X application implemented using Python. And then lastly, we're going to write integration tests for both stacks using the Pulumi integration testing framework and go tests. As always, you can follow along with our code by visiting the Pulumi TV repo on github. Let's jump into the code. We'll start with the AWS S3 bucket example in typescript. As always, we'll import the packages that we need using the package manager of our choice. In this case, we've pulled the packages down using M PM with the AWS package. We can create and define a new bucket called my bucket with that bucket created. You can actually place a new object into that bucket with the contents. Hello, world. Lastly, we'll export the name. So we know which bucket to reference. This is a pretty straightforward example. So let's jump over to the Gke example written in Python much like in typescript. Our Pulumi stack here is going to start off by importing the packages that we need to instantiate a GKE cluster. If we pull these packages down using pop with the packages in our local repository, we can create and define settings for the cluster create in the final cluster itself and creating to find the GKEQ and figure we need to access the cluster once it's actually up and running with the cluster created, we can then deploy an engine X deployment into that cluster along with a service that is a public load balancer to front it. So that way we can actually hit the end point and verify that it worked with both stacks established. Let's see how we actually test each of these stacks using our Go integration testing framework. Much like any other Go test. We're gonna write a test, go file in it. We're going to first start off by importing the packages that we need. The primary one we need to use the integration testing framework is the following one located on our github repo. Let's start off by checking out how we would write a test to test the S3 bucket stack. If you'll see, you'll notice that we use the same testing instantiation just like usual GO tests do and we actually wrap a couple of base options on the testing instantiation to tailor it to the Stax configurations. How does this know what properties it needs to use? That is what the program test option strut is about. Let's see what that looks like by jumping us into his documentation in program test options. You have a multitude of settings and configurations. You can tailor to your stacks, necessities. You can see that you can set the directory in which your stack is running in any dependencies that must be installed depending on the language used configuration settings for the stack itself secrets any changes you wanna make after the fact. And more importantly, the extra runtime validation is a function that we're going to use to test each of these stacks for validity in what we're expecting it to be. There's many options here and we explored to check these out to see what would be suitable to capture the scenarios you'd like to test. Let's jump back to the actual test on the left. So with the base options established, which are just as sent of settings that we want to apply to all these tests. We don't have to necessarily do this. This is optional, but it allows us to condense shared properties across multiple tests. So with the base options established, we're gonna tweak some more settings, supply the region and the stack is up and running once this test actually kicks off. But more importantly, we have not established the extra runtime validation before we do. So, let's examine what the test in Python for the Gke cluster using engine X looks like. As you can see, we have a similar test established with the same base options. We'll tweak it a little bit just to make sure its configuration settings match what we expect it in our GCP account. And likewise, we have yet to implement our extra runtime validation. Both of these tests will go through the motions of previewing, installing the updates, setting the configuration variables, doing the updates of the actual plume stacks and destroying the stacks and all these parameters are configurable. But once all of those are done, then once the update completes is when the extra runtime validation kicks in and that allows us to actually examine and test the stack that was stood up. So let's actually implement some of these. So we'll start off with the S3 bucket one. Our simple solution is gonna be, since we created a bucket, we put an object into it. Let's just see that when we get buckets, they're not going to be empty. So we'll start off by saying we're gonna import the AWS uh packages um by creating a new session. Um So we'll create a new session in that we're going to define our configuration for a region to use. We're gonna import the packages that we need to actually make sure that this code works So with the session established, let's actually make use of it. We're gonna create a new S3 client that makes use of the session. That service client is going to be S3 dot new on the session. And we're going to pull a list of the buckets by saying the result and an error is being set to the services list buckets operation with no particular parameters. We just want to see if we get buckets back. If there are no errors will be fine. But if there is an error where you want to throw an error on the testing ever, it says we're unable to list buckets. And if we have no error, we want to do an assertion that we don't have empty bucket lists, it will pass a new results object and say message buckets should buckets list should not be empty. So that is all set. Let's go ahead and open all these folds and we can see that we are close to. Oh, we have an error here. There we go. We have all the pieces in place to create a session to AWS and then leverage that session to communicate with an S3 client that will allow us to list buckets and validate that they're not actually empty. Similarly, let's create a runtime validation check for the GKE cluster. So in this cluster, as you'll remember, we deployed an engine X deployment and a publicly facing service load balancer that we want to verify is actually up and has the contents we expect it to have in its HTTP body. So we'll go into the extra run time validation and implement that now. So we'll say the end point can be pulled out of the stack that we have here. Because this function that we're defining here allows us to pass in and close over the stack information that is outputted when the stack is run through the integration testing framework. So we'll say in that stack, give me all of the outputs and we want particularly the external IP of the output. And we'll assert that to a string with the end point. We're gonna establish a max wait of 10 minutes time, that minute times 10. And we have some helper for helper functions that allow us to assert that we are getting an http 200 from the end point that we're visiting and that we're going to then take a step further and validate the contents of that body response. So we'll say assert http results with retry, we'll pass in our testing object, the end point, uh nil the max, wait time and a function that we're going to define just now for the body of how we want to handle that. And that encompasses saying we want to return the assertion that our body contains this stream. Welcome to engine X as famously displayed by default engine configurations and we'll add the missing parentheses. All right, great So let's review it. We have an AWS S3 bucket written in text script, this bucket is created and then an object that's put into it. In our second stack, we have a GKE cluster running in GCP. Once that cluster is up, we're going to communicate with it by deploying an engine X deployment and a service with the public load balancer. We can capture both of these stacks in go test by defining them here as test objects using the integration testing framework from Pulumi. And we can then take a step further and do runtime validation of these stacks to verify that they're running as we expect them with that said, let's actually run our test. This is as simple as saying, go test dash V and the current directory. This will kick off a chain of tests much like any other go testing framework. As you'll see, we're going to go through the various steps that you would imagine. But the integration testing framework is handling all of the, the operations that you normally would when interacting with a Pulumi stack such as installing the dependencies using uh pip end for the Python GCP project using yarn for the text one. We're initializing the project, we're setting configuration settings, we're performing the previews and updates and more. So as this continues, we'll get more output and more feedback. So there you go. The first test for the S3 bucket passed, we were able to verify that the bucket that was not only created but that when we did a fetch on the buckets that the bucket list was not empty. The Gke cluster will take a couple more minutes since it takes five or six minutes for a cluster to come up and for the application to deploy. And so we've gone ahead and ran this test already in a previous screen and we've shown you what that looks like when the output is complete. As you can see both tests pass, we have the S3 bucket that verifies that our buckets returned are not empty and that we have the ability to examine engine X by fetching into its public load balancer that we get and doing an HCTP 200 and returning the body to examine its content. That assertion is simply stated here in a couple of helper functions that allows us to do AC TP uh gets with retries. And all we're essentially doing here is doing those retries on, on a back off timer base that allows us to ensure that we actually are hitting the host and we are getting a status code of AC TP 200 to then process the body as we've done here with examining that welcome to engine X is existing with that. We hope we've learned a lot in how to integrate and test all of your stacks and Pulumi and how Pulumi can be used to instantiate different stacks for different languages. And how we can leverage the go integration testing framework to tie all these together and validate and verify your infrastructure is operating as you expect it. That's all we have for today. Thank you for your time and have a great day.
---
