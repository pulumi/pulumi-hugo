---
preview_image:
hero:
  image: /icons/containers.svg
  title: "From Zero to Production in Kubernetes | Workshop"
title: "From Zero to Production in Kubernetes | Workshop"
meta_desc: |
    Setting up your production Kubernetes environment brings many benefits, including scalability and portability for your applications. Before you rea...
url_slug: from-zero-production-kubernetes-workshop
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "From Zero to Production in Kubernetes | Workshop"
  description: |
    Setting up your production Kubernetes environment brings many benefits, including scalability and portability for your applications. Before you reach production, It‚Äôs important to understand key Kubernetes concepts and architectures available to keep your clusters secure and scalable. Ingress controllers are vital parts of any Kubernetes platform, and the NGINX ingress controller provides the best-in-class traffic management solution for cloud-native apps and containerized environments.  Using repeatable mechanisms to handle your ingress objects and controller deployments is important. Adopting infrastructure as code provides a mechanism to quickly deploy production-ready applications in a repeatable manner. In this workshop, we‚Äôll explore how to leverage the power of Python with Pulumi, an infrastructure as code platform to define and manage your Kubernetes deployments and build powerful abstractions that make getting to production easier than ever.  üë®‚ÄçüíªSource Code:  https://github.com/nginxinc/kic-reference-architectures  
  sortable_date: 2023-03-12T23:30:02Z
  youtube_url: https://www.youtube.com/embed/DPp4veSBUr0
transcript: |
    Welcome everybody. This is from 02 production in Kriti. Uh I'm Josh with me or Javier and uh Jason from a XF five, uh a little bit about the prerequisites uh for the stuff that we're displaying. Uh You'll need a copy of the code. It's open source. So no worries. There. Just go to engine X dot com slash Mara and uh go ahead and clone it. Uh You'll need a Linux workstation or VM, but I believe Mac Os is good as well provided that you are running a Mac Os on Intel. You should be good. We're having a few problems with Apple Silicon. Uh We, we are, we are clear 100% on Intel. Uh We have some Python compatibility issues with the M one architecture. Ah Very interesting. OK, cool. All right. So good. Right. So, yes, do, do this on uh on Linux. Uh You'll need an aws account. Uh You'll need a Pulumi account. Uh Plum accounts are free. Uh And you'll also need Python 3.9 with uh virtual environment support. Um So I'm gonna hand off to, to Jason now. He's gonna tell you a little bit about All right, thanks Josh. Um The Mara project. Uh Mara is something that we started almost two years ago. Now, uh we were approached by our business unit and asked to talk about what a modern application would look like. What when we talk about modern applications, what what are we talking about? And it became abundantly clear as we all started talking together that the application is only part of the overall stack. And you know, we use the iceberg metaphor here because underneath that application, we have monitoring and observ ability and management. Somewhere down there, there's Cuber Tti somewhere, there's actual hardware that's running all of this. Uh So what we like to say about Mara is that it's an example of architecture using crinet that aims to be as production ready as possible. So this is our green field deploy of a modern application on Cuber Tti. So why did we build this? Uh Well, the team that Javier and I are on the community alliances team, we have experience across the breadth of the IT world. So we have former operators like myself, we have developers like Javier, we have Q A folks, we have folks that are in documentation pretty much the entire gauntlet of what you do in a modern IT department. So we want to take all of our experiences and build a best practice example. Second, as we went through and started looking into modern applications, we found a lot of anti patterns out there. And by that, I mean, demos that require a bunch of licenses, trial licenses or what have you demos that actually don't work demos that are basically a very, uh very, very fluffed up marketing pitch. So we want to avoid those anti patterns. And most importantly, we wanted to highlight open source as a core part of the modern applications. Uh the core is open source. You know, there are proprietary pieces that go on top, but at the bottom it's open source. So how do I use Mara? We're gonna do this today in a short little video I have showing a deployment, but it's supposed to be simple. You clone the repository, you have your credentials today. We're using AWS, you could also deploy to Digital Ocean, you can deploy locally to a workstation based Kubernetes. Uh You run a setup script, you run a startup script that asks you a number of questions and Mar will be up and running. The goal was to make it easy for people to bring it up and run it. Now, how are we supposed to use this? Uh Well, you can run the whole project, you can take it, you can run it, it'll come up and we'll show you in a few minutes. This is what Mara looks like. Here's the application, here's the data flowing to our observably, but also it's designed to let you review patterns. Let's say you want to figure out how to instrument your applications with open telemetry. You can look and see how we did it. Uh You can question us, you can go to our github repository, raise an issue, open a discussion and ask us questions. Why did you choose this pattern? Uh We're not always right. Uh And we're willing to listen to the community input and make changes so you can review the patterns and use them or you can lift the code from it. We don't recommend that you run Mara as is in production. Uh But you can take the code and work it into your own solutions. So finally, who runs Mara? What are the use cases we see out there? Well, for developers, it's a way to stand up an application, they deploy Mara and instead of our bank of serious application, they go ahead and set up their own application as an operator. I want to understand how I manage my installation, how I deploy things and how things break. What are my failure modes. What do they look like? The Q A engineers on the team like to have a reproducible test environment. You can spin up a new Mar environment, do your testing, tear it down and spin up a completely new environment that's guaranteed to be the same as the previous one. And then finally, vendors and projects. Uh One of the things we're gonna talk about Mara is how it's a plug architecture, each section is a different Pulumi project. So what we can do in this is we'll be deploying Elk, we'll be deploying uh Grana, but we also can deploy Sumo, we also can deploy lights step so then can plug their components into Mara to use them, show them off and see if they work. And with that, I'm gonna send you back over to Josh to talk a little bit about Pulumi. Alrighty. So uh a little bit about Pulumi. Um first off, um we definitely encourage everyone if you haven't created a Pulumi account yet. Um It is free and we definitely encourage you to do so. So creating a Plumy account will allow you to use the Pulumi uh service back end. And what that will do is it will manage your state file for you and it will also handle secrets, encryption. Uh So Pulumi does not write any secrets to your state file uh in raw text. It's always encrypted. Uh So plume service will do that for you. There are all other also um there are also other options for that so you can use S3 and like K MS if you're in an Aws environment. Um But for at least like to just to get started using the Pulumi service is gonna give you like the smoothest experience. Um Yeah, so if you don't have an account, please do sign up. Um if you um well, at the point you're at the point where you have used me for a little while and are comfortable doing. So, uh we definitely encourage you to sign up for a 14 day free trial uh that it, there's no credit card required and it gives you access to like all of our features like including like the top end like enterprise stuff. So, um you know, so definitely sign up for an account, give it a go for a little while. Uh If you think uh that, that you're, if you're having a good time and you want to expand uh potentially expand your usage of Pulumi uh to your organization, do also try the free trial um individual to use it's free forever. Uh So you don't need to worry about any of that. Um It's basically at the point that you were, you need to collaborate on the team that uh use of the Plume service would incur a cost. So, um when you're learning Pulumi definitely, definitely sign up for an account. It's like the easiest way to get started. OK. So what is Pulumi uh Plume is an infrastructure code tool that allows you to use real programming languages. Uh And it works with pretty much any cloud out there. So when we say real programming languages, uh you can write your infrastructure as code using uh Python, which is what we're gonna be using today. Uh You can use typescript or javascript with node. Uh You can use uh the dot net languages including C# and F Sharp. Uh, you can use Gola. Uh, we have Java, uh, that's in preview and we also even have a YAML option, uh, which is kind of like a like sort of like baby steps way to on board, uh, to Pulumi. In addition, uh, we have a wide range of providers. The providers are the things that allow you to control the resources in a given cloud. So for example, we have an Aws provider. Uh we have an F five big IP provider, we have a Netti provider. Uh We'll definitely be using the um Aws and F uh and Aws and Cober netti providers today. Um Pulumi uh as on the whole is open source. Uh The only part that is commercial is the SAS back end. That's the Pulumi service. Uh But you can use Pulumi in an entirely open source uh way if that's what you wanna do. Uh a little bit about Pulumi program model. So, uh at the top level, we have a project, you don't really deal with projects that much directly. You kind of like when you initialize a Pulumi project, you're basically initializing a program and a stack. Uh So I'll describe the program first. That's where your code goes. That's where you are defining the resources, uh the infrastructure that you want. Pulumi to manage, for example, like a RUT or an aws S3 bucket, things like that. So, within your program, you're defining resources resources have inputs and outputs. An example of an input would be like the version of KTIS that you want for uh an EKS cluster. Uh And then resources also have outputs. Uh So let's take a very simple example where we are uh where we have an aws S3 bucket. And then we want to define a policy for that bucket. Uh We're gonna define the AWS S3 bucket as a resource and we're gonna use its output, which is its one of its outputs, which is its A RN. And we're gonna feed that into the bucket policy. And so through these uses of inputs and outputs, that's how Pulumi internally keeps track of what depends on what and what needs to be created, modified or deleted and in what order uh so resources, inputs and outputs uh If folks are familiar with uh promises in javascript, they behave very, very similarly. Uh Most of the time, you don't even need to worry about the fact that you're dealing with inputs and outputs. Uh As you'll see um when we get into the code a little bit, in addition to the program where we define our resources, we have stacks and stacks are basically instances of our program. Uh And then they might differ by configuration values. So like a common use case for this is we might have a, might want to stamp out a DEV environment, a Q A environment and a production environment. And we might want those environments to differ a little bit. So we might want to use like maybe a smaller crinet cluster that's running on like less powerful hosts in our dev environment than we would say in our production environment. And so we can add configuration uh to our stacks that would feed into our plumy program uh to help define the like right resource sizes that we need. Uh So that's, that's how stacks work. And that's essentially the plume program model. Uh a little bit about Pulumi like architecture. Uh So in the bottom left of this slide, you can see that we have our Pulumi program. Uh That's again where we're going to be defining our resources. We're gonna be using Python for that today. Uh The Pulumi program is run by the Pulumi engine, uh the Pulumi and actually that we're gonna even flip the script on this a little bit. Um Because we're using a little bit more advanced than called the Pulumi automation API uh which I will describe in a minute but a simpler plumbing program uh you have the Pulumi engine and that's going to interpret your program. It's going to check with the, in this case, the Pulumi service, although you can use different state back ends uh to say, hey, what exists in reality and what did you want to? What, what and what do you declare to be in your Plumy program? So Pulumi programs are declarative. You only have to say what you want the state of your infrastructure to be. And Pulumi will figure out the steps necessary by diffing versus the state file. Uh as to like what actions actually need to be taken against the cloud provider's API So if you say I want four worker nodes in my Kriti cluster, uh and there are three. In reality, the Pulumi engine will be able to figure out, hey, I need to add one. if you wanted four and there were five. In reality, the Pulumi engine would figure out, hey, I need to delete one. And the way that it accomplishes the way that it does the work against the cloud provider. API S is through the Pulumi providers and so that you can see on the bottom, right, we have AWS, uh and Kriti specified, but Pulumi uh works against a wide wide range of providers and, uh, to see the full range, uh, I will throw a link in the handouts. Uh, once I'm done, uh, talking, uh, that will, uh, send you to the Plum registry and there you can see the sort of full, uh breadth and depth of the providers that we have available. All right. So now I can hand back to Jason and we can get started with the code. You can go to internet dot com slash Mara, which is a convenience URL that will put you into the, uh, into this repository. And we just want to talk through this at a, at a high level. Uh As I mentioned, this was a project to create a fully deployable kubernetes application, a modern app including management and monitoring tools. Uh And when you go to the repository, you will be met with what we consider a modern app to be, what our architecture looks like, what's being built. Uh And again, as I mentioned, when I started, uh please do interact with us in the discussions or the issues. Uh Please do talk to us in the community slack. Um But let's take a look at some of the actual code. Uh The first thing I want to talk about is the automation API uh Josh mentioned this and this is a very powerful piece of plume that we managed to move to probably about 6 to 7 months ago. Initially, what we were doing is writing a bunch of bash scripts to stand things up and tear things down. But with the release of the Pulumi automation API, we went through and changed things so that we have a Python program that is basically following Python best practices and going through and calling out to the necessary infrastructure providers. So in this case, we're just showing how Aws works. If you're interested in the decision to move to the automation api how we use the automation API effectively. This is a good place to read through to understand what our assumptions were and what we did. Yeah. So just to explain the Pulumi automation api a little bit deeper. Uh So normally when you run a Pulumi program, you're gonna run Pulumi uh new, right? And it's going to scaffold out some stuff, including an underscore underscore main dot py. And the way that you execute that program in Python is that you run Pulumi up and then it will find that main and it will run it when you use the automation. API you actually flip the dependency and you're embedding Pulumi in just a regular old Python program. And you can do all the Pulumi command line stuff programmatically uh within just a regular old Python program. So here, what you're seeing is this is where we go through and iterate through our providers and we've taken care to do things in such a way that providers can be added. So right now, we're going to do AWS today. Uh We also deploy against Leno Digital Ocean and we also have deployments against just cube config. So if you're running micro K eight S, if you're running K three S or mini cube, you can deploy against that as well. Um But here are all the files that do the calls to stand up our infrastructure. So more specifically when we get to infrastructure, you can go through and see. So here is AWS which you're going to be showing. So here's our VPC, here's our Kubernetes installation, here's our registry and kind of to talk about, you know what, what advantages Pulumi offers. Uh This is Python code and this code is going through and using the P or using the Plume Kate provider to sand up our stack. So at this point, let's go ahead and take a look at the deployment. So this is gonna show us deploying Mara against AWS uh to get started, you just go out and clone the repository. Once you've clone the repository, since we are using Python, you're going to go ahead and need to set up a virtual environment. Uh And just a quick note at this point, everything you're seeing here has been accelerated. Uh Currently a full AWS stand up with Mara takes down the order of 30 to 40 minutes with most of that time being spent doing things like deploying Eks. Uh So we're speeding this up, but here's us deploying our virtual environment. Uh Javier has been involved with this. One of the issues we've been running into is versioning for Python. Uh making sure that every Python version works with. What else we're deploying. Once you stood up your virtual environment, we go ahead and actually call the runner command. This is the Python program that calls the automation API. Uh So in this case, we're using a visor provider, we're calling our stack, Mara test 05. And now it's gonna ask us some questions about what our environment looks like. Where are we deploying how big of an EKS cluster do we want? What version do we want to use? How many nodes once you've answered this, once you can tear the environment down and back up and it won't ask you a second time, it's building out its plumy configuration file. Right now, we also ask some password questions for the application itself. And once you've given it all the information it needs, now we start deploying. So we deploy the VPC, we deploy our kubernetes. And then at this point as we go forward, we're gonna see other parts of the stack being stood up. Josh mentioned Pulumi Secret store. The way we handle secrets in Mara is you put them into Pulumi. Uh And then this project here is gonna go through create a secrets project and it's gonna go ahead and store those secrets as Kubernetes secrets to make sure everything in Kubernetes can grab them. So here's the secret projects going through and you see we're getting a bunch of information back and we're gonna go to the back end of the Pulumi Sass and look at this because we can view this there as well. One of the additional benefits that we have in this project is we build our own a controller. You can use the off the shelf engine, Cuber, a controller or you can build your own version with custom modules with custom code. So our stack goes through and will build it for you and then it pushes it to the Elastic Container registry where we can then pull it later. And now we're going through and setting up the English controller. This is where things are getting set up. Have you ever, do you have any thoughts you wanna talk about this point? Um Well, I mean, I think that this, this kind of thing, like going through and being able to see, um going back to what you were showing with the automation api the part that's I kind of came into this project in the middle. And one of the things that's been nice for me is I come from more of a development background. So actually reading Terraform isn't part of my natural skill set. I kind of have to strain to do that. And so going through and being able to see, OK, here is a AWS um you know, piece that I know and I can see it represented as code where we're kind of sequentially going through and saying, OK, given this sort of configuration that you saw Jason set up at the beginning of this, you know, here's how we're gonna set up. Um for example, the elastic container registry and it's kind of, it's, it's something I wish I had uh because I spent a lot of time banging my head against um Terraform uh as a developer um because I worked at a place that kind of foisted that on us. Um And so that was the thing that coming into this project going, being able to go into each folder and have a file that said, OK, this is the elastic container registry set up and having it be expressed like step by step, create the elastic container registry with these options. Um Now we have an object that represents the container registry that we can then pass to other things and you know, get things like the Amazon resource, not what the N stands for an A RN, but the, the idea essentially of that and have them be able to use it. Um That's been really interesting. Um The other thing that this kind of reminds me of that um has been really helpful. I mean, Jason, you talked about starting up test environments and having them be clean and reusable. This is something that in my previous work at other companies with microservices was really beneficial where we'd have a team that was doing uh changes that spanned multiple applications that needed to be tested together and in an integration test and a lot of companies use shared integration environments for this. So you're never quite sure what's going on. Uh You can do your best to manage it, but you're never sure it's a clean environment because there's other folks using that integration environment. But we had a place where we could say, all right, I would like to set up a clean environment. That's just for me that has this changed this application, this changes other application um and then spin that up and you know, it's yours and you know that it contains exactly the versions of each service that you were developing to create your change. Um And then you can run your test and then you can decommission that environment uh cleanly and, and start it again if, if you need it. So that was something that from the development perspective um was hugely helpful that I haven't seen anywhere else. And this is the, the type of thing that you could easily accomplish with Mara and automation. Api Thanks Javier. So we've reached the end of the stand up here. Uh The final thing that we stand up is the bank of serious application. Uh The bank of serious application is actually a fork of the Google Bank of Ant. And we chose that application to kind of be the, the highlighted application, Mara because it's a polyglot application. Uh It's microservices based and it's not the world's best application. It has flaws. Uh We wanted to be able to put something out there that we could then work on and fix and make better. Uh So what we've done is we've forked it and we've gone ahead and, and de Googled it. So all the things that tied into Google Cloud have been taken out and our last big update to it added open telemetry tracing to it. And as you watch things stand up, you probably saw the observ ability section go through. Uh with Mar, we've started to go pretty heavy in the direction of open telemetry. This version you're seeing right now, stands up a log store which is elastic search, it uses file beat, it uses prometheus and those are all pretty heavy. Uh And honestly, one of the biggest, you know, if you're, you're watching your cloud spend or if you need to make by with less the weight of the elastic stuff being stood up is, is one of the more expensive parts of Mara. So we started looking at open telemetry and one of the integrations that we'll be speaking on later this week is an integration with Sumo logic which sits on top of Oel for logging for metrics and for observable and allows us to take out the elastic components, take out file beat. Once everything stood up at that point in time, we can go ahead and look at it. Now, the tool I use is K nine S. Uh it's a curses based console application that allows you to get in and look at things. Uh But I wanna go here and look and see what's my end point that I need to go to. Uh So that's my Ingres and I can look in the name spaces and figure out what I'm doing. Uh for this, I want to do things like I want to forward ports. You know, we can do this with, you know, the cube proxy, the cube cuddle. Uh Again, it's find canine s to be a little easier to use. So here we're going through, we're forwarding ports, we're making sure that we're able to get into everything. Uh I do see a question there about Mara for other cloud providers. Uh We accept pull requests. Uh We do, you know, we have a limited amount of bandwidth on the team. So we kind of focus on the big clouds, but we're always happy to work with it. Here. We see the bank of serious application running. It's a real application. You can make deposits, you can, you do everything that you could do in the Google bank of ants. And then we're gonna go take a look at Ghana um from Grana, we can go in, we can take a look. Uh we can add dashboards to it, we can pull up information on our installation. Again, the goal is to make Mara be as functionally complete as we can. Uh And again, this is all being stood up on the back end by Pulumi. Then finally, you know, here's us using the load generator. This is called Locust. We have a plan inside the project and this goes through and allows us to generate a load. This is where if you want to do some stress or if you're an ops person, you want to do some stress testing, you can just keep cranking up the load on the application and see what breaks and then from this, we can go ahead and look and see what our logging is pulling in. So I'm gonna go ahead and stop this now and I'm gonna pop over to my web browser to show you a few other aspects of mar here. And while you, while you're doing that, I just want to say that like this, the stuff that you showed with setting up the Grana dashboards and having the like the log organization and all that stuff. There's all these tags that go into it. Like it's easy to say, kind of going back to the iceberg. It's easy to say, you know, when you're setting up a new service, you know, the things that you need at the most basic levels, you need logging, you need metrics. But inside each one of those things are 1000 tiny decisions that will slow you down and having had to go through this process as a developer before having, even if you're not setting up the whole Mara having a place to go in there and say, OK, well, I've got a Python application. How are you getting these specific tags? I wanna make sure I get the VPC host name into all of the metrics that are getting pushed into my, uh my, you know, Prometheus or Hanna. And then I want to experiment with how that helps me. And when you're doing that from scratch. That's a huge time sync that a lot of people who are working on projects don't anticipate. And so that's what I found helpful is to be able to go in there and say, OK, this is how they're getting that tag into there. And then this is how I can kind of play with it in Hanna. And then I can make a decision about whether that's right for my project or whether I need to use a, a different pattern. Yeah. And, and that what Javier is describing is, is kind of one of the big things with Mara, like Javier and I have both worked with large production environments. We've both worked with environments that we really don't want to make rapid changes on. Like everything is very considered, everything is, is planned out with Mara. We were actually given the opportunity to say just do what you think is best and we're not gonna get paid in the middle of the night because things aren't working uh which is, which is really nice and it's, it's an ability to, to, to do this work. But we also want to give back to the community because we, we have friends, we have peers, we have colleagues who, who are on call. So we want to go ahead and break things before they do. Uh I want to take a second now and I'm on the app dot dot com. I'm looking at my deployment here now, the way we've broken out Mara is and so here's the serious deployment. I, I have a lot of stacks. Um, you, you may not have this many. Um, I'm not really good at cleaning up after myself, but this is the stack that right now, Josh, is there anything you want to highlight on this? Yeah, sure. So, so this is the Pulumi Service. Um, and so, uh, yeah, we can really just go through the, the tab. So in the Pulumi service, you'll see all the stacks that you spun up, either as an individual or as part of your organization uh within there, you can see the output so you can copy, paste those if you need to. Uh if you look at the top right hand corner of the screen, actually, let's go, can you jump up a level just to serious deploy? Cool. So, so that so serious deploy is the um that's the project. And then within these, you see all the stacks. Now this is not what you would necessarily see in like, you know, production thing. You're probably gonna see something like de Q A and prod uh or something like that. But let's click in. So let's click into the stack of six. Cool. All right. So right. So this is the overview, this will tell you what the last update was. But if you go to the activity tab, you can see so you can see all the changes that happen in the stack. So at some point, we spun it up, it added 43 resources. That's what the plus 43 is. Uh You can see the git commit that that's linked to. You can see the branch that it's on. That's the four ef at the bottom and the kick fix. Uh You can see an update number two, chase it to it down and then in update number three, you brought it back up again and then if you click into the details there, uh you see that you actually get uh this is, this is what you would see if you weren't using the automation API. Now you can certainly get this output by using the automation API but if you would just use the raw, like pull me up, uh this is the output that you would get. So you get um when you use the, when you use the service, you get this like history of like who changed what and when and then you also get to see the details. And so like, not only can you see like the summary output but if you click the diff which is not gonna be the easiest to read, but it'll give you like very, very detailed level of like what attributes changed and what resources now because this was a plumy destroy, followed by a plumy up, everything's going to be new. But if let's say you just change something about you know a helm chart or Kubernetes resource, maybe you change the number of replicas and a replica set uh that would be highlighted there in that diff uh In addition, if you go to the resources tab, this shows you all the resources in the stack. And if we scroll, scroll, scroll, you can see that uh there is actually the capability with those little links on the right hand column uh that will open uh those things in the dashboard. Uh So like if you um so like if you're like creating like a VPC in AWS, uh you'll see the links to like jump to the VPC or the subnet or the route table. It's really, really useful. Uh You can see here that like this stack actually references other Pulumi stacks. So you can like jump to that stack in the uh in the Pulumi service. Also super helpful. Uh We have a question of what's graph view. Uh Let's click on it and find out. So graph view uh gives you the dependencies uh between all of your um the resources. So at the top, uh on, on the left hand side, you can see that everything is kind of under the stack that's normal, that's just kind of how Pulumi works. Uh But then you can see like, hey, like, oh that config group, uh the config file belongs to the config group and things like that. Uh So that's what, that's what graph view does. Um So, yeah, I think that's, that's everything, let's, let's click on settings, see if there's anything interesting going on there. Uh No, that is, that's kind of like, that's very much like the danger zone in um in a, like a git repo. It's like where you like rename stuff. Uh And then real quick we can hop to the dashboard. So the dashboard gives you uh an overview of everything that's going on either in your individual uh account or like in your team's account. Uh So you can see like, hey, what stacks have changed recently? How many resources do we have? Um If you scroll up a little bit, you can favorite some of those stacks. Uh If you have a lot of stacks in your organization. And uh oh, also in the top, right, you can see that you can request access uh for Pulumi deployments, which is a new feature of the service. So, uh you know, go ahead and, and check that out uh and request that access and that is all I have to say about service for the moment. I'm gonna check the chat. I do not see any pending questions specifically about the service. Uh So I'm going to disappear now. Thanks, Josh. If you wanna uh before we move on from this, I do wanna say that um you know, Josh showed us the, the, the diff of what's torn down and what's started up and that was one of the welcome again as a developer who is not fluent in terraform uh coming to Pulumi. That was nice is that I find the discs that you get when you do a plan uh from the command line. Those gifts that you get are a lot more, a lot easier to parse uh at least for me um going into this. So that was a nice surprise being able to see that and then having it all up there in the service to, to look at after the fact is, is a really nice feature. Thanks Javier. So one more thing I, I wanna go into, we've, we've talked about a bit is the plug of the Mar project and specifically around observably. And so we've tried to make the directory structure, make sense. Uh Infrastructure projects live under infrastructure, things that just deploy into Kubernetes, live under Kubernetes. And one of the things that lives under Kumar TTI are log agent, our log store and observably. And this is where this is a Sumo logic integration that we've done uh recently with them last week to kind of showcase how we can rapidly add a project in Pulumi to change the complexion of Mara. Uh And I'm actually gonna go and bring up the code here and this code is not very long what this code does. And, and I think one of the strengths of Pulumi which we use throughout the Mara project is I can deploy things using native SDK calls to the Pulumi SDK, I can deploy things using Yamma manifest, which we do in several places in this project or I can deploy things via helm charts. So in this case, Sumo Loic has a helm chart that we use to deploy. And all this does this code figures out where the helm chart is. We have some information here that we use to figure out where to pull it from. And then down here we deploy it. So to get from Mara using elastic file beat and these other things, we're able to put Sumo in relatively quick. And then we just made a change up in the automation API to tell it to look for a flag, telling us to deploy Sumo. And this is a, you know, this is kind of the, the way we've been deploying Mara and adding in features, we'll do a feature branch. So in this case, here's the zoo logic integration preview and it'll walk you through and tell you you want to do zoo logic. Here's the steps you need to take as we move forward, we're going to be doing things like removing uh removing the complexity of trying to figure out which pieces to pick. So when you run that convenience runner script, it'll go ahead and prompt you what you want as your observ ability. What do you want as your application and make it a lot more plug and play configurable for the users. Now, I saw a question earlier about talking about using Pulumi in a pipeline to, to get around the the uh apple silicon issues. And I can tell you that we are currently using Jenkins as a way to test our deployments. Uh And this is not an endorsement of Jenkins. It just happened to be the C I solution that, that I knew. Uh but we do have a number of Jenkins files out here. So for instance, if you want to know how to deploy on Aws, you can pull this pipeline, add it into your Jenkins server and it will deploy Mara. We spin up instances on Digital Ocean to host our runners and, and this works and we're gonna go ahead and hop over tomorrow and you can go ahead and see it come up and we do this with every release. And one of the, one of the issues that we're facing with Mara is we are trying to support, trying to be as cloud agnostic as possible. And usually the things that we see break are not Pulumi are not components within Mara but are actually issues between the cloud providers. Uh So we're discovering things like, you know, cnet's 1.22 on three different clouds. It may work on two and not work on the third. So we have set up some Jenkins to run on this and then you can even notice right now, we're seeing AD O problem uh that is related to a change we made in Mara. I'm seeing some questions about uh github actions argo workflows. Uh Again, and you're looking at 98% of the Mara team right now with Javier and I, so like these are things we, we want to get to. Uh we haven't, we do use some github actions to do things when we, we check code in and we're, we're currently actively debugging those. Uh But these are things that we'd like to do in the future. So if anybody out there has experience with github action, anybody out there has experience with Argo and wants to join the project and help out uh please raise an issue, start a discussion and we can talk about it. Uh We, we know Mara fairly well, but some of these newer technologies, we do have to ramp up to learning them. Then I want to go ahead and show. So here is I do have a running application out there right now. Here is our application. And again, I have a load being generated from locust. Uh For those of you who haven't used Locust before, it's, it's a very simple load generator. Uh Bank of ants came with it and Bank of Athos had it on a much older version and it was pinned as a uh just run in the background. We've taken it and made it. So you actually run it through the, the interface here. Uh But you can do things like chart your usage chart, your response. Uh I, I personally like trying to get the load to a point where things start to break and then figure out where the failure points are. And Mara, uh one of the things we've been looking to do with Mara is use it to help benchmark things. Uh And those could be things like, like our RI controller, it could be things like third party things. I can tell you that when Mara reaches a certain load, we do start seeing problems with elastic that may indicate that it needs to be tuned better. Uh But you can go ahead and run this and see what your failure rate is. And then again, here, it's just pulling data from file beat. This is all configured and runs out of the box. So if you stand up on Aws, you will be able to do all these things within the 30 to 40 minutes. It takes to deploy uh Javier as somebody coming from the developer background, we've got about 10 minutes left. Is there anything you wanna talk about to, to folks out there that may not be from the ops side like I am that that might appeal to them as developers? Yeah, I think I've, I think I've gone over a lot of it. I just think that, you know, one of the places I really see pulling me like we we're doing this um very large scale uh usage of Pulumi here where we're, we're chaining together a lot of different things to, to do what would normally take a platform operations team, you know, monster architect, right? And then after that, you have the, the management of, of that and that, you know, that can be all done and ploy as a, as a developer, I haven't, you know, gone through that experience, I haven't set up an entire platform to run a service. Um But what I have done is the, the the common pattern. You know, I come from a background of um companies that use what, you know, micro services is kind of meaningless at this point. But some version of having one team handling many services and then other teams handle other services. And one of the things that we had to do was um to give a concrete example. Uh if I'm running a service that stores, you know, chat messages, if I'm building a service that does like kind of what we see in the corner here with chat and it has several databases, it might have a um it might have a, a fast database, it might have like a um spacing on the AWS thing that does that Dynamo DV, it might have a Dynamo DB for recent messages. It might have something else for cold storage of old messages. It might have some S3 stuff for maybe storing images. So there's a lot of like infrastructure that's not at the platform level, like it's not at the level of open telemetry collectors or um you know, a lot of the container registries and that type of thing, but it's infrastructure that is left to development teams to stand up, tear down and manage on their own. And that stuff is a huge drag on development teams where they're um the thing that they're good at is developing applications, they're not necessarily good at making sure that they are setting up a cloud database with the right log rotation policies with the right security considerations. For example, um you know, secrets and secret rotation for like a, you know, password rotation for a database and that type of thing. And so, you know, this is looking at the Mara project, this is kind of what I wish I had because I remember spending long sort of like debugging sessions with members of my team on Zoom being like, all right, well, if we change this terraform thing, this might hook it up to the, um, the, this might hook up the Aurora database to the uh AWS Secrets Manager, but we're not quite sure, let's try to read the diff whereas when I was first kind of onboarding to the Mara project and figuring out how do you use Pulumi, it was great because I could say, all right, get me an instance of the secrets manager. And now I wanna, you know, provide that as an argument to um the the API for the database manager. And so that type of thing would have been really killer um back in that context because a lot of folks, we have folks of all different levels of experience, right? You had people who were very senior but had never worked with infrastructure and you had people who were just out of school. And so they all know how to use a programming language. And um I think that's the thing that, that I think is, is really compelling looking at this is that those small changes that I would have to make that I'm gonna be on call for that. I'm gonna be woken up about, um you know, are, are here. And so that's uh that's kind of the thing I I take from Mara, the, the other pieces of it kind of just make that infrastructure, the massive infrastructure, piece of it a little bit more easy to understand. Like actually Jason and I last night were working on something wasn't quite working. We were getting the wrong version of the, the engine X Ingres controller being injected and we didn't want that to happen. And so I had never ventured into that part of the code and Mara, but Jason was able to pull it up and, you know, everyone, most folks can read Python. So I kind of, I'm not a great Python programmer, but I was able to read through it and we were able to have a discussion whereas I think that if, if he were an operator showing me a bunch of terraform that may have gone less smoothly. And so that was one of those things where I was coming out of it. Like, hey, we have this Pulumi uh webinar tomorrow and I actually just had this great little debugging session with Jason where I didn't feel useless. And, and that was a big, that was a big sort of like breakthrough moment for me. So sorry if that was long. But that's, that's kind of the, the general feeling as a developer I get coming, coming to a project that's using Pulumi. Yeah. And, and, and to, to add on to that, you know, I, I I'm not a, I'm not somebody you want doing a lot of development, I'll, I'll make it work, but it won't be pretty. Uh Javier's development expertise with Pulumi, the way we've structured Mara, I can bring him in and point to it and say this needs to be different and then he can help me figure out how to do it. Um I do see one question. Uh So there's some love for data dog in the chat. Uh Yes, Mark can export to data dog, it can export to lights step, it can export to insert your favorite uh you know, visualization tooling here. Uh It's just a matter of going through for Pulumi, you can use the, the data dog provider and it's a matter of setting things up. So the way to add data dog is to fork the project, create a data, you know, Fork Marra create a data dog project, do the stand up there and then plumb things together. And that's something we Javier and I and the other folks that help out can help with, you know, it's just raise an issue. Say I, I wanna add data dog. Here's, here's what I think needs to happen. Uh And we can go through it and this is where, you know, being modular and being conscientious about our interfaces between components enables us to add and remove components to meet what what people really want. Uh Some people want 100% open source. Some people want open source, both enterprise features. So things like data dog or, or lights up. Uh And Mars designed as kind of a a to realize what you actually want and then take it from there and build it in your production environment. I also like uh saw a question on typescript um that I wanted to address, which is, you know, as, as Jason said, you're, you're looking at most of the team and, and there's a lot of Pulumi code. So um we don't have current plans to move things to typescript. But I actually, when I did my, I'm not a Python person, the and so typescript was the closest thing to something I was familiar with, with, with Pulumi. And so that's where I started my, um, my learning and one of the things I did notice that the Plume team does a good job of keeping the, the API s of their various SDKS fairly consistent, like naming wise, like, you know, things are, you know, cased differently between Python and, and, and typescript. But what was nice is that I could, when I was reaching for a pattern that maybe wasn't in the, I wanted to try to do something that I had seen in Mara. I, I was able to pretty easily say, OK, I'm gonna find the equivalent of this, you know, object that's being referenced in the Python code and in typescript. And it was a pretty good experience doing that. So that was, that's something you can do if you still want to use Mara as a, as a, as a reference. But don't necessarily have a lot of competency in Java, which describes me too. Sorry, not Java Python. Yeah. OK. Sorry. Was that OK? For me to say something, I was typing away in the chat, a very, very engaged audience, by the way. Thank you. Uh So much to everyone who's like attended because uh the engagement in the chat has been awesome. And it's uh mostly things I, I know the answer to right away and a few things that, that uh kept me on my toes. So like great questions everyone and excellent participation. Also. Like this is like this is a very, very deluxe uh Pulumi code base that, that you're looking at here with Mara. So like, you know, this is this is really, really taking advantage of like of like the the productivity gains that the Pulumi can give you this. So be very, very difficult to do in some of the other tools out there in the ecosystem. All right. So we have like three minutes left and it looks like people are still asking questions. Um Maybe I can answer a few of those on camera. Let's see. Oh, so we have someone asking about, yeah, that they would like policies code. Uh It's available as an add on to the enterprise subscription. It's only available to business critical customers uh Scott that is feedback that we have received. And uh uh while I can't say anything definite, I'd say stay tuned. Um You can also use our policy as code features uh in local development or within a pipeline. Um It's just that with the um right now with the business critical uh that stuff could be like enforced at a more organizational level. So like you can, you, you can for example, apply default a default policy to like all stacks in your entire organization. Uh And like you get like, you know, nicer integration in the service uh with being able to like, you know, make rules optional and stuff like that. So it's like, you know, a little bit more scalable, particularly for, uh, orgs that might have like a separate security team that want to adjust like the policy settings. Um, but we, yeah, we have heard that feedback and, uh, stay tuned, I guess. Um, Johann asks says, uh, we have a lot of plumy code in typescript in production. It's working great. Again, the types are a bit tricky. Sometimes. Biggest heads up is to trans pi the javascript before publishing as an MP N package. Yes, perhaps. Um I do find that, that the typescript types are to me are, are advantageous. Uh Certainly if uh if, if types are bumming you out, uh javascript is an option and while uh Python has types that are, let's say suggestive uh rather than mandatory, uh you might, you might have a better experience using Python, which and I believe Python is like our second most popular language and are most popular probably among like certain segments of, of users. Um So yeah, it looks like we are about at time. Uh and it closing words from the fine people and at five, I just want to thank everybody for the participation. The engagement was great. Uh And if this is something that interests you the Mara project for it, uh go to internet dot com slash Mara, ask us questions, we have no ego. So if you think we did something wrong, please tell us. Uh because we want to make this better because, and it, it, it is hard. This is a community project. Uh And we want to help the community. That's great. Um Yeah, so I'd like to thank everybody for giving us your attention and uh hope you have a lovely day and hope that you found this webinar useful. Um And also thanks to my co-host, you've been an absolute pleasure to work with. Thank you so much, so long. Everybody be well.

---
