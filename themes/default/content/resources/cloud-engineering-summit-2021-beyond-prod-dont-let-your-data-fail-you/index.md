---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Cloud Engineering Summit 2021: Beyond Prod: Don't Let Your Data Fail You"
title: "Cloud Engineering Summit 2021: Beyond Prod: Don't Let..."
meta_desc: |
    In the era of microservices, decentralized ML architectures and complex data pipelines, data quality has become a bigger challenge than ever. While...
url_slug: cloud-engineering-summit-2021-beyond-prod-dont-let-your-data-fail-you
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Cloud Engineering Summit 2021: Beyond Prod: Don't Let Your Data Fail You"
  description: |
    In the era of microservices, decentralized ML architectures and complex data pipelines, data quality has become a bigger challenge than ever. While infrastructure-as-code and DevOps frameworks such as Pulumi enable best practices in managing and testing the infrastructure and software, much is left to be desired for managing data quality. As data becomes more entangled in software-based decisions, it’s critical for companies to start treating data with similar rigor to what the DevOps world has. In this talk, we will address this challenge through whylogs, an open source standard for data logging. We’ll deep-dive how whylogs fit into the general infrastructure as a whole and how it can enable end-to-end observability and monitoring for your data stack. This shift in paradigm will enable companies that operate with data to move faster and safer by building discipline and processes around data.  Talk by: Andy Dang
  sortable_date: 2021-10-20T23:00:46Z
  youtube_url: https://www.youtube.com/embed/OjCBDRDe9IA
transcript: |
    Good morning everyone. Uh My name is Andy Dang. I'm a co-founder and a tech lead at labs. Today, I wanna talk about the problem of monitoring data once is deployed to spread beyond the normal DeVos monitoring pipeline. So when A I is deployed everywhere and not just A I any data pipeline applications, we organization nowadays have adopted various stacks to deploy machine learning as well as analytics to production environment. And once in production, these data flows can fail in an anticipated way and pro and can prove to be very difficult to operate. For example, as of today, over 100,000 A I failures have been recorded by the partnership on A I in an A I incident database. So if your team run data and A I in production, you probably have a story of how it failed and upset your customers or affected your business just like Andreas here who got recommended a pepper as a substitute for roses by the whole foods algorithm. Luckily is a harmless failure in this case, but that's not always the case. So in the data centric world, uh machine learning or A I application is just one of the many uses of data centric applications. We have analytics software, we have traditional um decision tree making software. So all these pieces are creating a complex flow of uh of data pipelines and they come with their own scaling challenges and they are fighting complex business decisions. Unfortunately, the traditional DEV ops monitoring that revolves around application performance monitoring is insufficient to really address um the quality control as well as ensuring this data centric uh architecture can work effectively in the real world. Um Another thing is that uh when it comes to data, it has a very different characteristic from traditional de de of signals, we have the massive volume of data points. And so when things fail, it's really hard to detect, even when things work, it's also hard to manually configure all these possible dimensions. Um And by what I mean dimensions is because data tend to contain many fields, each of them might fail independently of each other. And oftentimes what the operators at the moment are doing are writing complex queries and extracting complex metrics to detect these issues. These are expensive mechanisms and it creates a lot of friction in the development and deployment process for data centric applications. And sometimes it creates this culture of fear because it's because when it's hard to process and detect failures, it's harder to deploy to make changes. Most of the time, we we often say that companies are flying blind without data monitoring in production and a lot of the existing DEV ops are not designed around these characteristics. So some mitigate no mitigations in the data apps and the machine learning world have done have applied like retrofitting des culture into the MOF culture. Like you've seen solutions around GLAB and github uh based deployment, git based operations. Uh and then you also have solutions where they retrofit the prometheus and graph FAA to monitor data pipelines. As I mentioned, those are insufficient to really address the characteristic of a complex real world pipeline. The landscape itself is still very early and very immature. So what are the gaps here? I I want to recap a bit here is, first of all, data itself is highly dimensional. Um It at the moment when it comes to data monitoring, we tend to couple them with the data warehousing solution because uh we would run query analysis on the using this existing technology most oftentimes sequel, which are powerful. However, because there's this coupling, it limits the ability to monitor this architecture in places where data warehouse warehousing is not available. And that comes to the next part where we have data being deployed to small devices like smartphones or ILT devices. And these are very different from traditional data warehouses um where you have everything in one single location with a massive processing engine and finally the explosion of configurations when you have tons of data. Uh it's harder to tell what to monitor, monitor what matters hard. And this is why observ ability and zero config monitoring philosophy can come to a rescue and solve the problem. So you just don't just deploy data to Prague beyond that, you want to monitor these pipelines and data flow continuously. Taking a step back. I wanna introduce myself a bit more in depth. Um I am the tech lead at dot A I. And what we're building is we are set where the team here set out to build a solution for continuously tracking data and machine learning models to help companies and teams measure what matters in their machine learning system. Uh We take the expertise of I spent six years at Amazon building massive data pipelines and data warehouses, warehouse solutions before entering the uh machine learning infrastructure and tooling world. So my background is um bringing uh to gather this uh kind of knowledge around operating real world massive data systems versus running machine learning in production, both of which are kind of separate fields at the moment. Um But they are converging into what we call M OS uh at WI labs, we build an open source solution called logs as an standard to track uh data quality. And we believe that every team must have access for continuous machine learning monitoring. And I'll talk about wild offs in the next part of my talk. And this solution can allow team to track and measure data quality regardless of where it is in the pipeline, whether it's on your a mobile phone device, on IOT device to where you're running it in a massive real time uh data processing system such as C fa uh this philosophy of lightweight statistic collection allow us to integrate with many points in the data flow and allow you to treat data monitoring problem in a very similar manner as um DeVos monitoring problem. So here's the agenda. Um I'm trying, I'll, I'll try to discuss in a little bit further about specific um the kind of uh data challenges around monitoring for data itself, uh the science behind it and then I'll discuss data observable and what that means in the context of um data deployment. And then I'll go in depth about wild offs and how to get started with wild offs. So data monitoring challenges. This is a very, very high level overview of a machine learning system. I'm taking machine learning system as an example of a data architecture here just to highlight the increasing complexity of how we operate data devops was coined in 2009. And our software architecture has evolved significantly then. Uh obviously, nowadays, we have infrastructures code like Pulumi uh where we deploy software as part of our pipeline and we can monitor and test our infrastructure. However, when it comes to data, um ML pipelines involves a lot of data and metadata, but there's a huge gap in the tooling space to really talk about what it means to do checks at each of these step for, for these boxes in the diagram. Um It's not, and the data comes in a complex form. It's just not just pure numbers, they come in high dimensional, they can be embeddings, they can be images. Uh So we require a solution that, that is slightly more in that provides more insights than traditional DEV solution. And with highly complex data that can support a large number of features and, and, and various machine learning use cases as well as traditional data use cases. And the data volume is one of the things that we want to focus on because we recognize that the pipeline is, first of all, is constantly evolving. Sometimes you don't, you can't keep a record every record of the data that your system seems or process because data is also can be ephemeral um simply because of cost privacy or security, you can't store every single data point sometimes, but you still want to monitor that data flow somehow to make sure that it doesn't look too different from the data flow you've seen yesterday. For example, existing philosophy around this tend to be expensive by storing all the data in the warehouse tedious because you need to run SQL query on top of that warehouse. And that requires writing SQL query which some people are a fan of. But I don't think um a lot of machine learning scientists or software engineers are excited about doing that as a day job and can be really time consuming because these queries get except expensive ex running against a massive uh data warehouse. Uh now and then with the infrastructure changes, it's a lot easier to build terabyte scale pipelines now in minutes. And you can spin off a massive spark job cluster in data breaks, for example, or snowflake and process tons of data at the same time. If you look at all the core steps of a machine learning pipeline or data pipeline, each of these steps involves data transformation and moving massive amount of data. And you can once you operate, start operating this sort of stat, you quickly learn that each step can introduce a data bug in production and can completely derail the whole system even though the software is sound. Uh data systems are very sensitive to data bugs and data changes, especially machine learning one and the majority of machine learning failures stem from data and not the code itself. So this slide, we talked to 100 and 50 data science team and we just ask about the recent failures they have to deal with. And this is just a sample of the issues. And as you see typically when we think about so software failures, we think about either bugs in the code, lack of unit testing, lack of integration testing or infrastructure deployment failures. Um But when it comes to data, everything can be correct. And the data itself is still just different uh because data reflects the real world. And when COVID happens, for example, um it changes how customer behave and therefore changes the data shape uh dev ops traditional debs. So testing validation does not transfer well, due to again, the data issues, uh dimensionality and volume issue I mentioned before. Um also, especially with machine learning system, uh a fluctuation of distribution itself can, can disrupt the whole system. And that is different from what the problem that a system like Prometheus or Hanna are trying to solve. So here's a quick list of data locking versus uh software locking um and different ways how we can monitor the problems in, in in this metrics. Uh And just to call out that if you look at the metrics that we have to collect to effectively monitor uh data stream, it the the number of metrics in if you map it back to the traditional DEV solution, it increases significantly and it increases the cost. So this is why traditional software la monitoring is insufficient for data monitoring. And on top of that, we really want to talk about observable. It's not about monitoring, it's about providing insights because a lot of the time you find problems by looking at metrics at chart rather than setting up individual monitors all the time. So what that means is that machine learning, we can learn a few things from quality control. In traditional de ops, we have Canary, we have constant monitoring for system in production by emitting metrics, we can do something similar with data. So data systems are not deploying nice pipeline into snowflake everywhere they run in live system, they run on devices. So we are proposing this technique called data profiling. Thinking about collecting lightweight, simple lightweight statistics that are meaningful that will allow users to detect data quality in production without having to actually store the data. So here is, let's say we want to design such data law with the sort of kind of quality we would want from our experience working with both machine learning and data warehouse problems. There are few five core categories. First is metadata like just simple things like when it was produced and then the count of data points, these are simple and more advanced ones are statistics um like standard deviation distribution like histogram of the data uh and then stratified samp and then building stratified sampling. On top of this is uh something that is not really addressed by uh by how, how, how traditional software monitoring works. Because using these statistics and distribution and you can decide smartly what data points to collect, like only collecting the outliers or the long tail data points rather than randomly sample your data stream to try to debug uh a data issue for example. And since logging to be part of every ML step or data pipeline step. We need to be thoughtful about the runtime footprint. What are the key properties? Here's some, taking some lessons from the DeVos best practices. Uh A good solution should, should be lightweight, should be portable, should be configurable and we're working with statisticss in massive data sets. The log probably should be merge. So you can build an o a global view of the distribution of your. Uh for example, when you run data on 20 machines, you can see the global view across 20 machines rather than um rather than having to run again SQL analysis to in order to collect global statistics. And then finally, when you have these properties, you can enable uh deployment everywhere for this solution. And this is what we were trying what we are building with wild locks and build in in terms of the kind of properties that allows wild to be uh infrastructure agnostic and can run in multiple environments. So what is why logs it is a data centric logging library. What it does is it log all key statistics of your data over intervals of time instead of producing a log files. For example, you get a a summary statistic file and there are multiple open source library that actually try to tackle the problem of testing data quality or evaluating drift. Um but is a very different approach where it doesn't require you to store the full data set to run such analysis. It decouples the process of producing uh distributions and metrics from the the step of generating drift and analysis. Because the drift and analysis and data quality checks might happen way downstream. When say you compare data between your production system against your uh training system, it can, it provides the foundation for profiling the data testing, data quality and mo and continuously monitor data after production deployment. And thanks to these properties, it works elegantly for big data systems such as Apache spark, which is a Scala. We have a scholar library that is actually wrapped around our Java library. It can work on terabytes of uh of of data, taking full advantage of spark raw processing power. It doesn't require double passing over the data. And because the locks are so lightweight, the cost of any uh bandwidth is minimized and because also tries to tackle the machine learning and all the data uh data engineering steps in Python, we have a Python version as well. And in this example, you're looking at integration with ML flow, it allows um user to track data, associate data fingerprints with every step of their experiment. And this is a very powerful feature when you can visualize these uh statistics over time to and then you can ideally build monitor and debug data issues by running through this uh exploratory data analysis in in a light weight manner. So you don't have to run it against say a whole sequel database. Um Here's just some a very example of what, how does it it takes in the key statistics per feature. So without zero, without any configuration from user, these statistics are automatically captured. The type of metrics logs that captures are um is configurable and would love the community to contrite, contribute new metrics and ideas. Our github and Slack community are very active and you're welcome to uh charming and look at our road map and directions and once you collect this statistic over time uh across different steps. Uh In in this example, we ran the example for 20 batches of data and we capture the one offs property for every feature in the batch. And we take we can visualize distribution, for example, free sulfur dioxide that across different batches here and visualizing this sort of distribution over time allow user to understand drifts and data blocks even without any automated monitoring. On top of this, you can see here there's a spike in the middle of the graph. And if when given that this feature is an important feature to the model, I probably wanna take a look and make sure that that batch does not have any problem associated with the data like user input. For example, now the most important property of a logging setup is the cost capturing data statistics should not involve massive amount of data uh or processing it postprocessing it. So why use stochastic streaming algorithms to capture remarkably lightweight data statistics? Uh This ensures that constant memory footprint and the ability to lock terabytes of data without breaking the bank. Furthermore, while LOS are merge, so uh streaming pipeline can capture micro batches like every five minutes and aggregate them into hourly batches. And then you can even aggregate them further to create a global view or daily view. Uh The ability to slice and dice is is there for you. And uh again, the result is very lightweight and doesn't contain raw data. So very privacy friendly and the ability to store this get uh using these algorithms like hyper log allow us to capture much, much more accurate data distribution than traditional uh method of sampling, for example. So once with this, you going back to the initial conundrum of monitoring after you deploy to production, this is why we develop logs to capture data every step of your pipeline in production and in development. And this allows you users to visualize to detect data quality issues and take action, make meaningful actions when we open source the library. Uh the A I community actually, and the data community actually found additional ways of using logs output. Uh For example, logs can also be used to test data. You can take the distribution in production and test it against your development uh data set to make sure that what you're doing in development, the transformation um doesn't cause deviation from the actual data and in production, for example, or the sample that you're using in uh in in development is representative of production data set. And then you can also extract constraint from wild off so that you can assert things like data point should be within this distribution or data point should not have X amount of missing value every batch. Uh These very, these become unit tests as well as Canary, we call them uh Canary val data validation for uh post deployment of machine learning and data pipelines. So how do you get started with logs? It is an open source library. It is available in Python and Java. So you can check out our logs, uh guitar repo and our example, we would love to have some feedback. And if you are curious about the one laps platform where we provide free monitoring for uh for your models, uh check sign up for this form and, and you can get an account to, to experience the work flow of visualizing and um and going through the flow of of of pushing and logging and monitoring data on the uh for for your real time and or for your batch models. Fine to conclude, I would like to invite everyone who is thinking about A I governance and transparency and data quality uh for data center infrastructure to join the effort to develop an open standard for for uh data monitoring with wild. We have a slack community to discuss the tooling and the future direction of the standard and the extensibility of it to make sure that it works for multiple use cases. And also check out the link on the right uh for the github package. Again, you can uh pip in story very easily in a Jupiter notebook environment. And try also try out how, how, how many, how, how to see how easy it is to log data as part of your machine learning and data operation. I would love to hear your feedback, uh your contribution and uh feature request to that will help us drive the direction and the integration into your favorite data and machine learning tool and to extend the concept of blogging to new data types of like images, audios. Um And so yeah, uh and also join our slat if you want just to talk to us. Thank you very much for listening.

---
