---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Platform Engineering with AWS Proton and Pulumi | Workshop"
title: "Platform Engineering with AWS Proton and Pulumi |..."
meta_desc: |
    Learn how to enable self-service infrastructure for your organization using AWS Proton and Pulumi. The workshop will briefly introduce Pulumi, an i...
url_slug: platform-engineering-aws-proton-pulumi-workshop
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Platform Engineering with AWS Proton and Pulumi | Workshop"
  description: |
    Learn how to enable self-service infrastructure for your organization using AWS Proton and Pulumi. The workshop will briefly introduce Pulumi, an infrastructure-as-code platform where you can use familiar programming languages to provision modern cloud infrastructure and AWS Proton, a managed service for self-service infrastructure templates.   This 200-level workshop is designed to help users with basic familiarity with Pulumi effectively handle real-world use cases. We will guide you through using AWS Proton and Pulumi with diagrams and a series of labs to help accelerate your organization's platform engineering efforts.  
  sortable_date: 2023-05-03T10:00:04Z
  youtube_url: https://www.youtube.com/embed/4cHN4wIrN7Y
transcript: |
    Hello, everybody. Uh My name is Josh. I am a solutions architect at Pulumi. Uh and with me is Adam and if you'd like to introduce yourself. Yeah. Hey, y'all. My name is uh Adam Keller. I am a developer advocate um with Aws and um specifically, I'm working with uh A BS proton as well as just overall on the platform engineering and DEV ops Space uh over at Aws. So, really excited to be here and talk to y'all today. All right. So, uh if you have not yet done, so I definitely encourage everyone to sign up for a Plumy account. Uh If you sign up for a Plume account, you can use the Plumy back end uh to manage your state file and your secrets. That is definitely the easiest way to get started. The uh ploy service is free to use for individual use forever uh with essentially unlimited usage. Uh So I definitely encourage you to if you're, especially if you're learning, Pulumi use the Pulumi Service back end. Um Once you are up and running with Pulumi and uh if you think that your organization might be adopting it uh on a larger scale. I would encourage you to sign up for a 14 day free trial that will give you two weeks of access to our enterprises enterprise features. You know, pretty much the full uh range of things that the Pulumi service offers. So you get two weeks of free access to all of that including like scin single sign on. Uh you know, our most advanced policy is code features all there. Uh and there is no credit card required for that. Uh So don't use your trial like right away for just like getting up and running with ploy. But like once you get comfortable with Pulumi and you're like, hey, I might want to show this to my organization. Uh That's like a good time to sign up for uh for a trial. OK. So I'm gonna hand off to Adam now and he's gonna tell you a little bit about proton platform engineering and internal developer platforms. Take it away. Thank you. So, um first of all, again, thank you everybody for, for being here. Super excited to just talk about platform engineering, proton Pulumi and how, how all of this kind of comes together. Um But usually when I, when I talk about platform engineering, I like to start by just kind of talking about how we got to where we are today and, and how we've evolved and you know, when we look at platform engineering, really, it's all foundational built on top of you know this the the DEV ops movement dev ops culture with, you know, the ultimate goal of bridging the gap between developers and operators, you know, and, and, and infrastructure folks. Um you know, we started where we had this, this kind of very siloed model where infrastructure, people built infrastructure, they received a request, then they provisioned the infrastructure provision, the application to run on that infrastructure and then it was ready to go. And the developer just said, here's my code. Now you do the rest obviously, that was a slow process, very time consuming. And where we've gotten to is this kind of more modern era now where uh you know, folks are leveraging the cloud, they are taking advantage of managed services. You know, we've kind of learned that if we don't have to manage a database, you know, server, uh why do it and let's offload that, that heavy lifting, let the cloud take care of it. Um You know, we're using more modern technologies, containers, serverless technologies. There's been this huge uh boom in in leveraging serverless managed services uh and containers. And I mean, look, we see it with Kubernetes today, we see it with, you know, Circulus functionality like Aws lambda. Uh it's just exploded and with that have come some really great uh Infra infrastructure is code tools. Um you know, Pulumi being one of them where you know, infrastructures code started with uh defining as you know, Yaml cloud formation but before that, it was using chef, you know, puppet and we've kind of evolved into really programmatically defining your infrastructure as code. And you know, with that has a lot of power comes with that, right? You can have a lot of conditional logic and really make smart decisions when you build your infrastructure using infrastructure as code. And of course, you know, C I CD pipelines have become all the rage and and that's even evolved into uh into using GIT as, as the source of truth. So we've seen a lot of, you know, pipelines kind of taking over git with get ops and finding, you know, modern approaches to deploying code, deploying infrastructure uh as well. And of course, the, the last thing to mention here is microservices, right? We've, we've gone from this um you know, monolithic uh model of, of developing software to a more micro service model. And uh you know, one thing I wanna say is is there's no right or wrong approach to how you build software and how, how you build uh infrastructure, ultimately, what it comes down to is what's the best model to support the culture of the organization and to support your, your teams. But with that, we found that micro services obviously give you more agility uh so you can move faster, deploy faster, your blast radius is lessened because if you, you break your micro service, you're not breaking the entire application. And you know, throwing up a 500 error on, on your website uh for the entire page, maybe it's just one particular function of uh of your uh application. But with that, while we gained a lot of advantages, ultimately, what we've realized is that, you know, we've kind of sold it as, as maybe some people have sold it as being simpler. Maybe some have said it's a simpler approach. But I think what we've realized is it's not um you know, to build micro services to build out your infrastructure as code, there are still a lot of resources that need to be created. Um And we still have to think about how do we deploy these resources and then once those resources are deployed and our applications deployed, how are we monitoring it? How are we keeping track of um of the health of our application, the health of our infrastructure to ensure that, you know, we're not just going down and, and no one's aware of it. So, you know, what we've seen is with the DEV ops movement that we've made some pretty great progress. You know, developers get the C I CD pipelines, this really awesome automation, but it's still the developers are still focused on, you know, when we're, when we're bringing the two teams together, there's still a lot of focus on infrastructure and developers still have to kind of think about a lot of the uh you know, what's on on the infrastructure side and there's a lot of work. Um And ultimately, what we're hearing is developers don't always want to be experts and you know, be the the 10 X developer, right? They want to do what they do really well, which is build application code. Um And, and you know, deploy that code, so they're looking for standardization. But with all of this, we want to enable developers to, to build at their pace in a self-service model, but ultimately have the guard rails in place to do that. And with that, we've seen, you know, in practice where we see these built giant extractions that are being built by platform teams mega pipelines. Uh you know, this one C I CD pipeline that kind of deploys everything and I call it like the Uber C I CD pipeline that deploys all your infrastructure across many accounts and it becomes really hard when someone wants to come in and maintain that or contribute or build on top of that. So ultimately, what we've seen is there with all the good that's come out of this. In reality, there's still a lot of complexity to manage and we're still looking for ways to simplify that. And this is where platform engineering has, has come in. And again, I will say platform engineering, in my opinion is just an extension of DEV ops. Um You know, I think titles are are great, you know, but at the end of the day, we all have the same goal in mind. And that goal is to just make developers lives easier so they can serve their uh end user, which is, you know, the customer so they can give, you know, fix bugs faster, provide features faster and the operations teams, the infrastructures teams can build resources and define them in a way that enables developers to move faster and that ultimately the developers are their customers. So to put it simply, platform engineering is just looking to bridge that gap with self service interfaces um and ways for the developers to get what they need with all the guard rails and, and compliance built in. So with that, we've seen the boom in platform engineering in two kind of emerging patterns. One of those is get ups where you know, get ups, there's, there's a whole thing on, on a definition on it according to the CNCF. But ultimately, the idea behind get ups is you use get as the source of truth and any time a uh manifest is updated and get ups or any changes made, um There's a controller out there that's going to apply that change. And if anything changes manually outside of code, that controller is gonna reconcile and make sure that desired state is always being met. And then on the other side, we have self-service developer platforms, otherwise known as internal developer platforms. Sometimes you'll hear in in uh internal developer portals. Um But ultimately the goal of a a self-service developer platform is to set your standards and build common blueprints that represent your organization and your best practices of how you operate. So an example may be, you know, defining kubernetes clusters and then defining a standard way to deploy your applications onto those kubernetes clusters that meet compliance that follow the guard rails and ensure that what you're, what a developer is deploying is safe according to your standards. So ad BS proton aims to solve the problem of platform engineering and having to manage the platform on your own. So as I mentioned earlier, you know, platform engineering, the goal is to bridge the gap between developers and infrastructure operators. And proton really believes in that as as its mission. So I like to refer to proton as infrastructure as code orchestration where ultimately the platform team is defining the templates um that developers can, can deploy from oftentimes we call this golden path where we're saying here are the patterns that you can use to deploy. And you know, oftentimes these patterns are, you know, like I mentioned earlier, you know, uh containerized services, serverless functions that are, you know, behind an API gateway and these patterns deploy to all your shared environments based on on where you know, you want them to deploy. So a couple of features about proton and I realize I'm talking a lot, I could honestly talk all day, but I wanna get to the demo because I think that's where the most fun is. But proton provides um some pretty powerful functionality that we found our uh our customers have found to be really exciting and they've had great success with one is the self-service interface. So uh as a as a platform uh engineer, you're able to define your template centrally store them in GIT and use GIT as that source of truth. And proton will actually automatically uh present templates uh and publish those templates based on what you've stored in GIT. And when you set up that sync process, um but it has that really nice self-service interface for developers to go in, they have a nice catalog of, of um different blueprints or patterns that they can choose from, they pass in their inputs to configure the infrastructures code. And then proton actually deploys that infrastructure as code on their behalf. So from the developer's perspective, they get that nice self-service interface, whether that's via the console or API S and proton manages the deployment of those resources as well as the the insight and visibility into all the resources that have been deployed across accounts um using AWS proton. Uh And the last uh you know, feature I'll talk about with proton is, is the versioning uh system. So one common problem we see out there in the wild is, you know, as customers uh you know, grow and grow and, and their footprint gets really large, having insight into knowing what you've deployed across all your different environments becomes very challenging. And where that challenge becomes even more of a potential problem is when you need to apply a fix. And you know, let's just say, you know, there's an ami that you're referencing Amazon machine image, right? And that machine image is referencing a particular package on the operating system that is now has a a pretty critical vulnerability. Well, in order to find out what version of infrastructures code is running across all your environments, you have to do some creative digging to figure out what's running where. Well with proton, that's all central in the proton dashboard, I can very quickly see what a version of my infrastructures code templates are running where and then we can take the appropriate action, do a one click upgrade and then we can get to that latest version. So that's it for proton. Uh I'm gonna pass it off to Josh and we'll go from there. OK? And, and folks, I'm gonna go a little faster than I usually go. Uh Because I wanna make sure that I'm able to get to the code uh because the code is uh more extensive than it usually is uh for uh for one of these hourlong workshops. OK. So what is Pulumi uh pluming is an infrastructure code tool. Uh The essential um elements of it are that it allows you to use real programming languages. Um We'll go over the full list in a moment on one of the future slides. Uh and it works with a multitude of clouds so it works with AWS uh A w uh the other public clouds. So we won't mention by name but you probably know them. Uh and also a long tail of likes uh solution. So like data do new, like GB git lab, so on and so forth. Um Off zero, I think uh there's like um a huge range of uh providers that uh plume can work with. So, um and so like, essentially that is the uh the, the crux of, of like Pulumi is real programming languages and then it works with a multitude of clouds. Um Plume is uh open source. Uh All of the, all the stuff that we're doing today is either open source or using the free version of our SAS back end. Um So uh do, do you know that the only cost that would be incurred by uh running this demo would be the cost of the EWS resources? And that should also be likewise pretty minimal uh a little bit about Pulumi program model. So at the um the overarching kind of structure, the big, the big container is a project. You don't generally deal with projects directly. Um within a project, you have a program that's uh that's your code. Uh that's uh for us that's going to be uh a typescript file uh within your program you're defining resources. An example of resource uh is like a BBC or an S3 bucket, resources had inputs and outputs. Uh And so you will very frequently see this pattern where the output of one resource becomes the input of the next resource. For example, you might define an AWS S3 bucket and the uh bucket name uh output would become the input of a uh a policy that you might apply to the bucket. Uh And so you'll see that pattern repeated uh in our code. Uh Likewise. So behind the scenes, within those inputs and outputs, that's where Pulumi is keeping track of what needs to be uh create uh changed or created or destroyed. And in what order, uh so that's kind of where the magic happens and that's what allows you to make your Pulumi program declarative. Uh You don't have to say, you know, I want to update this attribute of this bucket and then attribute uh change. Like this part of the policy, Pulumi will keep track of those uh dependencies for you by the behind the scene. In addition to our program, where we define our resources, we have stacks, stacks are essentially instances of the program. Uh A very, very common use for stacks is to make a different environments. So, like your Q A DEV and prod um that is actually not as applicable um when you're using proton as we'll see later, but it's just important to know uh you know, how you would do this in a sort of vanilla polly uh situation. Uh a quick overview of Plume's model. So we have our Plume program that is where we are writing our code plume programs can be written in a wide range of languages including Python uh typescript, javascript. That's what we'll be using today. Uh But we also support Go uh C# F sharp. Uh We have java support which is in preview. Uh We also have YAML. Uh YAML is kind of like there for um organizations that might not be ready to take like a, a full step to using full programming languages to define their infrastructure's code. And it's kind of there so that like folks have a little bit easier onboarding if they're not yet comfortable um with, with writing code, uh real, real programming languages to define infrastructure. Your Pulumi program is executed by the Pulumi engine that is like the Pulumi cli. Uh it is going to interact with the Pulumi service to read your state file to find out what state uh your infrastructure is in. And then it is going to inter interact with uh one too many Pulumi providers to actually uh change your resources. So, uh today we'll be using the AWS provider. Uh that is what is making the actual API call to um to the API uh AWS API and uh, but there are other providers out there. Like we have Kubernetes provider that is like very popular. And so like, you know, and you, it is very much idiomatic to use multiple providers in a single um in a single plumbing program. So you might want to spin up an Eks cluster and then deploy some KTIS manifest onto that cluster. Uh And that's like that's a very uh common use of plumbing. Ok. So now we're up to demo time. I'm gonna check the Q and A tag real quick. OK. That is a long question. And I'm gonna have to get to that uh a little bit later because I wanna make sure that I get through the code. Uh I am going to share my screen, rearrange some windows, please be patient. All right, we're gonna share this screen. We're gonna make sure we share the entire screen and the correct screen. OK. So folks should now see visual studio code and now we get to play my favorite part, my favorite game in any workshop. Uh We're gonna play a game called, is it large enough for you to read? Uh How does that look to you, Adam? Does that look? Does that look reasonable? Yeah, that look, that looks pretty good. If anyone complains, I will, I will interrupt you and let you know. Yes, please do. OK. So taking a tour of the code, um There are two important con concepts I understand in proton, there are environments and there are services. Uh your environment is like uh your shared infrastructure and that is typically going to be defined by your platform team and deployed by your platform team. Uh So I'm going and then on top of the in into those environments, you will be deploying services and your services. Uh The templates are still going to be defined by your platforms team. But the actual deployment is typically going to be happening uh by your application development team. So you know, if I am, so as a service team member, I'm going to like come into the proton U I uh and like, you know, fill out a form and then I'm going to click a button and then it will deploy. I will also select the environment into which I am deploying my service and then my service will be deployed. Uh We'll, well, you'll see that in detail uh very shortly. So in this proton templates directory, uh we have an environment template and a service template in the environment template. Um It's important to note that when you bundle uh proton templates, they do have to have a uh prescribed structure to them. Uh And that is detailed of course in the proton dots. Um So within the infrastructure folder here, this is this is like where you're gonna see like the stuff that looks very familiar if you've worked with Pulumi before plus a couple of extra things that I will go into very shortly. Uh So in our pluming program, this is a pretty straightforward pluming program. Uh I'm gonna highlight a few lines. So uh we create a VPC. Uh this is the AWS X VPC. So this creates like a VPC and sub nets and net gateways and all that, all that goodness. So you have a fully functioning VPC like right out of the box with just a couple of lines of code. Uh We also create a in a uh an ECs cluster. Uh We're gonna deploy uh a Fargate service onto that um in the service portion and we have a security group for our load balancer. Uh This is not by the way, 100% production ready. Uh This could certainly be tweaked to be a little bit more, both flexible uh and uh and like uh a little bit more secure. So don't, you know, use this right out of the box for like production. Uh But it should give you a really, really good starting point. Uh And then we also have an A LB and we are deploying uh our A lb into the public subnet of our B PC. So here's an example, I'm gonna highlight over this. That is any um oh I know why uh normally in a pluming program, you would get, you would get the hard typing. Uh But because of the way that we have this set up, you'll notice that there's no node modules here. Uh That's because when we deploy our proton template, we do not include node modules. Uh That is they are uh they will get grabbed when proton runs the template, you'll see all of this later. Uh The important thing to take away from this is like this is our shared infrastructure. So we have a VPC, we have an ECs cluster and we have a load balancer and all of the services that we deploy into this environment uh are going to use this shared infrastructure at the bottom of our file. We have some uh some outputs. Uh These are stack outputs in Pulumi uh in typescript, you do that just by adding the export keyword. Uh So we're exporting the VPC ID, the private subnet I DS, the cluster A RN, the uh the low balancer A RN and low balancer TNS name and all of these things are there because they are going to be consumed by our service template. Notice by the way that we do not export the public subnet I DS because we do not want people deploying services directly into public subnets. That's what the low balancer is partially there for. OK. So that is uh are we so far so good? Uh I, I can't really see the chat because I only have two monitors and one of them has my notes. Uh But I don't even give me a thumbs up. Uh Yeah, I just wanna add, yeah, it seems like everyone's good. Um There was one person just mentioned con maybe some confusion around services. And I just think with, with proton like when, when you think about services think, you know, uh the blueprint that defines how your application runs. So if that's a container, if that's a serverless function, it's whatever the infrastructure uh that is required for your application to run. And then the environment is just those shared resources as Josh said, which by the way, Josh just I wanted to commend you, you explained it like, like maybe you should be the proton developer advocate because that was really good. But the environment is where the shared resources live. And essentially when you deploy a service through proton uh proton bridges uh environments and services together through inputs and outputs. So you make your configurable inputs in the schema file and then proton is able to then dynamically configure resources based on those values. So anyways Josh, so far. So good. Cool. OK. So um how does proton actually like run Pulumi commands? It does it by uh using code build, which is uh for those who don't know code build is um AWS S managed uh continuous integration service. Uh More to, to boil it down to a very, very simple concept. It is basically a con they give you a container and it runs bash commands, whatever you supply it and you define those batch commands uh in this file manifest dot YAML. Uh So there are two scripts that you're basically gonna be supplying here. One is to provision the infrastructure that is going to run a few Pulumi commands and then run Pulumi up and then there's also a deep provision section that's going to uh do whatever it needs to do to set up and it's gonna run plume destroyed. So it's pretty straightforward. Uh Let's take a quick talk uh tour through our provision script. Ok? So we have some debugging stuff You can ignore that safely. OK? So we're gonna download Pulumi, we're gonna run M PM install. That's gonna give us our node modules back and then we're gonna run some plumbing commands to essentially configure the environment. So uh we have a stack name, right? And so um what you can't see here but I have some, I have a sample over here. Uh And I'll go into it in just a moment. Proton takes the inputs that you give through the console and it uh it puts them in AJ O A known JSON file called proton dash inputs dot JSON. And then, so what we're doing here is we're taking proton dot inputs, proton dash inputs dot JSON. And we're using the JQ uh command line tool to get those values and pass them to our Pulumi stack. For example, uh we're grabbing the stack name. Uh You'll see, you'll see more configuration actually when we go from the uh when we, when we go over the uh the service template because that's going to be using all the outputs from the environment which are passed along to the service. Uh If you want to see what that looks like for an environment here is the sample file. So uh the environment name gets passed through like, so we use that environment name that's gonna become the name of our VPC. Uh And then we also, this is actually uh this is actually a little bit outdated, but essentially the one we'll keep an eye on here is the name. So we'll use this name. And then over in our plume program, we pull that in using the plum config functionality. We require the name. We also require a cyber block and then we use this name down here when we actually name the VPC resource. And so this name will filter down into like everything that you see in the console. So like if you're looking at the VPC, if you call this thing, uh you know, my environment, the VPC will end up being called out my environment dash VPC. Uh The um the subnets will have names that are also based off of that. So you'll see which ones are the public subnets and the private subnets and stuff like that. Uh And then we also pass through the cyber block uh as a as a configuration parameter just as a sort of demonstration. It doesn't actually have any effect on uh this particular infrastructure we build. But like if you can think of a scenario where you might have like hub and spoke network with like centralized egress. Uh Those VPC cyber blocks become very, very important because you need them to not collide. And so you need them to be parames. Josh, can I say that is the environment? Uh And I think, you know, so that again, so plume super powerful because you, you can programmatically define your infrastructure as code, you're using typescript here. Now imagine as you get more advanced in your templates, maybe you wanna make a uh one of the optional inputs, load balance or maybe you want to have a central load balancer in your VPC. So with Pulumi, I could have a conditional statement in my code that says if load balancer was enabled. So if that bullying value was set to true when someone created the environment, create a load balancer, create this shared load bouncer. But if not, then that load balancer never gets created. So you can really get you, you there's a lot of power that you have when you mix Pulumi with proton here because you can have this really nice self-service interface with all these options. And the end user is just selecting what they want and programmatically you're able to make decisions at deploy time at render time. So that was all I wanna say. Thanks, Josh Awesome. So um and then, so then the next question is like, well, how do we and how do we like let the console user know um what inputs are available. And so that comes from this file under schema schema dot A. Uh It is a YAML file that essentially lets you define like what the inputs uh the possible inputs are. Uh you can, we, this is a pretty minimal example. Um So right here, we just have the BBC Cider block um environments always have a name anyway. Uh So you don't actually have to define that in the scheme of block. Um But you, there's like a lot you can do with this file, you can say which ones are required, you can add validation uh to these inputs. So like, you know, uh if, if I had more time I could add a uh a validator here um that would like ensure that this is actually a valid cycle blocking. And so, um but let's uh let's get to like the actual action here and let me where, where do we go here? Where did you go? There you are proton console. OK. So uh and now can folks see this? I'm making it nice and big. Hopefully that is OK. And give me a thumbs up if that looks good to you. Little bigger, uh little bigger. All right one. Alright, cool. So here's the proton dashboard. And what we're gonna do now is we are going to create an environment, OK? And create an environment. So here is our environment template. It's called fargate dash env. Uh I may at some point after uh this workshop, go and clean some of these up. So the names are a little bit more consistent. Um But we're gonna select that environment. We're gonna click configure. OK? So under provision, we're gonna leave this alone because we are using code build provisioning. Uh which is, so that's basically neither, none of the above. Uh We're gonna go to which deployment account we're gonna deploy to this account, which is fine. We're gonna give this uh environment a name, my env uh we can give it an option description, but we're not gonna do that here. And then we need to select which role uh is going to be used to deploy it. Uh I will show you how I created this code uh role uh later if there's time, but I'm gonna select this uh this role that I created uh using actually, OK? And then we can add some tags if we need to, but we don't need to do that. So let's click next. Ok? Now, we are able to actually enter the cyber block. Let's let's go 10.3 0.0 0.0 0.16 total, classic classic cyber block. Uh Let's, and then we're gonna hit next, ok? So then we're gonna get a chance to review the inputs that we did. Uh They all look good because this is a simple example. And then we're gonna click create. OK. So now we can see that our deployment status here is in progress. Uh What I'm gonna do is I'm actually gonna reload the page uh and now that we can reload the page, oh Look at this. We have code build provision. And so what we can do is we can click this link to the code build job which will take us into the code build uh console and we can tail the logs and we can watch it, do it, do its thing. And we're not gonna stick around here for the entire creation process. Uh I'm actually gonna jump back to the code so we can maximize our time together. Uh But very shortly, uh I'm actually gonna, I'm gonna jump back to the code now. We'll, we'll check back in here. But what you'll see is you'll see that these um these commands that we define our in our manifest dot L they're gonna start executing. Uh And then we're going to deploy our infrastructure using polluting. Let's see. Do we have any, we don't have any output yet, but that's OK because we have plenty more to show. OK. So that is the environment template. Now, let's take a look at the service template. OK? So our service template um likewise, yeah, this should look pretty familiar. Let's go into the Pulumi programs is the index dot TS file. And what are we doing here? OK. So we are requiring now, remember that we had all these outputs at the end of the environment. Well, we're now making the required configuration parameters uh for the service because this is how the service knows where to deploy its infrastructure. And so what is that infrastructure? Um And let me also preface this by saying uh this, the partitioning of the infrastructure between environment and service is not necessarily optimal. Uh That's another thing that I'd like to work on if I have time. Uh So, so you may see a few minor changes to this after um after this workshop is concluded. Uh So do like, you know, you might want to keep an eye on the um on the workshops uh on the workshops at Repo um that is linked in the handouts. That's where all the code is. So we create a target group for our service and an HD TB listener on the low balancer. And so like, you know, when we create this target group, right, we have this VPC ID and then where is this VPC ID ultimately coming from? It is coming from the environment. And how do we get our environment? Well, when we create our service, we select which environment it's going into. And when we do that, um proton will create a um a proton inputs that chase on file that includes all the outputs of the environment. And so that's right here. That is this file. So you can see that like you know, this is a lot longer than the uh than the, than the JSON file that we have for our environment. Our environment is very short, but our service contains not only like our inputs to the service but also all of the outputs from the environment. And so like here, for example, is like, you know, the, the uh the VPC ID that we're gonna use uh to deploy our listener. So hopefully everybody's following so far. So point being you get the outputs of the environment are passed as inputs to the service. Can I add one thing, Josh really quick, go ahead. Could you go back to that uh input really fast? Um In, in, in addition and again, a a again, you're explaining proton so beautifully. Um If you, one other thing that's really cool about the service templates is you can also bundle a pipeline definition in your service template. So if you decided that you wanted to have a pipeline, what proton would give you here is the pipeline inputs as well. So if you know, you want to make these templates generic, so one thing is maybe you wanna let you know you have a polyglot environment where one team writes and go the other team, you know, develops in Python, but they all need to run unit tests. So you could just say what's your unit test, script location? And then in your pipeline, you're just gonna run that um that that line based on what the the user passed in. And this is where you'll get all the information. So super powerful, you can really build extensive uh like full fledged services here with pipelines end to end and proton will, you know, fill in those blanks uh in this, in this file. All right, that's excellent. Oh And so we can see by the way that our, our V BC is uh being created, uh We're now down to the load bound. So our environment is, is progressing along very nicely uh back to so back to our service. So we're gonna create a target group, a listener I enrolled to run our um to run our far G task. We have a task definition and uh it is running, we're just gonna run essentially. So we're gonna create a far G task if you don't know what Fargate is. Uh fargate is ewss managed uh serverless container orchestrator. Uh It basically means that you can run uh containers without having to worry about the hosts on which they're actually running. You just point it kind of at a VPC and say, uh you know, scale out it within this VPC and it does its thing. Uh It's an excellent option. Uh If you are running an application on containers and uh are just kind of getting started and don't wanna have to necessarily deal with the complexity of running like a Cobert cluster. Uh We have a security group. The security group is this is basically to allow traffic properly. Um From our far day cluster, we need to be able to download the actual engine X container. That's why we have to allow port 443 and then port 80 is where the actual engine X traffic will flow. Again, this is not a production uh ready uh uh set up. You would normally have like uh http S and have that terminate the low bouncer. Uh But for simplicity, we're just gonna run straight port 80 through to uh to engine. Finally, we define the service. Uh The service is going to be deployed into our private subnets. That's where you want. You don't want necessarily want your container to be publicly addressable. Typically you do not. And then the last thing we're gonna do is we're gonna export our service URL uh again in a more um one of the things I do want to eventually do to improve um The this demo code is to allow it to support multiple uh services uh on a, in a single environment right now. It only supports one. So this URL would uh if we had multiple services, there would be like a, an extra um an extra URL segment tacked on to the end and we would use um layer seven based routing to be like, OK. Uh Whatever, whatever, whatever dot Amazon app that AWS dot com slash service one, we'll route to the first service slash service two would route to the second one. Um So that's a future enhancement that hopefully we'll be able to, to get out at some point uh looking at the schema uh or schema dot yaml. So this is the, this is what defines the console inputs for a service. Uh We have a, this is essentially a dummy. Uh uh This is a dummy input basically. Uh But we're gonna allow you to define the service port. It doesn't actually work at a, at present, but that is definitely something that you might uh in a more production scenario of like one clear example of like a, a parameter to the service would be the name of container, right? So you're gonna have like my org slash, you know, uh login service or my org slash orders service or whatever. Uh That's like how you would use it in a more productive scenario. So, like each team would fill out what the name of the container is that contains their application code. OK? So let's see how our uh OK. Good. So our environment is now deployed uh And we can close the code build tab back in proton Town. Uh So we see our environment is called my ENV. And if we hop on over to the VPC console lo and behold to go to our VPC S, we see that we have my environment VPC right here. So that is what we just deployed there is the cer block that we defined and we can see that our inputs to our environment are have been passed along to the actual generated infrastructure. Great. OK. So let's go back to proton and a really cool thing to highlight there, Josh is you didn't. So all you did as the platform engineer and when I say all you did, I'm not trying to, you know, lessen all the, you know, all the hard work, but like you just define your templates using infrastructure as code, right? You're comfortable. That's your, that's your territory. You feel good there. What you didn't have to do was build an infrastructure pipeline. You didn't have to think about cross account roles and like all these all the complexities that come with automating, deploying of these resources, you just, here's my template bundle, here's my infrastructures code, published it and then went through the self service interface and proton handles the rest. So it's just kind of neat that you don't have to think about all of those complex things anymore. You're just really focusing on the value to you, which is defining the infrastructure in a way that's reproducible and deployable. Excellent. Uh So now that we're ready to create a service, we're gonna create, uh we're gonna choose our service template. It's the same single template that we just went over and visual studio code, we're gonna click configure, we're gonna give it a name but my service cool. Uh We don't, uh And then we can define some tags if you want to. That's optional. We're gonna skip that for now and we're also going to define a service instance name. So you can actually deploy multiple instances of the same service uh into multiple environments at once. Uh We'll call this uh my service instance and we need to select the environment. Um It's worth noting that um behind the scenes proton will let you do some pretty sophisticated things so that you can ensure that uh a service is only deployed into a compatible environment and not only a compatible environment but a compatible major version of that environment. Um You can do and we didn't really, we're not gonna show that here today but you uh you do when you create environment templates, there is semantic versioning that you, that you attach to that so you can create a new version of an environment and it might not even be available to anybody yet. Uh You actually have to explicitly publish it. Um So it's like this is like such a huge leap forward. Um For like for these types of manage services, this is like really, you know, proton is like really one of the nicest uh products I've seen uh in this area. So we're gonna select the environment that we just uh deployed into my environment. We're gonna enter our service port which doesn't actually do anything. We don't tell anybody uh we're gonna click next. Ok? Again, we get a chance to review all the choices we made. Uh We think we feel pretty good about them and we're gonna click create. OK? And so now we see our service status has created in progress. Uh I'm going to reload page and we're gonna go to this service instance we're gonna click here. Uh and lo and behold, we have our code build provisioning link And if we click this, it will open uh the codebuild console in a new tab. And again, we can tail the logs and keep an eye on uh what's going on. Uh So it is now running like M PM install. Uh Plume has already been installed and it's eventually going to run Pulumi up. Uh So at this point, uh I think I want to uh hm let's uh actually, you know what? We only have about 10 minutes left. I am by the way more than happy to stick around, I could talk about this for another hour. Uh If folks wanted to. Uh but I don't know that there is necessarily that much demand that said uh maybe this would be a good time to check in on Q and A. Um How are folks doing over there? Is everybody following? Does this all make sense? You know, Josh, there was one question. Yeah, the, the most recent one I think you you may be able to help with is how does proton compare to using uh Pulumi automated deployments could use them in conjunction? Are they more competing? So, uh yeah, I, I think that if you were using proton, you would probably not be using Pulumi deployment. Now that said there's still a lot of value in those scenarios that comes from using the Pulumi service, which uh I I can bring up right now actually. So this is the Pulumi Service. Um One example that comes to mind, right is, um, if you are uh on Pulumi business critical level of um, of service, um we give you the ability to define. So Pulumi has uh a policies code feature called policy packs. And you can, when you're, when you're on business critical, uh you can define a policy pack that needs to apply to every stack in your organization. So if you need HIPA compliance and there's certain AWS services that you can't allow anyone to use, uh you can write one rule in code uh to ensure that those services are not used and they will automatically be applied to all stacks across your organization. Um So that, so no, you would not necessarily be using Pulumi deployments if you're using proton, but there's still, uh, you would still probably want to use the Pulumi service, uh, by the way, uh, in the Pulumi Service, uh, you do get a, uh, history, uh, of this is our service that's being spun up. I guess it, I guess it just finished, um, within the p Pulumi service, by the way, you get, uh, the output, the same output you would see from the ploy cli uh, as well as like the change history to any stacks and any stacks that have been spun up or down as well. Uh So, uh, you get that history in there that's like so important when you're like, trying to figure out like who changed what, when and possibly why. Uh That's always good to know when you're working with production workloads. Uh Also in the Pulumi service. Uh Again, you have like uh we know we have a dashboard that shows you like your resource count over time, stuff like that. Um So I think that I think that the ultimate like proton and the plume service work really well together. Uh But you would not be using the specific feature of colum deployments in all likelihood if you are using proton. Um let's check back by the way on co build. OK. So we are all done. Our service has been uh deployed and if we go back to the pro top console, uh we can reload page and if we scroll down, uh I think it's actually what this up. No, here it is. So if we look here, details, outputs, we have our service URL. Let's copy that. Let's paste it into the browser and lo and behold, we have ourselves a new fresh, fully functional instance. Of engine X. So it works, which is always good to know. Um So we only have about eight minutes left. Um I wanna make sure I take this time to cover any questions because there's, I mean, this is, this is a very, very, this is a particularly dense uh presentation. I know I went over a lot of stuff. Um What else can I, can I help uh folks uh with, let's see, we have some questions here. Can you deploy services to environments that you already set up outside of proton proton? For example, existing ECs clusters? Yes, yes, you can. Um There's a lot of different ways you could accomplish that, but essentially, you just need to get the cluster ID and any other supporting I DS and infrastructures to Pulumi. And uh there are a lot of ways to do that in Pulumi, you could hard code it. Uh Pulumi has what's called a get method on every resource type. So you can query for example, a VPC by VPC ID uh or other um you know, there are functions within the AWS provider to grab a VPC by different identifying um characteristics. You might be looking for something with a specific tag. Um Again, so there's really, there's lots of ways to reuse that existing infrastructure um outside of um that that was created outside of Pulumi, you can also by the way uh import resources into Pulumi and have and so you, you would be able to potentially create a proton environment or another Pulumi program and import your existing infrastructure. Uh If you go to our blog, I wrote a blog post uh about that very subject pretty recently. Uh Let me see if I can grab it and I'll try and throw it in the handouts or uh for you. And yes. So on top of that. So, so you can also with proton if you want to, if you manage a, a stack outside. But like oftentimes what we see with customers is they kind of wanna, they wanna move to proton, but they want to do more of like that Strangler method type approach where you slowly isolate your stuff created outside proton has a feature called customer manage environments where basically you could define uh an environment and your environment is literally just uh the output. So VPC ID, uh ECs cluster A RN whatever you'd want to share. And then when you create a service definition in proton and you deploy it to that environment and I'm air quoting here, it's gonna grab those outputs and that's how proton knows of those values. So if you wanted to do that like natively in proton, that would, that's how you could do it creatively there. So Graham asked a great question. Can you provision the application on top of the infrastructure with pro time or is it just for the infrastructure? So Graham, that's that there's actually a this is actually a fairly nuanced answer. Uh If you are deploying a containerized service, the application is the infrastructure. Um However, if you have a scenario, for example, where uh you're running software on V MS, uh you could just as easily define in your infrastructure as code, you could define a um code, deploy deployment and do it that way. Um Or you know, with these, you know, because you're using um because you're using code build as your provision. If you can deploy it by executing bash for uh powershell commands, I believe the windows containers are also supported. Uh Check me on that, Adam make sure that's correct. Um But essentially if you can deploy it using a script. Yeah, good. So that is supported. Um You know, if you can deploy it using a script, then you can use pro to, to do this. You could just, you don't even need to be using Pulumi at all necessarily, you can pass parameters. Uh you know, to like if you're using uh what is it Capistrano, I think you could just pass parameters like Capistrano if you're deploying a ruby application like on to virtual machines. Um We have a lot more questions. Uh How does proton auto tagging work if plume is provision in the environment? Does it work if using cloud formation for provisioning? Uh So OK. So proton is going to tag uh in the console, right? Proton is gonna generate tags for let's say the environment regardless, I believe. Right. Yes. Yes. So it's gonna have these sort of Aws generated proton, um, tags. If you want to tag the actual resources the, uh, environment is creating, then in that case, um, you would just pass them through to your Pulumi program and then apply them. Uh, however you need to use in Pulumi. Um, if you were using the AWS, um, excuse me, if you're using the AWS X VPC component, uh the name of the resource which we call my ENV dash VPC or whatever. Uh those, those names are going to automatically filter down because that's how we design the component. So you will see those name tags in the um in the console. Uh Are there any other questions? Let's see. II I think we covered too much, maybe, maybe, but his mind is so full. There's no room for questions there. There's what can the templates be managed directly through proton? And I, I, I'd probably like to get some more uh elaborate a little more on that. But I mean, proton, essentially, you, you define your templates, um you know whether that's in GIT or in S3 and then you tell proton here's the location of the template and then proton knows what to deploy and how, what to render at deploy time. Yeah, the example code um by the way, uses S3 but in a production scenario, um you should use, you should probably use get uh sync instead. The reason that we use S3 um for this demo code is that it allows us to keep all the code in one repository so that things like can't possibly get out of sync. Um And I didn't really get a chance to dive too much into this, but it's worth mentioning that um we act, I actually set up proton and the templates themselves using Pulumi that I ran on my local machine. Uh So it's like a little bit of a uh uh an inception type thing. If you're familiar with that, with that movie, um When we set up the proton resources, they're actually pretty minimal. Um We have a role that codebuild can assume it's gonna allow it to create um AWS resources. So we give code build uh full admin, which is, that's like pretty standard practice. Um So, you know, if you're, if that, if that raises some hairs in the back of your neck, that is actually pretty normal um for, for tools like Pulumi that you, you do want to give them a role with full administrative access because if you give them anything short of that, they're not gonna be able to create I AM roles which are like pretty critical for most uh real world use usages. So like, for example, you can't, most of the time you're not gonna be able to run like a service application unless you are allowed to create I AM roles which in turn requires full administrator. Uh So that is, that is normal. Um In addition, so we have that. Uh so we have that role. Uh We're gonna apply that role to uh code build. We are giving uh we're also passing our Pulumi access token. So that's what allows us to interact with the Pulumi service. Uh We are putting that in secrets manager. Uh And then we give it. So we're essentially creating a secret and secret version. Uh That's so um when codebuild does its thing, uh we are actually passing that through to codebuild as an environment variable and you can see all the details of how that works in manifest dot YAML. Uh And then we're also creating a bucket in which to place our templates. Um So that's essentially how the protein infrastructure works. Uh If you want to look at the details, you can also dive into the environment, template folder and service, template folder. Um That is what actually uploads uh the G ZI template up here. So we have a make file that I know I'm like really, really tacking on quite a bit to the end of this uh workshop. But hopefully the, the folks are following along, we have a make file, the make file will take the code uh for our, for our template and tar and G zip it. And then uh if you run this Pulumi stack, it will upload uh the template into uh a uh S3 and uh create and publish a new version of the template. Um You can also do all of this through the U I. The reason that I decided to automate it is that uh in offering this workshop, I was running through this over and over and over again. And so it became very much worth my while to actually try and automate it as much as possible because I was doing it so many times. Uh If you're doing this in practice, you, it may be totally fine to like set these things up just through the proton console. Um Adam, if you by the way, have to go. Uh No worries. Uh I can stick around for a little while and try to answer uh the remaining questions um up to you. Uh So we have a question. Can I use I am to allow access deployment to groups of templates to a group of users? Uh I would certainly imagine so, but Adam, if you wanna fill in some details there, yeah, so this is something we're we're looking at with proton. So today you can't, you do have some control over who can deploy what in the proton console and API S. However, we are looking at um having more advanced identity uh and access management privileges uh there. So if that's something that you're interested in and you have some, some really good use cases, I'm just gonna post my email in here. Please feel free to reach out to me because that's something we're actively looking at. Uh right now. Awesome. Uh So Camila asks, uh what are some of the potential pitfalls to be aware of when using this combination? Pulumi plus proton? Um All right. So you don't want to do platform engineering in your company until like you're ready to do it. Um Some of the skills that you're gonna need, um some of the skills are gonna be necessary. OK? You're gonna have to understand how to separate environments versus services and which resources go into which place. Because if that gets messed up and you start having a whole bunch of developers, you know, deploying their services and then you're like, uh-oh, we put something in the wrong place that's gonna be difficult uh to fix, not impossible, just, just difficult. Uh It's very expensive in terms of time, not that. So in terms of money, um you definitely uh your platform engineering team needs to understand how needs to understand how to keep a stable versions of environments and services you uh you know, do not want to be uh unnecessarily creating new major versions of either environments or services. Um And so like, you know, and these, these are, these are skills that, that that can be learned. They're not, they're not inherent, no one's more in understanding semantic versioning or like, you know how to partition infrastructure between environments and services. Um but, you know, start small, I would say start small. Um you know, think of, try to put like really obvious shared infrastructure like VPC S, like, you probably don't want uh a service application team uh having to define their own cyber block or like aspects of their VPC. That's like a really great starting point. That's like really easy to, um you know, that's very clearly an environment in my mind. Uh Typically, you're going to want to have some sort of shared. Uh If you have, if you're using ecs or EKS clusters, those are also very good candidates to go into environments. Um So, yeah, again, I would say like, don't definitely don't be in a rush to like roll this out to your entire organization, start small. Um Make sure you go through a few major revisions and that your templates are relatively stable before you start to roll them out en masse. Um I think those are some of the, I think those are the biggest pitfalls that I can think of. Yeah, I, if I can, yeah, I, I think if I can just add a couple of things, I I agree with everything you're saying. One, you know, it it's very common and this is in, in platform engineering in general, we build these, these platforms to enable developers to self serve. One of the biggest failure points is we think we can solve all the problems in this central self-service. Uh platform. And the reality is we can probably get to about 80%. But at some point, you, you're not gonna be able to solve every problem at your organization. So recognize and start, as Josh said, you start small, recognize the problems that you can solve, build your templates. Make sure you're you think programmatically when you're defining these templates and have, you know, that conditional logic to uh you know, use that schema to to have some conditions to deploy things or not deploy things based on inputs. Um But also really quick segway here, proton does have a feature called components where when you get to that point where you just simply can't solve every problem. And there's that like the 80 20 rule, 80% you've solved with the platform, but there's 20% that you just pos can't possibly do everything. This is where components help developers can come in and actually bring their own infrastructure as code and deploy it on top of uh their services. So I'm not gonna dive into that now. But I think as we look forward and as we look at how we're building platforms in the future, this is gonna be critical uh because you gotta be a be able to think of the the developers that need to self serve, but also self enable. And that's where you know, features like components will help. And that's so sorry. There was one more question that I actually, I was gonna touch on too and I see my, my audio, my cameras off, but hopefully my audio is ok. Um proton is not opinionated about where you deploy to. So as Josh was showing proton launches a code job and then deploys your infrastructure as code based on what you have to find there. So if that means that you want to deploy somewhere else, you absolutely can. Uh proton is not forcing you to deploy to one particular cloud. It just runs in AWS runs that code, build, job runs your infrastructure as code. So if you want to deploy something into your, you know, uh data center, you can do that with proton as well. And uh one more thing I wanted to call out um in the uh in the sample code. Uh is that so if you the read me has a lot of the details that I went over, um It definitely explains pretty clearly. I hope, I hope it explains clearly uh all of the uh components of the code base. So like what's in each directory, what word do you need to deploy things? Uh All that information is there for you in the read me. So do um you know, hop on over and by the way, if there's anything that's not clear, please file in issue um in the github repo. Uh I'll be more than happy to uh to take a look at it and, and try to address it as quickly as possible. Um Also, you know, if there's any mistakes in the code, uh likewise, please, uh you know, do file an issue and I'll try and get it fixed up. Uh OK. So I think maybe we have time for like, maybe one more question because we're a little bit over time. Uh Craig, uh My former coworker asks, how does uh proton integrate with co catalyst if it does at all? OK. So yeah, again, my, my computer is getting a little wonky here, but it seems like my audio is good. But yeah, it was good. Thank you. So, uh OK. So it's a good question. So code catalyst really focuses on the that the developer experience um where developers can just kind of get pre vetted blueprints, like kind of opinionated blueprints and they can just click kind of drag and drop their um uh their, their C I CD workflow and it's really like really focused on on developer experience number one and having the opinion built in proton is opinionated in how we deploy infrastructures code and how we deploy. Um But it's very extensible. So you can build on top of proton, you, proton enables you to use Pulumi for example, or it literally you could use proton to deploy just A S A series of bash scripts if you wanted to. So uh that's kind of the main difference is proton offers that developer platform, but the platform team is defining the opinion where code catalyst is AWS has the opinion and that, you know, you can leverage Aws as a opinion there. Cool. And we have one more question. Uh Are there any particular features that make proton different from open source solutions like backstage? Yeah, that that's a good question and, and it's common uh a very common one backstage is really uh a really powerful uh developer portal. Um I would say that just the, the main difference between the two is just proton is, is uh fully managed, very infrastructure is code focused. Um you know, pipeline focused and backstage is really focused on the internal developer portal where developer can you know, kind of get that documentation, get scaffolding, like if they want to get in a a sample project, uh react project, for example, and get that repo you know, presented to them and, and kind of get the scaffolding, that's where backstage really shines. Um But I would say they're, they're, they're very, they, they do have similarities. But also there is a we have a backstage plug-in for proton as we do have some users that like to use backstage, but they want to leverage protons versioning for the infrastructures code uh in Aws. Cool. All right. So I think that we should be wrapping things up. Uh Unfortunately, I would, I would, I could, I could keep going forever on this stuff. This is like one of my favorite topics to really, to really cover um some links for folks that are interested in learning some more. Um If you want to keep on tap of our future um workshops, please go to plume dot com slash resources. So, um I am easily findable uh on the Pulumi Slack. Uh If you have not yet joined, I definitely encourage you to join uh hop on the inside of the workshop channel or even the general channel or the Aws channel. Uh My name is Josh. Uh and I'm pretty easy to find, uh I am the guy that looks like this and has the Pulumi icon next to my name, uh that identifies me as a plumy employee. Um So, uh Adam has disappeared visually. I think he's still with us uh in, in audio. Um But on behalf of both of us, I really want to thank everyone for uh choosing to spend this time with us. Uh We hope that this content is valuable uh that you've learned something useful that you can apply in the real world. Um And uh yeah, thank you so much for your attention. So on everybody.

---
