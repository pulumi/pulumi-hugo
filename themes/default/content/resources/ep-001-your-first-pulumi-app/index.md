---
preview_image:
hero:
  image: /icons/containers.svg
  title: "Ep 001: Your First Pulumi App"
title: "Ep 001: Your First Pulumi App"
meta_desc: |
    Luke builds buckets, containers, and lambdas using JavaScript and AWS with Pulumi.  And, of course, we have some fun with live streaming gotchas.
url_slug: ep-001-your-first-pulumi-app
featured: false
pre_recorded: true
pulumi_tv: false
unlisted: false
gated: false
type: webinars
external: false
no_getting_started: true
block_external_search_index: false
main:
  title: "Ep 001: Your First Pulumi App"
  description: |
    Luke builds buckets, containers, and lambdas using JavaScript and AWS with Pulumi.  And, of course, we have some fun with live streaming gotchas.
  sortable_date: 2018-06-06T19:08:27Z
  youtube_url: https://www.youtube.com/embed/yycztX2iNWE
transcript: |
    All right. Hi, everyone. Uh Can you guys see now? OK. I think we're, we're back on uh sorry for the technical uh difficulties there. Um All right. Uh So if everyone can see uh see what I've got here, um Let me kind of um uh recap uh what I was just talking about. Um And we'll, we'll keep moving from there. So, uh yeah, so uh the, the app that I'm gonna kind of uh walk through today um takes advantage of a bunch of uh of sort of the modern cloud uh features that we know developers, application, developers are trying to use to, to build their applications. And so there's a few things you'll see in this diagram. This is a video thumbnail, we're going to build some pieces of this, we're going to combine together containers. So in this, in this video thumbnail that was F FM peg. But this could be any piece of compute that we want to bring and run in the cloud. We're going to combine LAMBDA and LAMBDA. Really give us the ability to sort of do a event based computation when something happens, run a piece of code and and react to things. Um and then we're going to use sort of managed data services. And so in this case, that's S3, we're going to also maybe throw in some dynamo tables and things like that and see how we can bring managed data services from the cloud platforms into our application. And really the thing that's sort of unique about, about Pulumi is the ability to sort of work with all three of these pieces and combine them all into my application and compose them together really, really quickly into my app. And so let's kind of take a look at what it looks like to do that from, from scratch. So I'm going to come in here to my terminal. I have the Pulumi Cli installed. Um Once you run that installer command, I showed it a second ago. Um You'll have the Pulumi Cl I uh the first time you, you get Pulumi, you'll want to log in to, to the service so that you can run all the other commands. Um And then the next thing you can do is just say Pulumi New. And in my case, I'm going to say, actually let me make a new directory here. Let's create a demo directory. And then I'm gonna say Pulumi New Aws typescript. So, uh Pulumi supports javascript typescript. Um Python, uh I happen to have a personal uh uh connection to typescript. So I'm going to work with typescript today and show up a couple of things we can do there. Um But, but all these demos should be, uh, should work just fine in javascript or typescript or Python. So let me start with the demo app. Um I'm going to run this in uh U West two just so it's a little bit closer to me in here in Seattle and then it's going to go and install the dependencies. So one thing to note about this, installing dependencies because we're using typescript and javascript here, these dependencies are node packages. And so all of the libraries that we're going to use to connect to the cloud providers and build cloud software are delivered as, as node libraries. And so you'll see things like app Pulumi slash aws is a library that we just ship in NPM. And these libraries we can install just like we would any other NPM component. And it means that you can also build your own Pulumi libraries and ship those in NPM and have other folks use them. We're going to have all these libraries be open source as well. And so folks can come and contribute to these libraries that we've built and build their own and share them with others on NPM. OK. So, uh let me just open up a uh editor here and look at this code. So I'll go ahead and just sort of uh delete the default code here and we'll start really from scratch here. So the one thing I want to do before I get started here. You see, I have two of these packages. Pulumi Pulumi and Pulumi Aws. Pulumi Pulumi gives me all the sort of basic things I need to use. Pulumi from within my programs. And Pulumi Aws lets me connect to and create resources uh inside Aws and Pulumi Aws has sort of all the things you might expect. So three bucket and bucket notifications, ec2 instances, dynamodb, all a variety of different kinds of resources. And overall, I think there's something like 500 different resources that Aws exposes and and are available here to program against directly using the Pulumi A W package. Of course, there's similar libraries available for Azure GCP for targeting Coda that sort of thing. But in this example today, instead of just using those Aws resources directly, what I want to use is some higher level frameworks that combine together those resources and make them much easier to use. And so this is where you'll really see the benefits from, from Pulumi is when you use higher level frameworks that people have built that package together, you know, commonly used components. So let me come over here and I'm going to say N PM install that Pulumi cloud and that Pulumi uh cloud dash Aws. So I'll go and show these libraries off a little bit in more detail soon. But Pulumi cloud and clouds are just some, some high level cloud programming frameworks that build on top of all those primitives from AWS that we have found really useful, working with customers we've worked with during the private beta and that we've put into these, these, these packages for everyone to use. One key thing about these. These are just some useful patterns. You don't have to use these. You can build your own patterns like these, you can come and contribute to the ones that we've built so far. We really think of these as just sort of a nice starting point for, for getting started and quickly building up some really productive cloud applications. OK. So let me come into some code and start using those. So I'm gonna say right, gonna import that and now we can see what is available here. So let me create a simple bucket. So I'm gonna say new cloud dot bucket. Uh And here we notice that in this cloud package, we have a bucket class. Um This bucket class really is just a wrapper over that Aws uh um you know, creating an Aws bucket and I'm going to call it um bucket. Now, the the interesting thing is that this bucket class exposes some additional capabilities. So uh one of the things it exposes is the ability to hook up events. Um And so I can use on UT and on delete and these do all of the heavy lifting to combine together the various different resources that Aws provides to hook up events to these to an S3 bucket. And so in a second, we'll go ahead and use that on put. But before I do that, let me just export the name of this bucket so that we can see it from our code. And so I'm going to say bucket name uh equals and then I'll say bucket. And this is a bit funny. I, this is my bucket. Uh That thing has a property called bucket, which is the S3 bucket inside it. And the S3 bucket itself has a property called bucket, which is the, um, the name of the bucket itself. Um, so you can see that here in the definition. Um, so I'm gonna take that and since that's just the name, um, I wanna go and create S3 slash slash and then, and, and that will give me the, uh, the qualified URL for accessing the bucket. Ok? So let me just take that program. Uh One more thing I want to do since I'm using typescript. Um, I'm gonna run the watch task here just so I'm constantly building this and don't have to think about building my code. Ok? So now I'm gonna come over into my terminal and I'm just going to run this program. Um, so to run the program and stand it up and, and, and update a stack, I'm gonna say Pulumi update. So let me update since I haven't yet deployed my app, this is going to deploy it from scratch. So it's going to deploy all the resources that my program indicates it wants and it's going to deploy those into Aws. And in particular, since I said I was wanted to run this in, in US West two, it's going to deploy this into the Oregon region of Aws. And so you see here, I get a description when I run Pulumi update of what I'm, what it's going to deploy. And so we see it's going to deploy that Aws three bucket. You can see the AWS three bucket is being deployed because it's part of this, you know, cloud bucket component, which I used to create it. Um And so if I want to get more details on what's being created, I can sort of click details here and see all the properties of the bucket I'm going to create. Um But let me go ahead and say yes and go ahead and create that uh that bucket resource now, ok. So this is going to take a second to provision the bucket in Aws. Uh And then when it's done, we'll go ahead and look at that, look at that bucket. Ok? So it finished in a few seconds there. Uh And so now let's go and look at a few things where they did this. So first off, um I exported that, uh I exported that bucket name. And so here we can see, I have the name of a bucket and So if I use my Aws command line, I can say AWS S3 LS grab this. Mhm. And I can see, well, there's nothing currently in the bucket. Um, but if I copy some file in there, uh, so maybe I'll copy my Pulumi dot L uh into here, I've uploaded that. And so now if I, uh look at what's in here, I can see that my Pulumi dot Yao file is up in S3. So I provisioned an S3 bucket and I can interact with that uh from the command line here. Ok. So the other thing I can do is I can get this link and go and see kind of all the information that Pulumi understands about my program. Um And so for this stack, we can see that I've only done one update, the one that I just did, we can see some of the details about it like the region I've configured the bucket name that was that output. And I can also see the list of all the resources. Um And I can go and click the AWS link to jump over into Aws uh and see all the contents of my bucket. So here's that Pulumi Amo file again. So, ah ok. I'm told I need to move the cli up a little bit here. Uh Does that work great? Ok. Um ok. So we've got uh an S3 bucket created now. Um And let's go ahead and do a few more interesting uh things with that bucket. So first I can say bucket dot input. Um And so I'm going to say on input, you know, um new input. Uh and this, uh you can see here because I'm using typescript. I get a little bit of help inside my editor to tell me all the sort of things I can do here. And I'm gonna say, uh when I get a new input file, I want to run a little piece of code and, and why don't I just print that out? And I'll say new input file, uh print out the bag of data that I got given there. OK. Uh And so what this will do is it'll say whenever a new file is, is placed into the bucket, run this piece of code here. Uh And that's just a handy way to react to that event. Uh And in fact, what I want to do is just um only fire that if the um item gets put into the input bucket, I'm going to create this thing where I have two buckets where I have two folders, the input and output. And I, when things get placed in the input, I process them and move them over into the output file also move over in the output folders. OK? So here we go, I've got some code, I'm gonna run only when items are placed into the uh input bucket. So let me go ahead and come over here and I'll just do another Pulumi update. Now, the first thing to notice here, we're going to see that preview again, but you'll notice that it's not going to go and try to create that S3 bucket again. We're still, we have the existing application already deployed, which has that S3 bucket. And all we're going to do is see what is the delta we need to provision in AWS to bring these new capabilities that I just authored into my application. And so you'll see that that basically is two things. One, it's this new input function. So this cloud function here which involves building up a few different resources. It involves building up that A S LAMBDA. It involves provisioning some roles and policies and then also this bucket notification, which is the thing that hooks up the bucket to that function to execute. And so the key thing here is we're really the app that we're building, we're describing in terms of sort of what we want to do. But the implementation of it is actually being deployed as managed Aws resources. So we're getting the benefits of all of the great work that aws has done with S3, with LAMBDA, with all of these different managed services. But we're able to use those and compose those in a really simple way in our code. So OK, let me go ahead and perform this update and this is going to take just a little bit um to create those resources. So you'll see it's, you know, creating the roles and role policy attachments and it's creating the function. Currently, the function is going to take about, you know, 30 seconds or so to create. That's something that we're, we're hard at work improving. And actually in our development branches right now is just taking a few seconds. So we're really excited to have that to make this interloop even more productive. But, but for now, we'll just wait a few seconds on this until it completes. So while that's going, uh let me go ahead and sort of build out the next part of this. Um And for the next part, what I'm gonna do is actually have uh a handle files getting placed into the output folder as well. Uh So I'll say a new output, say new output and I'll only fire this on the uh output folder and then just to make something work real quick here. Uh Let's go ahead and when a file gets placed on the input into the input path, we'll just copy it directly over into the output path. And so I can just write some code in here, so I can say that file name equals and I'll say RGs dot key. So this is the file that was placed. Um And since that is going to be input slash something, I'll just say split on slash and get the second part of it, we could of course, write this a little bit more robustly if we had more time, but I'll just go ahead and do it the simple way for now. And then I'm gonna get the contents of that file. Uh So I'll say, uh wait buckets dot get. Um And so this is a way I can go and actually get the contents of this bucket at a given path. And so I'll just say AG dot Key and then I want to go ahead and uh put that into the new location. So I'm gonna say bucket dot put, uh the new location is going to be up, put slash filename and I'm gonna place those contents in here, ok? So this will just copy over that as files um uh into the output folder. Um This should fire off this other lamb. So there we go, I've, I've written some code to do that. So let me come back here and just look at kind of what we did. Um It ended up taking 44 seconds. So, uh definitely something that we're going to make even better, but, but it's pretty amazing we're able to deploy all these different resources into the cloud. Um uh You know, really uh quite quickly here. So I've now got this uh that first piece of code running. Um Let me just start by doing a couple of things. So first off, let me copy of a new file over into that folder. So I'm going to copy over this file into and let me get the for, let me, um, let me do a Pulumi stack output to see what the name of the folder is, the bucket and then I'll just copy the file in that folder. Ok. Oops got the wrong. So to AWS S3 CP, uh, copy that file into the folder and that should trigger that original event handler we had. And so I'll say Pulumi logs now to kind of see if we can look at those logs that we got from the console dot log command. And I'll just do dash F to, to wait until we get some logs here. So we see that just a little bit after we copied it in, we got the logs provided for that. And so this said on new input, which is that function we wrote, I got the bag of arguments. So the key was input. Pulumi was 41 bytes and we got the event time that it happened. And so this is kind of neat. We can both write and deploy the application, but then we can get logs for all the behavior of the application. So all of that compute that we write inside these callbacks and things I can access directly from my logs and sort of interact with that here. Now, these logs are actually themselves coming directly through cloudwatch because that's what, you know, we're targeting A S all the logging for LAMBDA and all of our APIS is going into, into cloudwatch. And so if I just open up the, the app again here, uh, you'll notice if I come over here, uh, I can actually come and look at my, uh, my function. So let me click on this. Um, and this is going directly to my Lambda function in, uh, in AWS. And you see, it did get fired once in Aws. And if I jump to logs in cloudwatch, we'll see those same logs that we pulled this new input file, same logs that we pulled from Pulumi. So all of this is really just using the raw primitives that are available inside Aws. Um And we're taking advantage of that and you can go and use it at the level, you can go, let's blunk around in the Aws console if you want to. Um But we're making it really easy to, um, to interact with that through the CL here. OK. So let me do another Pulumi update to deploy, um, that those changes that I just made. All right. And this time we see a couple of things. So one that new output function that I wrote um is going to get deployed. Um So that one is getting created here. Um You see the bucket notification is getting updated because we now have two different uh notifications that we're wiring up to this bucket. Um uh 11 from the input folder and one from the output folder. And then we're actually going to update the code because we changed the code of that first function to actually copy the file over. And so again, you see that Pulumi is just identifying the, the specific changes I need to make to deploy this code into my application. Uh And I say yes and I will go ahead and, and, and sort of do uh that work. OK? Uh So again, that's gonna take uh around a minute to deploy one of the things while that's going. Um I kind of just want to show um a little bit more of is uh is kind of what these uh projects look like um that we're building on top of. So we're using this Pulumi cloud library and Pulumi cloud is, is just a, a package of Pulumi programs that is available for anyone to build on top of. And so this is the kind of library that, that you could build yourself. Many of the folks who've been using Pulumi so far have built their own libraries that do similar things. But in the domains that they're working in, um the Pulumi cloud library is really targeting, being a high level framework for doing cloud programming and being one that uses general high level concepts that can be applied across multiple different cloud providers. And so in the future, we'll have support for Pulumi cloud targeting, targeting Azure and targeting other platforms as well. And so if I come in here I can kind of look at, uh, at how this bucket class that I'm using works. Um And for instance, if I come to its constructor, um, in the library, you'll see that it's just doing the kind of things that I could do in Pulumi. Um, but it's taking care of a bunch of the heavy lifting for me. Um So under the hood, it's creating an A S S3 bucket and it's providing some good defaults here. Um It's when I hook up an event handler. So when I do an input, it's going to call this a handler method, which is going to create a function and a lamb of permission and a whole bunch of the underlying pieces that I might need to stitch this together. And so all of those things that you would have to do manually um either using some YAML files if you're using uh kind of uh cloud formation or terraform or these sorts of things or describe using sort of some bespoke config language if you're using uh um a, a bespoke serverless tool. Um All those things you can just express as a library here um and reuse them uh in your application code. And so the benefit of all this is, of course, I get to still use the raw Aws things, but I get to program them in a way that feels a lot more natural here. OK? So that that completed and this application is deployed. And so let me go ahead and copy that file in again and this time that should cause both of my lamb to execute. Um And so I'll copy that in. Um And I'll run that Pulumi logs command again and we should see what happens here. So that's from before. Um Let's see if we get some new events now that we copied that file over. OK? So this time, uh at 11 28 which is looked right about now, we got the new input and then uh you know, what is it? Just a couple of seconds later. Uh We got that second lambda filed when the new output object was placed into outputs. Pulumi. Um So there's a couple of interesting things here. One is, you know, we've, we've had a very small amount of code stitched together kind of multiple lambda firing off of an S3 bucket, um sending data into and out of that S3 bucket. Um We'll be able to express all that in a very kind of what it feels like to me is the kind of application developer a very natural way. The second is with Pulumi logs here because Pulumi understands the whole program, we're able to get kind of that aggregated logs across the different sources of compute. And so even though we have deployed two different LAMBDA functions to AWS, we can get a unified log stream for them from both this new input function and this new output function here. Um And so that's kind of a nice way to, to be able to think about your application as a whole. Instead of thinking about each of your land as, as being a little island, you've got to manage uh kind of separately. OK. So, uh so that's kind of a little working uh sort of serverless example. Um There's a couple of directions that we might want to go from here and I'll, uh, I'll, I'll, I'll try out a couple of those and we'll see how much time we have to kind of look at a few of them. The first one that I think is really exciting is, is bringing kind of containers into this as well. Um Containers give us a way to sort of run arbitrary compute that we may have uh in our application, we can as long as we can create a Docker file that, that, uh that, that wraps that compute up, we can run it uh inside the cloud providers. Now, the challenge with kind of running Dockerized computers. Well, it's really easy to, you know, if you're using Docker on your, on your local machine or Docker compose or something, it's really easy to get that up and running locally on your machine. What tends to be a lot harder is sort of getting that into a cloud divider and running it on top of a managed container service and So, one of the nice things about Pulumi is that we can express it in that really natural way like you would, if you were just deploying it using Docker compose locally. Um But we get, we can turn that into something which actually um you know, manages and deploys all the necessary infrastructure in the cloud provider to give you that, you know, uh sort of production ready uh container deploy. And so let me uh let me do a quick thing and instead say, let's say I'm gonna write process file and this one's gonna be a new cloud dot task. And that task is just a, you know, piece of containerized compute that I want to be able to run on demand. Um And so I can come in here and I can look at it and again, you know, kind of using uh you know, typescript and visual C code, we can get some help uh to, to sort of learn how to use this API. And so we can see that there's a bunch of options for how I can configure this task. Um And I could configure it by passing it an image. So if I've got a pre baked image that's already available in the Docker hub or in the private registry, uh I can pass the reference that image here. Um that would work just fine. Um But oftentimes what I want to do is actually build my own custom Docker image uh that's part of my application uh and to deploy that along with my application. So, what I'm gonna do here is I'm gonna, um, is a million put that in a local folder. Um And so I'm gonna say I want to build the contents of this local dot slash process file folder and that's going to build a container that I want to push and, and be able to run uh remotely. The other thing I'm gonna do is just sort of say how much memory I want. So I'm gonna give this thing one gigabyte to run with. Um And then I'm gonna call that uh done for now. So let me go over here. I said that this file existed in process file. Um But I haven't yet uh created that. So let me create a folder for this. I'm gonna create a little docker file here and I would say from ubuntu you get, so I get some help there. Um And let me, actually, I'm just gonna borrow a little bit from here. Um Let me borrow some commands right here. Uh That will just run up to get updates, install Python uh and then install and pip, install the A S cli once I've done that, then I can sort of write some arbitrary code in here and this could be implemented in any language and I could have done this in, in go or I could have done in Java. I could have this, whatever. I'm gonna just use the A CLI directly to do this. So I'm gonna say AWS S3 CP and I'm gonna say, um, you know, bucket slash input and I'm gonna copy that to bucket slash output. Ok. So this is a really simple Dockerized application. Uh It doesn't actually, you know, have a ton in it except it's got the whole ubuntu image uh that, that I can use. I've got database cli and then it's got a little bit of custom logic um for how to invoke the database cli. Uh This is just gonna do that exact same thing that I had been doing inside the Lambda previously where I just copy the files between the two buckets. But this time I'm gonna show kind of moving that compute into uh a container and running that on demand. So let me come over here and I'm still going to use the file name. But instead of me copying it like this, what I'm gonna say is I'm gonna grab that process file uh that I had above and it has a few different API S on it. And the most important one is run and run is an API which lets me uh run the task so I can kick off an instance of the task and that will spin up a container, uh run it to completion when it's done, it'll tear it down. And so let me just say run. Uh and we can see, I have a few different options for what I can pass to this um environment and host. The environment is the one I'll need because I referenced a couple of environment variables there. So I want to say bucket uh and for bucket, I am going to say, what do I have bucket name, the buck. So I want to use the bucket name and not call dot get the input. Uh I wanna be the input file name. And so in this case, that's AGS dot And the output, I wanna be the name that I use for the output, which is going to be output slash. Uh The final name, maybe even for consistency, I'll just kind of rename this to. Ok? Ok. So that's gonna go ahead and run that. Now, this actually returns a promise because it will return once it's kicked once it's launched the task. And so we'll do, we'll go ahead and await that. Um And then once that returns, I'll just say started yes. So now we'll print out once we've kicked off that task, we'll print out, started a task. And then once that task completes it should put the S3 file in and kick off this other land over here. OK. So let me go ahead and uh come back over here um and just do another Pulumi update. Ah OK. So we got an error, um always good to get errors and let's us see the real experience for using this. Uh you know, when you're really working with uh with any piece of technology, you tend to um you tend to run into uh things you've done wrong. Uh So in this case, I forgot to add some configuration. So the error message here says that I cannot create a task, I'm missing uh close configuration. And so there's three options for what I could provide. And this is actually sort of an interesting thing. So today, I'm going to actually use Fargate. And so that's the third option here is I can provide the setting clouds use Fargate and I'll set that the true and that will run our containerized compute using Fargate. There's actually two other interesting options which I won't use today but are actually really interesting to think about. Um One is that I can use clouds EC auto and what this will actually do is spin up a whole for me in DC two container surface. So it'll spin up some EC2 V MS, it'll register those in a E CS cluster and it will then run the containerized compute on top of that. Um The other is EC cluster A RN, which is if I already have my own cluster that I've deployed, my my infrastructure team has already deployed an ECs cluster somewhere. And I just want to run my compute on that existing thing instead of running it either in Fargate or on a new cluster, I can just provide the uh A RN for that cluster and all of the compute will get scheduled there. And this is actually one of the things I think is really interesting with using kind of these libraries like Pulumi Cloud is we can write this code that just says, hey, I want to run this task. And that's sort of what logically I think about as an application developer is, hey, I've got this task. I want to run it and then we can provide configuration that specifies kind of how to connect that into my A S environment in the way I want. So just by switching a configure variable, I can switch from using Fargate to using my own custom S cluster. And so my code stays the same. The way I express what I want to do is the same. But the set of resources that will actually be managed and created in natives is going to be different in those two cases. So with that background, let me say I'm gonna set this variable. So I'm gonna set cloud Aws. Use Fargate. True. Now, I haven't spent much time talking about Fargate. It's worth just for folks who haven't used Fargate before talking for a second about that. And kind of why um why we're using it here and why, why we think it's sort of interesting from a Pulumi perspective. So Fargate is a way to um if you're using Aws and you want to run containers. It's a way to run those without having to manage any BC two instances or any infrastructure yourself. Uh You can just say, hey, I've got a task. Uh I've got a container. I want um I want you to run that in a W and so this is really great uh from kind of a Pulumi developer perspective because it means that uh you kind of get to offload, not just the complexity of what resources to create, which is something Pulumi is great at taking care of. But it means that operational burden of dealing with all of the underlying infrastructure for your application, you get, we have to take care of that for you. And this really lets you focus on your application code and your logic and really quickly deploy it into the cloud provider and have a nice managed service underneath that. So we'll go ahead and use that. Um Let me say, let me update again this time, we'll see kind of a uh some more interesting things being created. So first off, uh you'll notice this update here and once it completes the preview, you'll notice that it actually ran a Docker built. And so because we had pointed at this folder on disk where we had a Docker file, that folder contains the specification of our Docker image. And so we're actually going to run as part of the Pulumi update a Docker build local owning machine. And then we've got to get that darker image from your local machine up into Aws. And so to do that, the cloud task abstraction actually goes and creates a repository in Aws. And so Aws has a managed concept, a managed container repository. And so we're going to go ahead and use that and then we'll push that code up into that repository from the local Docker build and then we'll connect it up into the running task. So we'll create an a task definition. We'll create a log group to collect the logs from that and we'll create a few pieces of infrastructure. So we have to specify the network we want to connect to. And so we'll use the default VPC. We're not going to create any new networking infrastructure here. Um And we're going to just create a little cluster and security group to specify what the networking parameters are going to be for that. So all this is sort of the boiler plate if you've ever tried to use Fargate yourself, even though the service makes it really easy. Once you've gotten everything stood up, actually getting all the various pieces connected together is a little bit of boiler plate that you have to do. Typically, every time you kind of want to use Fargate a new application. And so one of the things we can make really easy here is just by writing, you know, just by writing these couple of lines of code here, these described to Pulumi and to the Pulumi cloud framework that it wanted to create all of these different resources here and hook them up in the right way. And again, all that's really boiler plate that you kind of would have had to do yourself. So I'm going to go ahead and say yes and kick this off. Um You'll see that it, the darker build happened, you'll see now we're getting the updates as this is actually pushing that into the repository in Aws. Um And so the darker build completed and then we're actually pushing those images up uh to Aws. And let me go take a look at kind of what some of that looks like um in uh in Aws. So let's come back over to our resources tab. We can see uh all the resources that are now part of our application. And so one of the things we have is sort of a cluster. Um So this ECs cluster here uh and we can come over, let's just look at the repositories. Um uh Let's just see what we've got here. So we have this uh this repository here. Maybe we haven't finished pushing. So we may not have anything in there. Oh Yeah, I think we're still in the middle of pushing some images up. Uh Let me go look at the task definition then I think we've already uh deployed that, let me see, maybe we haven't yet deployed that either. But OK. Uh We'll wait for this thing to complete before we go look at those in um in ecs. So right now, uh it looks like we're still pushing this up to uh the repository. I think this zoo bunk image is not uh super small. So it's going to take a second to get that up into uh in the AWS, I guess. Actually, you know, one of the other things we can do here in the activity pa we see this update that I'm running locally on my machine. We can actually watch the status of that also in the console here and we can see it in a little bit more detail. So we can see, you know, here we have the full details of what's happening and you see we're pushing up these images, these these layers of the darker image. Um And it looks like, yeah, it looks like we're still waiting. Oh, there we go. We just finished. Um and, and finished pushing it up to the repository and now we're creating the task definition. So this thing should be making progress now here as well. OK? Yeah. Now we're creating the function. Uh So all these other resources have been created now and our code has been pushed up into that registry. So let's go look at that over here. OK? So here we go, we've got that image. It was 192 megabytes in the end and this is that image that we pushed from our local machine is now up in Aws. Um And there we go. So we completed that. So it took a couple of minutes there to get all that up. But, but the net of that was we created a whole bunch of a WS resources built and pushed the dark image from our local machine and updated some lambdas to connect into that. So let me go ahead and uh and just copy that file again and see if we can see what this new workflow uh does here. So I copy that file and I'd say Pulumi logs dash F to watch this uh kick off. And here we see uh you know, 11 43. So that's right now. Uh we created that new input file and then we hit, hit that started task. So we actually kicked off that task in Aws. And so I can see kind of what's going on here if I come into my um let me just come back here and find the uh the cluster resource. So I'll go over to this uh ECs cluster that's running my compute. And you can see that it's uh it's right now provisioning uh this task. Um And so, uh because I kick this off, this is the first time we're running uh this task in Aws. Um And so it's going to take a second for it to, to warm up um in the future. Uh these things can run really fast once I've warmed up an instance in it with them. Um, but for now this is the first time it's gonna uh provision some compute for me, it's going to pull down that image. Um All the things that Fargate will typically do when you've got some new compute, easy went from provisioning to pending. Um, it's going to take another second here to run. Um, but over here we'll see once it does run, we'll see kind of the output from uh from that task. One of the things you'll also see um I can come over and look at this um task definition here. Um Is this has the, the full task definition that I provided. Um And so uh you'll see this, this hooks up to that registry provided provides, you know, says that this wants to run and fargate all the different details that we kind of took care of um building that up. Um is apple. So this will take a little bit. Oh, there we go. OK. So it ran now. Um uh So this was the command actually running uh inside uh of the uh container inside uh Fargate. And if I refresh this, it should say, OK. So now the task is completed. Uh If I look at stop tasks, there we go, this one completed. Um And so now this is done and now you see that new output I got fired as well. So the Lambda function on the other side of that got fired with the results of my container. Ok. Um So I think I kind of have run out of time for today. Um uh So I'll kind of start wrapping up here. Um Next time we'll, we'll spend a lot more time looking at, at containers and building things on top of some of the the container technology here. Um But I just want to kind of highlight a few things about what we uh built in this particular example and how some of this uh kind of generalizes. So first off, you know, this application again, combined kind of three sets of things. It combined, you know, serverless functions and LAMBDA. So these callbacks here that I can provide are each a little LAMBDA that got deployed into AWS and run as, as a managed piece of compute. And it used those to handle events on various data stores. And so in this case, I was working with just a cloud dot bucket which is going to be sort of an S3 bucket that I can store blob data into. Um but I can hook these things up to a variety of different sources. So in this cloud package, we have, for example, um timers, so I can run compute on a, on a regular schedule. We have H two BM point which gives me the ability to run in response to an rest API call. Uh We have um we have tables, so we have the ability to provision a no sequel data store and hook events off of that. So a variety of different kinds of sources of events that we can hook up to. But then on top of just deploying these serverless functions to that, I can also take arbitrary compute that I've got. And so in this case, I just wrote a Docker file that packaged up some, some aws scripts with ubuntu. And I ran that, you know, inside my application across all of this. Though, the thing that I think is really interesting is just that um we were able to express it in this very kind of natural application centric way. But the end result was um was a whole, you know, whole set of managed data, various resource that we were able to compose together as part of our application. And just to summarize again, what we were doing here today was really using this, this cloud library which gives me some of these high level obstructions, makes it really easy for me to combine together these Lambdas containers and manage data services. Um But you can do whatever you want with Pulumi. So if you want to just build a, you use a bunch of raw Aws resources or raw Azure resources, that's something you can go and do directly. If you want to combine those with these resources or send contributions to cloud to enable it to do more things, uh you can do that. Um And so we're, uh, we're really excited to get, uh, more folks uh kind of uh contributing and, and uh, and building packages uh as part of the ecosystem of these higher level packages on top of uh the raw cloud providers. All right. Um So with that, uh, I will uh close down for today. Uh Again, we're in private beta right now. Uh Definitely excited for anyone who's interested in playing around with this stuff to uh join the private beta now, uh um and start getting started building Pulumi programs, building pluming components uh and giving us your feedback uh on the experience. Um You can get started at Pulumi dot com. Uh And uh we'll see you next week uh to go a little bit deeper on containers. Great. Thanks everyone.

---
